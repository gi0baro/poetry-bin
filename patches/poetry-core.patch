diff --git a/pyproject.toml b/pyproject.toml
index 04c1b78..0540b09 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -137,6 +137,5 @@ drop = [
 pyrsistent = "https://raw.githubusercontent.com/tobgu/pyrsistent/master/LICENSE.mit"
 
 [build-system]
-requires = []
+requires = ["poetry-core>=1.0.0"]
 build-backend = "poetry.core.masonry.api"
-backend-path = ["src"]
diff --git a/src/poetry/core/__init__.py b/src/poetry/core/__init__.py
index df62f6e..4c1a225 100644
--- a/src/poetry/core/__init__.py
+++ b/src/poetry/core/__init__.py
@@ -8,8 +8,3 @@ from pathlib import Path
 # this cannot presently be replaced with importlib.metadata.version as when building
 # itself, poetry-core is not available as an installed distribution.
 __version__ = "1.8.1"
-
-__vendor_site__ = (Path(__file__).parent / "_vendor").as_posix()
-
-if __vendor_site__ not in sys.path:
-    sys.path.insert(0, __vendor_site__)
diff --git a/src/poetry/core/_vendor/fastjsonschema/LICENSE b/src/poetry/core/_vendor/fastjsonschema/LICENSE
deleted file mode 100644
index 1d77bbf..0000000
--- a/src/poetry/core/_vendor/fastjsonschema/LICENSE
+++ /dev/null
@@ -1,27 +0,0 @@
-Copyright (c) 2018, Michal Horejsek
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without modification,
-are permitted provided that the following conditions are met:
-
-  Redistributions of source code must retain the above copyright notice, this
-  list of conditions and the following disclaimer.
-
-  Redistributions in binary form must reproduce the above copyright notice, this
-  list of conditions and the following disclaimer in the documentation and/or
-  other materials provided with the distribution.
-
-  Neither the name of the {organization} nor the names of its
-  contributors may be used to endorse or promote products derived from
-  this software without specific prior written permission.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR
-ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
-(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
-LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
-ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/src/poetry/core/_vendor/fastjsonschema/__init__.py b/src/poetry/core/_vendor/fastjsonschema/__init__.py
deleted file mode 100644
index 0d2d388..0000000
--- a/src/poetry/core/_vendor/fastjsonschema/__init__.py
+++ /dev/null
@@ -1,266 +0,0 @@
-#    ___
-#    \./     DANGER: This project implements some code generation
-# .--.O.--.          techniques involving string concatenation.
-#  \/   \/           If you look at it, you might die.
-#
-
-r"""
-Installation
-************
-
-.. code-block:: bash
-
-    pip install fastjsonschema
-
-Support only for Python 3.3 and higher.
-
-About
-*****
-
-``fastjsonschema`` implements validation of JSON documents by JSON schema.
-The library implements JSON schema drafts 04, 06, and 07. The main purpose is
-to have a really fast implementation. See some numbers:
-
- * Probably the most popular, ``jsonschema``, can take up to 5 seconds for valid
-   inputs and 1.2 seconds for invalid inputs.
- * Second most popular, ``json-spec``, is even worse with up to 7.2 and 1.7 seconds.
- * Last ``validictory``, now deprecated, is much better with 370 or 23 milliseconds,
-   but it does not follow all standards, and it can be still slow for some purposes.
-
-With this library you can gain big improvements as ``fastjsonschema`` takes
-only about 25 milliseconds for valid inputs and 2 milliseconds for invalid ones.
-Pretty amazing, right? :-)
-
-Technically it works by generating the most stupid code on the fly, which is fast but
-is hard to write by hand. The best efficiency is achieved when a validator is compiled
-once and used many times, of course. It works similarly like regular expressions. But
-you can also generate the code to a file, which is even slightly faster.
-
-You can run the performance benchmarks on your computer or server with the included
-script:
-
-.. code-block:: bash
-
-    $ make performance
-    fast_compiled        valid      ==>  0.0464646
-    fast_compiled        invalid    ==>  0.0030227
-    fast_file            valid      ==>  0.0461219
-    fast_file            invalid    ==>  0.0030608
-    fast_not_compiled    valid      ==> 11.4627202
-    fast_not_compiled    invalid    ==>  2.5726230
-    jsonschema           valid      ==>  7.5844927
-    jsonschema           invalid    ==>  1.9204665
-    jsonschema_compiled  valid      ==>  0.6938364
-    jsonschema_compiled  invalid    ==>  0.0359244
-    jsonspec             valid      ==>  9.0715843
-    jsonspec             invalid    ==>  2.1650488
-    validictory          valid      ==>  0.4874793
-    validictory          invalid    ==>  0.0232244
-
-This library follows and implements `JSON schema draft-04, draft-06, and draft-07
-<http://json-schema.org>`_. Sometimes it's not perfectly clear, so I recommend also
-check out this `understanding JSON schema <https://spacetelescope.github.io/understanding-json-schema>`_.
-
-Note that there are some differences compared to JSON schema standard:
-
- * Regular expressions are full Python ones, not only what JSON schema allows. It's easier
-   to allow everything, and also it's faster to compile without limits. So keep in mind that when
-   you will use a more advanced regular expression, it may not work with other libraries or in
-   other languages.
- * Because Python matches new line for a dollar in regular expressions (``a$`` matches ``a`` and ``a\\n``),
-   instead of ``$`` is used ``\Z`` and all dollars in your regular expression are changed to ``\\Z``
-   as well. When you want to use dollar as regular character, you have to escape it (``\$``).
- * JSON schema says you can use keyword ``default`` for providing default values. This implementation
-   uses that and always returns transformed input data.
-
-Usage
-*****
-
-.. code-block:: python
-
-    import fastjsonschema
-
-    point_schema = {
-        "type": "object",
-        "properties": {
-            "x": {
-                "type": "number",
-            },
-            "y": {
-                "type": "number",
-            },
-        },
-        "required": ["x", "y"],
-        "additionalProperties": False,
-    }
-
-    point_validator = fastjsonschema.compile(point_schema)
-    try:
-        point_validator({"x": 1.0, "y": 2.0})
-    except fastjsonschema.JsonSchemaException as e:
-        print(f"Data failed validation: {e}")
-
-API
-***
-"""
-from functools import partial, update_wrapper
-
-from .draft04 import CodeGeneratorDraft04
-from .draft06 import CodeGeneratorDraft06
-from .draft07 import CodeGeneratorDraft07
-from .exceptions import JsonSchemaException, JsonSchemaValueException, JsonSchemaDefinitionException
-from .ref_resolver import RefResolver
-from .version import VERSION
-
-__all__ = (
-    'VERSION',
-    'JsonSchemaException',
-    'JsonSchemaValueException',
-    'JsonSchemaDefinitionException',
-    'validate',
-    'compile',
-    'compile_to_code',
-)
-
-
-def validate(definition, data, handlers={}, formats={}, use_default=True):
-    """
-    Validation function for lazy programmers or for use cases when you need
-    to call validation only once, so you do not have to compile it first.
-    Use it only when you do not care about performance (even though it will
-    be still faster than alternative implementations).
-
-    .. code-block:: python
-
-        import fastjsonschema
-
-        fastjsonschema.validate({'type': 'string'}, 'hello')
-        # same as: compile({'type': 'string'})('hello')
-
-    Preferred is to use :any:`compile` function.
-    """
-    return compile(definition, handlers, formats, use_default)(data)
-
-
-#TODO: Change use_default to False when upgrading to version 3.
-# pylint: disable=redefined-builtin,dangerous-default-value,exec-used
-def compile(definition, handlers={}, formats={}, use_default=True):
-    """
-    Generates validation function for validating JSON schema passed in ``definition``.
-    Example:
-
-    .. code-block:: python
-
-        import fastjsonschema
-
-        validate = fastjsonschema.compile({'type': 'string'})
-        validate('hello')
-
-    This implementation supports keyword ``default`` (can be turned off
-    by passing `use_default=False`):
-
-    .. code-block:: python
-
-        validate = fastjsonschema.compile({
-            'type': 'object',
-            'properties': {
-                'a': {'type': 'number', 'default': 42},
-            },
-        })
-
-        data = validate({})
-        assert data == {'a': 42}
-
-    Supported implementations are draft-04, draft-06 and draft-07. Which version
-    should be used is determined by `$draft` in your ``definition``. When not
-    specified, the latest implementation is used (draft-07).
-
-    .. code-block:: python
-
-        validate = fastjsonschema.compile({
-            '$schema': 'http://json-schema.org/draft-04/schema',
-            'type': 'number',
-        })
-
-    You can pass mapping from URI to function that should be used to retrieve
-    remote schemes used in your ``definition`` in parameter ``handlers``.
-
-    Also, you can pass mapping for custom formats. Key is the name of your
-    formatter and value can be regular expression, which will be compiled or
-    callback returning `bool` (or you can raise your own exception).
-
-    .. code-block:: python
-
-        validate = fastjsonschema.compile(definition, formats={
-            'foo': r'foo|bar',
-            'bar': lambda value: value in ('foo', 'bar'),
-        })
-
-    Exception :any:`JsonSchemaDefinitionException` is raised when generating the
-    code fails (bad definition).
-
-    Exception :any:`JsonSchemaValueException` is raised from generated function when
-    validation fails (data do not follow the definition).
-    """
-    resolver, code_generator = _factory(definition, handlers, formats, use_default)
-    global_state = code_generator.global_state
-    # Do not pass local state so it can recursively call itself.
-    exec(code_generator.func_code, global_state)
-    func = global_state[resolver.get_scope_name()]
-    if formats:
-        return update_wrapper(partial(func, custom_formats=formats), func)
-    return func
-
-
-# pylint: disable=dangerous-default-value
-def compile_to_code(definition, handlers={}, formats={}, use_default=True):
-    """
-    Generates validation code for validating JSON schema passed in ``definition``.
-    Example:
-
-    .. code-block:: python
-
-        import fastjsonschema
-
-        code = fastjsonschema.compile_to_code({'type': 'string'})
-        with open('your_file.py', 'w') as f:
-            f.write(code)
-
-    You can also use it as a script:
-
-    .. code-block:: bash
-
-        echo "{'type': 'string'}" | python3 -m fastjsonschema > your_file.py
-        python3 -m fastjsonschema "{'type': 'string'}" > your_file.py
-
-    Exception :any:`JsonSchemaDefinitionException` is raised when generating the
-    code fails (bad definition).
-    """
-    _, code_generator = _factory(definition, handlers, formats, use_default)
-    return (
-        'VERSION = "' + VERSION + '"\n' +
-        code_generator.global_state_code + '\n' +
-        code_generator.func_code
-    )
-
-
-def _factory(definition, handlers, formats={}, use_default=True):
-    resolver = RefResolver.from_schema(definition, handlers=handlers, store={})
-    code_generator = _get_code_generator_class(definition)(
-        definition,
-        resolver=resolver,
-        formats=formats,
-        use_default=use_default,
-    )
-    return resolver, code_generator
-
-
-def _get_code_generator_class(schema):
-    # Schema in from draft-06 can be just the boolean value.
-    if isinstance(schema, dict):
-        schema_version = schema.get('$schema', '')
-        if 'draft-04' in schema_version:
-            return CodeGeneratorDraft04
-        if 'draft-06' in schema_version:
-            return CodeGeneratorDraft06
-    return CodeGeneratorDraft07
diff --git a/src/poetry/core/_vendor/fastjsonschema/__main__.py b/src/poetry/core/_vendor/fastjsonschema/__main__.py
deleted file mode 100644
index e5f3aa7..0000000
--- a/src/poetry/core/_vendor/fastjsonschema/__main__.py
+++ /dev/null
@@ -1,19 +0,0 @@
-import json
-import sys
-
-from . import compile_to_code
-
-
-def main():
-    if len(sys.argv) == 2:
-        definition = sys.argv[1]
-    else:
-        definition = sys.stdin.read()
-
-    definition = json.loads(definition)
-    code = compile_to_code(definition)
-    print(code)
-
-
-if __name__ == '__main__':
-    main()
diff --git a/src/poetry/core/_vendor/fastjsonschema/draft04.py b/src/poetry/core/_vendor/fastjsonschema/draft04.py
deleted file mode 100644
index 25cb374..0000000
--- a/src/poetry/core/_vendor/fastjsonschema/draft04.py
+++ /dev/null
@@ -1,600 +0,0 @@
-import decimal
-import re
-
-from .exceptions import JsonSchemaDefinitionException
-from .generator import CodeGenerator, enforce_list
-
-
-JSON_TYPE_TO_PYTHON_TYPE = {
-    'null': 'NoneType',
-    'boolean': 'bool',
-    'number': 'int, float, Decimal',
-    'integer': 'int',
-    'string': 'str',
-    'array': 'list, tuple',
-    'object': 'dict',
-}
-
-DOLLAR_FINDER = re.compile(r"(?<!\\)\$")  # Finds any un-escaped $ (including inside []-sets)
-
-
-# pylint: disable=too-many-instance-attributes,too-many-public-methods
-class CodeGeneratorDraft04(CodeGenerator):
-    # pylint: disable=line-too-long
-    # I was thinking about using ipaddress module instead of regexps for example, but it's big
-    # difference in performance. With a module I got this difference: over 100 ms with a module
-    # vs. 9 ms with a regex! Other modules are also ineffective or not available in standard
-    # library. Some regexps are not 100% precise but good enough, fast and without dependencies.
-    FORMAT_REGEXS = {
-        'date-time': r'^\d{4}-[01]\d-[0-3]\d(t|T)[0-2]\d:[0-5]\d:[0-5]\d(?:\.\d+)?(?:[+-][0-2]\d:[0-5]\d|[+-][0-2]\d[0-5]\d|z|Z)\Z',
-        'email': r'^[^@]+@[^@]+\.[^@]+\Z',
-        'hostname': r'^(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])\.)*([A-Za-z0-9]|[A-Za-z0-9][A-Za-z0-9\-]{0,61}[A-Za-z0-9])\Z',
-        'ipv4': r'^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\Z',
-        'ipv6': r'^(?:(?:[0-9A-Fa-f]{1,4}:){6}(?:[0-9A-Fa-f]{1,4}:[0-9A-Fa-f]{1,4}|(?:(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|::(?:[0-9A-Fa-f]{1,4}:){5}(?:[0-9A-Fa-f]{1,4}:[0-9A-Fa-f]{1,4}|(?:(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(?:[0-9A-Fa-f]{1,4})?::(?:[0-9A-Fa-f]{1,4}:){4}(?:[0-9A-Fa-f]{1,4}:[0-9A-Fa-f]{1,4}|(?:(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(?:[0-9A-Fa-f]{1,4}:[0-9A-Fa-f]{1,4})?::(?:[0-9A-Fa-f]{1,4}:){3}(?:[0-9A-Fa-f]{1,4}:[0-9A-Fa-f]{1,4}|(?:(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(?:(?:[0-9A-Fa-f]{1,4}:){,2}[0-9A-Fa-f]{1,4})?::(?:[0-9A-Fa-f]{1,4}:){2}(?:[0-9A-Fa-f]{1,4}:[0-9A-Fa-f]{1,4}|(?:(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(?:(?:[0-9A-Fa-f]{1,4}:){,3}[0-9A-Fa-f]{1,4})?::[0-9A-Fa-f]{1,4}:(?:[0-9A-Fa-f]{1,4}:[0-9A-Fa-f]{1,4}|(?:(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(?:(?:[0-9A-Fa-f]{1,4}:){,4}[0-9A-Fa-f]{1,4})?::(?:[0-9A-Fa-f]{1,4}:[0-9A-Fa-f]{1,4}|(?:(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}(?:[0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5]))|(?:(?:[0-9A-Fa-f]{1,4}:){,5}[0-9A-Fa-f]{1,4})?::[0-9A-Fa-f]{1,4}|(?:(?:[0-9A-Fa-f]{1,4}:){,6}[0-9A-Fa-f]{1,4})?::)\Z',
-        'uri': r'^\w+:(\/?\/?)[^\s]+\Z',
-    }
-
-    def __init__(self, definition, resolver=None, formats={}, use_default=True):
-        super().__init__(definition, resolver)
-        self._custom_formats = formats
-        self._use_default = use_default
-        self._json_keywords_to_function.update((
-            ('type', self.generate_type),
-            ('enum', self.generate_enum),
-            ('allOf', self.generate_all_of),
-            ('anyOf', self.generate_any_of),
-            ('oneOf', self.generate_one_of),
-            ('not', self.generate_not),
-            ('minLength', self.generate_min_length),
-            ('maxLength', self.generate_max_length),
-            ('pattern', self.generate_pattern),
-            ('format', self.generate_format),
-            ('minimum', self.generate_minimum),
-            ('maximum', self.generate_maximum),
-            ('multipleOf', self.generate_multiple_of),
-            ('minItems', self.generate_min_items),
-            ('maxItems', self.generate_max_items),
-            ('uniqueItems', self.generate_unique_items),
-            ('items', self.generate_items),
-            ('minProperties', self.generate_min_properties),
-            ('maxProperties', self.generate_max_properties),
-            ('required', self.generate_required),
-            # Check dependencies before properties generates default values.
-            ('dependencies', self.generate_dependencies),
-            ('properties', self.generate_properties),
-            ('patternProperties', self.generate_pattern_properties),
-            ('additionalProperties', self.generate_additional_properties),
-        ))
-        self._any_or_one_of_count = 0
-
-    @property
-    def global_state(self):
-        res = super().global_state
-        res['custom_formats'] = self._custom_formats
-        return res
-
-    def generate_type(self):
-        """
-        Validation of type. Can be one type or list of types.
-
-        .. code-block:: python
-
-            {'type': 'string'}
-            {'type': ['string', 'number']}
-        """
-        types = enforce_list(self._definition['type'])
-        try:
-            python_types = ', '.join(JSON_TYPE_TO_PYTHON_TYPE[t] for t in types)
-        except KeyError as exc:
-            raise JsonSchemaDefinitionException('Unknown type: {}'.format(exc))
-
-        extra = ''
-        if ('number' in types or 'integer' in types) and 'boolean' not in types:
-            extra = ' or isinstance({variable}, bool)'.format(variable=self._variable)
-
-        with self.l('if not isinstance({variable}, ({})){}:', python_types, extra):
-            self.exc('{name} must be {}', ' or '.join(types), rule='type')
-
-    def generate_enum(self):
-        """
-        Means that only value specified in the enum is valid.
-
-        .. code-block:: python
-
-            {
-                'enum': ['a', 'b'],
-            }
-        """
-        enum = self._definition['enum']
-        if not isinstance(enum, (list, tuple)):
-            raise JsonSchemaDefinitionException('enum must be an array')
-        with self.l('if {variable} not in {enum}:'):
-            self.exc('{name} must be one of {}', self.e(enum), rule='enum')
-
-    def generate_all_of(self):
-        """
-        Means that value have to be valid by all of those definitions. It's like put it in
-        one big definition.
-
-        .. code-block:: python
-
-            {
-                'allOf': [
-                    {'type': 'number'},
-                    {'minimum': 5},
-                ],
-            }
-
-        Valid values for this definition are 5, 6, 7, ... but not 4 or 'abc' for example.
-        """
-        for definition_item in self._definition['allOf']:
-            self.generate_func_code_block(definition_item, self._variable, self._variable_name, clear_variables=True)
-
-    def generate_any_of(self):
-        """
-        Means that value have to be valid by any of those definitions. It can also be valid
-        by all of them.
-
-        .. code-block:: python
-
-            {
-                'anyOf': [
-                    {'type': 'number', 'minimum': 10},
-                    {'type': 'number', 'maximum': 5},
-                ],
-            }
-
-        Valid values for this definition are 3, 4, 5, 10, 11, ... but not 8 for example.
-        """
-        self._any_or_one_of_count += 1
-        count = self._any_or_one_of_count
-        self.l('{variable}_any_of_count{count} = 0', count=count)
-        for definition_item in self._definition['anyOf']:
-            # When we know it's passing (at least once), we do not need to do another expensive try-except.
-            with self.l('if not {variable}_any_of_count{count}:', count=count, optimize=False):
-                with self.l('try:', optimize=False):
-                    self.generate_func_code_block(definition_item, self._variable, self._variable_name, clear_variables=True)
-                    self.l('{variable}_any_of_count{count} += 1', count=count)
-                self.l('except JsonSchemaValueException: pass')
-
-        with self.l('if not {variable}_any_of_count{count}:', count=count, optimize=False):
-            self.exc('{name} cannot be validated by any definition', rule='anyOf')
-
-    def generate_one_of(self):
-        """
-        Means that value have to be valid by only one of those definitions. It can't be valid
-        by two or more of them.
-
-        .. code-block:: python
-
-            {
-                'oneOf': [
-                    {'type': 'number', 'multipleOf': 3},
-                    {'type': 'number', 'multipleOf': 5},
-                ],
-            }
-
-        Valid values for this definition are 3, 5, 6, ... but not 15 for example.
-        """
-        self._any_or_one_of_count += 1
-        count = self._any_or_one_of_count
-        self.l('{variable}_one_of_count{count} = 0', count=count)
-        for definition_item in self._definition['oneOf']:
-            # When we know it's failing (one of means exactly once), we do not need to do another expensive try-except.
-            with self.l('if {variable}_one_of_count{count} < 2:', count=count, optimize=False):
-                with self.l('try:', optimize=False):
-                    self.generate_func_code_block(definition_item, self._variable, self._variable_name, clear_variables=True)
-                    self.l('{variable}_one_of_count{count} += 1', count=count)
-                self.l('except JsonSchemaValueException: pass')
-
-        with self.l('if {variable}_one_of_count{count} != 1:', count=count):
-            dynamic = '" (" + str({variable}_one_of_count{}) + " matches found)"'
-            self.exc('{name} must be valid exactly by one definition', count, append_to_msg=dynamic, rule='oneOf')
-
-    def generate_not(self):
-        """
-        Means that value have not to be valid by this definition.
-
-        .. code-block:: python
-
-            {'not': {'type': 'null'}}
-
-        Valid values for this definition are 'hello', 42, {} ... but not None.
-
-        Since draft 06 definition can be boolean. False means nothing, True
-        means everything is invalid.
-        """
-        not_definition = self._definition['not']
-        if not_definition is True:
-            self.exc('{name} must not be there', rule='not')
-        elif not_definition is False:
-            return
-        elif not not_definition:
-            with self.l('if {}:', self._variable):
-                self.exc('{name} must NOT match a disallowed definition', rule='not')
-        else:
-            with self.l('try:', optimize=False):
-                self.generate_func_code_block(not_definition, self._variable, self._variable_name)
-            self.l('except JsonSchemaValueException: pass')
-            with self.l('else:'):
-                self.exc('{name} must NOT match a disallowed definition', rule='not')
-
-    def generate_min_length(self):
-        with self.l('if isinstance({variable}, str):'):
-            self.create_variable_with_length()
-            if not isinstance(self._definition['minLength'], int):
-                raise JsonSchemaDefinitionException('minLength must be a number')
-            with self.l('if {variable}_len < {minLength}:'):
-                self.exc('{name} must be longer than or equal to {minLength} characters', rule='minLength')
-
-    def generate_max_length(self):
-        with self.l('if isinstance({variable}, str):'):
-            self.create_variable_with_length()
-            if not isinstance(self._definition['maxLength'], int):
-                raise JsonSchemaDefinitionException('maxLength must be a number')
-            with self.l('if {variable}_len > {maxLength}:'):
-                self.exc('{name} must be shorter than or equal to {maxLength} characters', rule='maxLength')
-
-    def generate_pattern(self):
-        with self.l('if isinstance({variable}, str):'):
-            pattern = self._definition['pattern']
-            safe_pattern = pattern.replace('\\', '\\\\').replace('"', '\\"')
-            end_of_string_fixed_pattern = DOLLAR_FINDER.sub(r'\\Z', pattern)
-            self._compile_regexps[pattern] = re.compile(end_of_string_fixed_pattern)
-            with self.l('if not REGEX_PATTERNS[{}].search({variable}):', repr(pattern)):
-                self.exc('{name} must match pattern {}', safe_pattern, rule='pattern')
-
-    def generate_format(self):
-        """
-        Means that value have to be in specified format. For example date, email or other.
-
-        .. code-block:: python
-
-            {'format': 'email'}
-
-        Valid value for this definition is user@example.com but not @username
-        """
-        with self.l('if isinstance({variable}, str):'):
-            format_ = self._definition['format']
-            # Checking custom formats - user is allowed to override default formats.
-            if format_ in self._custom_formats:
-                custom_format = self._custom_formats[format_]
-                if isinstance(custom_format, str):
-                    self._generate_format(format_, format_ + '_re_pattern', custom_format)
-                else:
-                    with self.l('if not custom_formats["{}"]({variable}):', format_):
-                        self.exc('{name} must be {}', format_, rule='format')
-            elif format_ in self.FORMAT_REGEXS:
-                format_regex = self.FORMAT_REGEXS[format_]
-                self._generate_format(format_, format_ + '_re_pattern', format_regex)
-            # Format regex is used only in meta schemas.
-            elif format_ == 'regex':
-                with self.l('try:', optimize=False):
-                    self.l('re.compile({variable})')
-                with self.l('except Exception:'):
-                    self.exc('{name} must be a valid regex', rule='format')
-            else:
-                raise JsonSchemaDefinitionException('Unknown format: {}'.format(format_))
-
-
-    def _generate_format(self, format_name, regexp_name, regexp):
-        if self._definition['format'] == format_name:
-            if not regexp_name in self._compile_regexps:
-                self._compile_regexps[regexp_name] = re.compile(regexp)
-            with self.l('if not REGEX_PATTERNS["{}"].match({variable}):', regexp_name):
-                self.exc('{name} must be {}', format_name, rule='format')
-
-    def generate_minimum(self):
-        with self.l('if isinstance({variable}, (int, float, Decimal)):'):
-            if not isinstance(self._definition['minimum'], (int, float, decimal.Decimal)):
-                raise JsonSchemaDefinitionException('minimum must be a number')
-            if self._definition.get('exclusiveMinimum', False):
-                with self.l('if {variable} <= {minimum}:'):
-                    self.exc('{name} must be bigger than {minimum}', rule='minimum')
-            else:
-                with self.l('if {variable} < {minimum}:'):
-                    self.exc('{name} must be bigger than or equal to {minimum}', rule='minimum')
-
-    def generate_maximum(self):
-        with self.l('if isinstance({variable}, (int, float, Decimal)):'):
-            if not isinstance(self._definition['maximum'], (int, float, decimal.Decimal)):
-                raise JsonSchemaDefinitionException('maximum must be a number')
-            if self._definition.get('exclusiveMaximum', False):
-                with self.l('if {variable} >= {maximum}:'):
-                    self.exc('{name} must be smaller than {maximum}', rule='maximum')
-            else:
-                with self.l('if {variable} > {maximum}:'):
-                    self.exc('{name} must be smaller than or equal to {maximum}', rule='maximum')
-
-    def generate_multiple_of(self):
-        with self.l('if isinstance({variable}, (int, float, Decimal)):'):
-            if not isinstance(self._definition['multipleOf'], (int, float, decimal.Decimal)):
-                raise JsonSchemaDefinitionException('multipleOf must be a number')
-            # For proper multiplication check of floats we need to use decimals,
-            # because for example 19.01 / 0.01 = 1901.0000000000002.
-            if isinstance(self._definition['multipleOf'], float):
-                self.l('quotient = Decimal(repr({variable})) / Decimal(repr({multipleOf}))')
-            else:
-                self.l('quotient = {variable} / {multipleOf}')
-            with self.l('if int(quotient) != quotient:'):
-                self.exc('{name} must be multiple of {multipleOf}', rule='multipleOf')
-
-    def generate_min_items(self):
-        self.create_variable_is_list()
-        with self.l('if {variable}_is_list:'):
-            if not isinstance(self._definition['minItems'], int):
-                raise JsonSchemaDefinitionException('minItems must be a number')
-            self.create_variable_with_length()
-            with self.l('if {variable}_len < {minItems}:'):
-                self.exc('{name} must contain at least {minItems} items', rule='minItems')
-
-    def generate_max_items(self):
-        self.create_variable_is_list()
-        with self.l('if {variable}_is_list:'):
-            if not isinstance(self._definition['maxItems'], int):
-                raise JsonSchemaDefinitionException('maxItems must be a number')
-            self.create_variable_with_length()
-            with self.l('if {variable}_len > {maxItems}:'):
-                self.exc('{name} must contain less than or equal to {maxItems} items', rule='maxItems')
-
-    def generate_unique_items(self):
-        """
-        With Python 3.4 module ``timeit`` recommended this solutions:
-
-        .. code-block:: python
-
-            >>> timeit.timeit("len(x) > len(set(x))", "x=range(100)+range(100)", number=100000)
-            0.5839540958404541
-            >>> timeit.timeit("len({}.fromkeys(x)) == len(x)", "x=range(100)+range(100)", number=100000)
-            0.7094449996948242
-            >>> timeit.timeit("seen = set(); any(i in seen or seen.add(i) for i in x)", "x=range(100)+range(100)", number=100000)
-            2.0819358825683594
-            >>> timeit.timeit("np.unique(x).size == len(x)", "x=range(100)+range(100); import numpy as np", number=100000)
-            2.1439831256866455
-        """
-        unique_definition = self._definition['uniqueItems']
-        if not unique_definition:
-            return
-
-        self.create_variable_is_list()
-        with self.l('if {variable}_is_list:'):
-            self.l(
-                'def fn(var): '
-                'return frozenset(dict((k, fn(v)) '
-                'for k, v in var.items()).items()) '
-                'if hasattr(var, "items") else tuple(fn(v) '
-                'for v in var) '
-                'if isinstance(var, (dict, list)) else str(var) '
-                'if isinstance(var, bool) else var')
-            self.create_variable_with_length()
-            with self.l('if {variable}_len > len(set(fn({variable}_x) for {variable}_x in {variable})):'):
-                self.exc('{name} must contain unique items', rule='uniqueItems')
-
-    def generate_items(self):
-        """
-        Means array is valid only when all items are valid by this definition.
-
-        .. code-block:: python
-
-            {
-                'items': [
-                    {'type': 'integer'},
-                    {'type': 'string'},
-                ],
-            }
-
-        Valid arrays are those with integers or strings, nothing else.
-
-        Since draft 06 definition can be also boolean. True means nothing, False
-        means everything is invalid.
-        """
-        items_definition = self._definition['items']
-        if items_definition is True:
-            return
-
-        self.create_variable_is_list()
-        with self.l('if {variable}_is_list:'):
-            self.create_variable_with_length()
-            if items_definition is False:
-                with self.l('if {variable}:'):
-                    self.exc('{name} must not be there', rule='items')
-            elif isinstance(items_definition, list):
-                for idx, item_definition in enumerate(items_definition):
-                    with self.l('if {variable}_len > {}:', idx):
-                        self.l('{variable}__{0} = {variable}[{0}]', idx)
-                        self.generate_func_code_block(
-                            item_definition,
-                            '{}__{}'.format(self._variable, idx),
-                            '{}[{}]'.format(self._variable_name, idx),
-                        )
-                    if self._use_default and isinstance(item_definition, dict) and 'default' in item_definition:
-                        self.l('else: {variable}.append({})', repr(item_definition['default']))
-
-                if 'additionalItems' in self._definition:
-                    if self._definition['additionalItems'] is False:
-                        with self.l('if {variable}_len > {}:', len(items_definition)):
-                            self.exc('{name} must contain only specified items', rule='items')
-                    else:
-                        with self.l('for {variable}_x, {variable}_item in enumerate({variable}[{0}:], {0}):', len(items_definition)):
-                            count = self.generate_func_code_block(
-                                self._definition['additionalItems'],
-                                '{}_item'.format(self._variable),
-                                '{}[{{{}_x}}]'.format(self._variable_name, self._variable),
-                            )
-                            if count == 0:
-                                self.l('pass')
-            else:
-                if items_definition:
-                    with self.l('for {variable}_x, {variable}_item in enumerate({variable}):'):
-                        count = self.generate_func_code_block(
-                            items_definition,
-                            '{}_item'.format(self._variable),
-                            '{}[{{{}_x}}]'.format(self._variable_name, self._variable),
-                        )
-                        if count == 0:
-                            self.l('pass')
-
-    def generate_min_properties(self):
-        self.create_variable_is_dict()
-        with self.l('if {variable}_is_dict:'):
-            if not isinstance(self._definition['minProperties'], int):
-                raise JsonSchemaDefinitionException('minProperties must be a number')
-            self.create_variable_with_length()
-            with self.l('if {variable}_len < {minProperties}:'):
-                self.exc('{name} must contain at least {minProperties} properties', rule='minProperties')
-
-    def generate_max_properties(self):
-        self.create_variable_is_dict()
-        with self.l('if {variable}_is_dict:'):
-            if not isinstance(self._definition['maxProperties'], int):
-                raise JsonSchemaDefinitionException('maxProperties must be a number')
-            self.create_variable_with_length()
-            with self.l('if {variable}_len > {maxProperties}:'):
-                self.exc('{name} must contain less than or equal to {maxProperties} properties', rule='maxProperties')
-
-    def generate_required(self):
-        self.create_variable_is_dict()
-        with self.l('if {variable}_is_dict:'):
-            if not isinstance(self._definition['required'], (list, tuple)):
-                raise JsonSchemaDefinitionException('required must be an array')
-            self.l('{variable}__missing_keys = set({required}) - {variable}.keys()')
-            with self.l('if {variable}__missing_keys:'):
-                dynamic = 'str(sorted({variable}__missing_keys)) + " properties"'
-                self.exc('{name} must contain ', self.e(self._definition['required']), rule='required', append_to_msg=dynamic)
-
-    def generate_properties(self):
-        """
-        Means object with defined keys.
-
-        .. code-block:: python
-
-            {
-                'properties': {
-                    'key': {'type': 'number'},
-                },
-            }
-
-        Valid object is containing key called 'key' and value any number.
-        """
-        self.create_variable_is_dict()
-        with self.l('if {variable}_is_dict:'):
-            self.create_variable_keys()
-            for key, prop_definition in self._definition['properties'].items():
-                key_name = re.sub(r'($[^a-zA-Z]|[^a-zA-Z0-9])', '', key)
-                if not isinstance(prop_definition, (dict, bool)):
-                    raise JsonSchemaDefinitionException('{}[{}] must be object'.format(self._variable, key_name))
-                with self.l('if "{}" in {variable}_keys:', self.e(key)):
-                    self.l('{variable}_keys.remove("{}")', self.e(key))
-                    self.l('{variable}__{0} = {variable}["{1}"]', key_name, self.e(key))
-                    self.generate_func_code_block(
-                        prop_definition,
-                        '{}__{}'.format(self._variable, key_name),
-                        '{}.{}'.format(self._variable_name, self.e(key)),
-                        clear_variables=True,
-                    )
-                if self._use_default and isinstance(prop_definition, dict) and 'default' in prop_definition:
-                    self.l('else: {variable}["{}"] = {}', self.e(key), repr(prop_definition['default']))
-
-    def generate_pattern_properties(self):
-        """
-        Means object with defined keys as patterns.
-
-        .. code-block:: python
-
-            {
-                'patternProperties': {
-                    '^x': {'type': 'number'},
-                },
-            }
-
-        Valid object is containing key starting with a 'x' and value any number.
-        """
-        self.create_variable_is_dict()
-        with self.l('if {variable}_is_dict:'):
-            self.create_variable_keys()
-            for pattern, definition in self._definition['patternProperties'].items():
-                self._compile_regexps[pattern] = re.compile(pattern)
-            with self.l('for {variable}_key, {variable}_val in {variable}.items():'):
-                for pattern, definition in self._definition['patternProperties'].items():
-                    with self.l('if REGEX_PATTERNS[{}].search({variable}_key):', repr(pattern)):
-                        with self.l('if {variable}_key in {variable}_keys:'):
-                            self.l('{variable}_keys.remove({variable}_key)')
-                        self.generate_func_code_block(
-                            definition,
-                            '{}_val'.format(self._variable),
-                            '{}.{{{}_key}}'.format(self._variable_name, self._variable),
-                            clear_variables=True,
-                        )
-
-    def generate_additional_properties(self):
-        """
-        Means object with keys with values defined by definition.
-
-        .. code-block:: python
-
-            {
-                'properties': {
-                    'key': {'type': 'number'},
-                }
-                'additionalProperties': {'type': 'string'},
-            }
-
-        Valid object is containing key called 'key' and it's value any number and
-        any other key with any string.
-        """
-        self.create_variable_is_dict()
-        with self.l('if {variable}_is_dict:'):
-            self.create_variable_keys()
-            add_prop_definition = self._definition["additionalProperties"]
-            if add_prop_definition is True or add_prop_definition == {}:
-                return
-            if add_prop_definition:
-                properties_keys = list(self._definition.get("properties", {}).keys())
-                with self.l('for {variable}_key in {variable}_keys:'):
-                    with self.l('if {variable}_key not in {}:', properties_keys):
-                        self.l('{variable}_value = {variable}.get({variable}_key)')
-                        self.generate_func_code_block(
-                            add_prop_definition,
-                            '{}_value'.format(self._variable),
-                            '{}.{{{}_key}}'.format(self._variable_name, self._variable),
-                        )
-            else:
-                with self.l('if {variable}_keys:'):
-                    self.exc('{name} must not contain "+str({variable}_keys)+" properties', rule='additionalProperties')
-
-    def generate_dependencies(self):
-        """
-        Means when object has property, it needs to have also other property.
-
-        .. code-block:: python
-
-            {
-                'dependencies': {
-                    'bar': ['foo'],
-                },
-            }
-
-        Valid object is containing only foo, both bar and foo or none of them, but not
-        object with only bar.
-
-        Since draft 06 definition can be boolean or empty array. True and empty array
-        means nothing, False means that key cannot be there at all.
-        """
-        self.create_variable_is_dict()
-        with self.l('if {variable}_is_dict:'):
-            is_empty = True
-            for key, values in self._definition["dependencies"].items():
-                if values == [] or values is True:
-                    continue
-                is_empty = False
-                with self.l('if "{}" in {variable}:', self.e(key)):
-                    if values is False:
-                        self.exc('{} in {name} must not be there', key, rule='dependencies')
-                    elif isinstance(values, list):
-                        for value in values:
-                            with self.l('if "{}" not in {variable}:', self.e(value)):
-                                self.exc('{name} missing dependency {} for {}', self.e(value), self.e(key), rule='dependencies')
-                    else:
-                        self.generate_func_code_block(values, self._variable, self._variable_name, clear_variables=True)
-            if is_empty:
-                self.l('pass')
diff --git a/src/poetry/core/_vendor/fastjsonschema/draft06.py b/src/poetry/core/_vendor/fastjsonschema/draft06.py
deleted file mode 100644
index f5aca06..0000000
--- a/src/poetry/core/_vendor/fastjsonschema/draft06.py
+++ /dev/null
@@ -1,188 +0,0 @@
-import decimal
-from .draft04 import CodeGeneratorDraft04, JSON_TYPE_TO_PYTHON_TYPE
-from .exceptions import JsonSchemaDefinitionException
-from .generator import enforce_list
-
-
-class CodeGeneratorDraft06(CodeGeneratorDraft04):
-    FORMAT_REGEXS = dict(CodeGeneratorDraft04.FORMAT_REGEXS, **{
-        'json-pointer': r'^(/(([^/~])|(~[01]))*)*\Z',
-        'uri-reference': r'^(\w+:(\/?\/?))?[^#\\\s]*(#[^\\\s]*)?\Z',
-        'uri-template': (
-            r'^(?:(?:[^\x00-\x20\"\'<>%\\^`{|}]|%[0-9a-f]{2})|'
-            r'\{[+#./;?&=,!@|]?(?:[a-z0-9_]|%[0-9a-f]{2})+'
-            r'(?::[1-9][0-9]{0,3}|\*)?(?:,(?:[a-z0-9_]|%[0-9a-f]{2})+'
-            r'(?::[1-9][0-9]{0,3}|\*)?)*\})*\Z'
-        ),
-    })
-
-    def __init__(self, definition, resolver=None, formats={}, use_default=True):
-        super().__init__(definition, resolver, formats, use_default)
-        self._json_keywords_to_function.update((
-            ('exclusiveMinimum', self.generate_exclusive_minimum),
-            ('exclusiveMaximum', self.generate_exclusive_maximum),
-            ('propertyNames', self.generate_property_names),
-            ('contains', self.generate_contains),
-            ('const', self.generate_const),
-        ))
-
-    def _generate_func_code_block(self, definition):
-        if isinstance(definition, bool):
-            self.generate_boolean_schema()
-        elif '$ref' in definition:
-            # needed because ref overrides any sibling keywords
-            self.generate_ref()
-        else:
-            self.run_generate_functions(definition)
-
-    def generate_boolean_schema(self):
-        """
-        Means that schema can be specified by boolean.
-        True means everything is valid, False everything is invalid.
-        """
-        if self._definition is True:
-            self.l('pass')
-        if self._definition is False:
-            self.exc('{name} must not be there')
-
-    def generate_type(self):
-        """
-        Validation of type. Can be one type or list of types.
-
-        Since draft 06 a float without fractional part is an integer.
-
-        .. code-block:: python
-
-            {'type': 'string'}
-            {'type': ['string', 'number']}
-        """
-        types = enforce_list(self._definition['type'])
-        try:
-            python_types = ', '.join(JSON_TYPE_TO_PYTHON_TYPE[t] for t in types)
-        except KeyError as exc:
-            raise JsonSchemaDefinitionException('Unknown type: {}'.format(exc))
-
-        extra = ''
-
-        if 'integer' in types:
-            extra += ' and not (isinstance({variable}, float) and {variable}.is_integer())'.format(
-                variable=self._variable,
-            )
-
-        if ('number' in types or 'integer' in types) and 'boolean' not in types:
-            extra += ' or isinstance({variable}, bool)'.format(variable=self._variable)
-
-        with self.l('if not isinstance({variable}, ({})){}:', python_types, extra):
-            self.exc('{name} must be {}', ' or '.join(types), rule='type')
-
-    def generate_exclusive_minimum(self):
-        with self.l('if isinstance({variable}, (int, float, Decimal)):'):
-            if not isinstance(self._definition['exclusiveMinimum'], (int, float, decimal.Decimal)):
-                raise JsonSchemaDefinitionException('exclusiveMinimum must be an integer, a float or a decimal')
-            with self.l('if {variable} <= {exclusiveMinimum}:'):
-                self.exc('{name} must be bigger than {exclusiveMinimum}', rule='exclusiveMinimum')
-
-    def generate_exclusive_maximum(self):
-        with self.l('if isinstance({variable}, (int, float, Decimal)):'):
-            if not isinstance(self._definition['exclusiveMaximum'], (int, float, decimal.Decimal)):
-                raise JsonSchemaDefinitionException('exclusiveMaximum must be an integer, a float or a decimal')
-            with self.l('if {variable} >= {exclusiveMaximum}:'):
-                self.exc('{name} must be smaller than {exclusiveMaximum}', rule='exclusiveMaximum')
-
-    def generate_property_names(self):
-        """
-        Means that keys of object must to follow this definition.
-
-        .. code-block:: python
-
-            {
-                'propertyNames': {
-                    'maxLength': 3,
-                },
-            }
-
-        Valid keys of object for this definition are foo, bar, ... but not foobar for example.
-        """
-        property_names_definition = self._definition.get('propertyNames', {})
-        if property_names_definition is True:
-            pass
-        elif property_names_definition is False:
-            self.create_variable_keys()
-            with self.l('if {variable}_keys:'):
-                self.exc('{name} must not be there', rule='propertyNames')
-        else:
-            self.create_variable_is_dict()
-            with self.l('if {variable}_is_dict:'):
-                self.create_variable_with_length()
-                with self.l('if {variable}_len != 0:'):
-                    self.l('{variable}_property_names = True')
-                    with self.l('for {variable}_key in {variable}:'):
-                        with self.l('try:'):
-                            self.generate_func_code_block(
-                                property_names_definition,
-                                '{}_key'.format(self._variable),
-                                self._variable_name,
-                                clear_variables=True,
-                            )
-                        with self.l('except JsonSchemaValueException:'):
-                            self.l('{variable}_property_names = False')
-                    with self.l('if not {variable}_property_names:'):
-                        self.exc('{name} must be named by propertyName definition', rule='propertyNames')
-
-    def generate_contains(self):
-        """
-        Means that array must contain at least one defined item.
-
-        .. code-block:: python
-
-            {
-                'contains': {
-                    'type': 'number',
-                },
-            }
-
-        Valid array is any with at least one number.
-        """
-        self.create_variable_is_list()
-        with self.l('if {variable}_is_list:'):
-            contains_definition = self._definition['contains']
-
-            if contains_definition is False:
-                self.exc('{name} is always invalid', rule='contains')
-            elif contains_definition is True:
-                with self.l('if not {variable}:'):
-                    self.exc('{name} must not be empty', rule='contains')
-            else:
-                self.l('{variable}_contains = False')
-                with self.l('for {variable}_key in {variable}:'):
-                    with self.l('try:'):
-                        self.generate_func_code_block(
-                            contains_definition,
-                            '{}_key'.format(self._variable),
-                            self._variable_name,
-                            clear_variables=True,
-                        )
-                        self.l('{variable}_contains = True')
-                        self.l('break')
-                    self.l('except JsonSchemaValueException: pass')
-
-                with self.l('if not {variable}_contains:'):
-                    self.exc('{name} must contain one of contains definition', rule='contains')
-
-    def generate_const(self):
-        """
-        Means that value is valid when is equeal to const definition.
-
-        .. code-block:: python
-
-            {
-                'const': 42,
-            }
-
-        Only valid value is 42 in this example.
-        """
-        const = self._definition['const']
-        if isinstance(const, str):
-            const = '"{}"'.format(self.e(const))
-        with self.l('if {variable} != {}:', const):
-            self.exc('{name} must be same as const definition: {definition_rule}', rule='const')
diff --git a/src/poetry/core/_vendor/fastjsonschema/draft07.py b/src/poetry/core/_vendor/fastjsonschema/draft07.py
deleted file mode 100644
index 470e23b..0000000
--- a/src/poetry/core/_vendor/fastjsonschema/draft07.py
+++ /dev/null
@@ -1,116 +0,0 @@
-from .draft06 import CodeGeneratorDraft06
-
-
-class CodeGeneratorDraft07(CodeGeneratorDraft06):
-    FORMAT_REGEXS = dict(CodeGeneratorDraft06.FORMAT_REGEXS, **{
-        'date': r'^(?P<year>\d{4})-(?P<month>\d{1,2})-(?P<day>\d{1,2})\Z',
-        'iri': r'^\w+:(\/?\/?)[^\s]+\Z',
-        'iri-reference': r'^(\w+:(\/?\/?))?[^#\\\s]*(#[^\\\s]*)?\Z',
-        'idn-email': r'^[^@]+@[^@]+\.[^@]+\Z',
-        #'idn-hostname': r'',
-        'relative-json-pointer': r'^(?:0|[1-9][0-9]*)(?:#|(?:\/(?:[^~/]|~0|~1)*)*)\Z',
-        #'regex': r'',
-        'time': (
-            r'^(?P<hour>\d{1,2}):(?P<minute>\d{1,2})'
-            r'(?::(?P<second>\d{1,2})(?:\.(?P<microsecond>\d{1,6}))?'
-            r'([zZ]|[+-]\d\d:\d\d)?)?\Z'
-        ),
-    })
-
-    def __init__(self, definition, resolver=None, formats={}, use_default=True):
-        super().__init__(definition, resolver, formats, use_default)
-        # pylint: disable=duplicate-code
-        self._json_keywords_to_function.update((
-            ('if', self.generate_if_then_else),
-            ('contentEncoding', self.generate_content_encoding),
-            ('contentMediaType', self.generate_content_media_type),
-        ))
-
-    def generate_if_then_else(self):
-        """
-        Implementation of if-then-else.
-
-        .. code-block:: python
-
-            {
-                'if': {
-                    'exclusiveMaximum': 0,
-                },
-                'then': {
-                    'minimum': -10,
-                },
-                'else': {
-                    'multipleOf': 2,
-                },
-            }
-
-        Valid values are any between -10 and 0 or any multiplication of two.
-        """
-        with self.l('try:', optimize=False):
-            self.generate_func_code_block(
-                self._definition['if'],
-                self._variable,
-                self._variable_name,
-                clear_variables=True
-            )
-        with self.l('except JsonSchemaValueException:'):
-            if 'else' in self._definition:
-                self.generate_func_code_block(
-                    self._definition['else'],
-                    self._variable,
-                    self._variable_name,
-                    clear_variables=True
-                )
-            else:
-                self.l('pass')
-        if 'then' in self._definition:
-            with self.l('else:'):
-                self.generate_func_code_block(
-                    self._definition['then'],
-                    self._variable,
-                    self._variable_name,
-                    clear_variables=True
-                )
-
-    def generate_content_encoding(self):
-        """
-        Means decoding value when it's encoded by base64.
-
-        .. code-block:: python
-
-            {
-                'contentEncoding': 'base64',
-            }
-        """
-        if self._definition['contentEncoding'] == 'base64':
-            with self.l('if isinstance({variable}, str):'):
-                with self.l('try:'):
-                    self.l('import base64')
-                    self.l('{variable} = base64.b64decode({variable})')
-                with self.l('except Exception:'):
-                    self.exc('{name} must be encoded by base64')
-                with self.l('if {variable} == "":'):
-                    self.exc('contentEncoding must be base64')
-
-    def generate_content_media_type(self):
-        """
-        Means loading value when it's specified as JSON.
-
-        .. code-block:: python
-
-            {
-                'contentMediaType': 'application/json',
-            }
-        """
-        if self._definition['contentMediaType'] == 'application/json':
-            with self.l('if isinstance({variable}, bytes):'):
-                with self.l('try:'):
-                    self.l('{variable} = {variable}.decode("utf-8")')
-                with self.l('except Exception:'):
-                    self.exc('{name} must encoded by utf8')
-            with self.l('if isinstance({variable}, str):'):
-                with self.l('try:'):
-                    self.l('import json')
-                    self.l('{variable} = json.loads({variable})')
-                with self.l('except Exception:'):
-                    self.exc('{name} must be valid JSON')
diff --git a/src/poetry/core/_vendor/fastjsonschema/exceptions.py b/src/poetry/core/_vendor/fastjsonschema/exceptions.py
deleted file mode 100644
index d2dddd6..0000000
--- a/src/poetry/core/_vendor/fastjsonschema/exceptions.py
+++ /dev/null
@@ -1,51 +0,0 @@
-import re
-
-
-SPLIT_RE = re.compile(r'[\.\[\]]+')
-
-
-class JsonSchemaException(ValueError):
-    """
-    Base exception of ``fastjsonschema`` library.
-    """
-
-
-class JsonSchemaValueException(JsonSchemaException):
-    """
-    Exception raised by validation function. Available properties:
-
-     * ``message`` containing human-readable information what is wrong (e.g. ``data.property[index] must be smaller than or equal to 42``),
-     * invalid ``value`` (e.g. ``60``),
-     * ``name`` of a path in the data structure (e.g. ``data.property[index]``),
-     * ``path`` as an array in the data structure (e.g. ``['data', 'property', 'index']``),
-     * the whole ``definition`` which the ``value`` has to fulfil (e.g. ``{'type': 'number', 'maximum': 42}``),
-     * ``rule`` which the ``value`` is breaking (e.g. ``maximum``)
-     * and ``rule_definition`` (e.g. ``42``).
-
-    .. versionchanged:: 2.14.0
-        Added all extra properties.
-    """
-
-    def __init__(self, message, value=None, name=None, definition=None, rule=None):
-        super().__init__(message)
-        self.message = message
-        self.value = value
-        self.name = name
-        self.definition = definition
-        self.rule = rule
-
-    @property
-    def path(self):
-        return [item for item in SPLIT_RE.split(self.name) if item != '']
-
-    @property
-    def rule_definition(self):
-        if not self.rule or not self.definition:
-            return None
-        return self.definition.get(self.rule)
-
-
-class JsonSchemaDefinitionException(JsonSchemaException):
-    """
-    Exception raised by generator of validation function.
-    """
diff --git a/src/poetry/core/_vendor/fastjsonschema/generator.py b/src/poetry/core/_vendor/fastjsonschema/generator.py
deleted file mode 100644
index a7f96c5..0000000
--- a/src/poetry/core/_vendor/fastjsonschema/generator.py
+++ /dev/null
@@ -1,348 +0,0 @@
-from collections import OrderedDict
-from decimal import Decimal
-import re
-
-from .exceptions import JsonSchemaValueException, JsonSchemaDefinitionException
-from .indent import indent
-from .ref_resolver import RefResolver
-
-
-def enforce_list(variable):
-    if isinstance(variable, list):
-        return variable
-    return [variable]
-
-
-# pylint: disable=too-many-instance-attributes,too-many-public-methods
-class CodeGenerator:
-    """
-    This class is not supposed to be used directly. Anything
-    inside of this class can be changed without noticing.
-
-    This class generates code of validation function from JSON
-    schema object as string. Example:
-
-    .. code-block:: python
-
-        CodeGenerator(json_schema_definition).func_code
-    """
-
-    INDENT = 4  # spaces
-
-    def __init__(self, definition, resolver=None):
-        self._code = []
-        self._compile_regexps = {}
-        self._custom_formats = {}
-
-        # Any extra library should be here to be imported only once.
-        # Lines are imports to be printed in the file and objects
-        # key-value pair to pass to compile function directly.
-        self._extra_imports_lines = [
-            "from decimal import Decimal",
-        ]
-        self._extra_imports_objects = {
-            "Decimal": Decimal,
-        }
-
-        self._variables = set()
-        self._indent = 0
-        self._indent_last_line = None
-        self._variable = None
-        self._variable_name = None
-        self._root_definition = definition
-        self._definition = None
-
-        # map schema URIs to validation function names for functions
-        # that are not yet generated, but need to be generated
-        self._needed_validation_functions = {}
-        # validation function names that are already done
-        self._validation_functions_done = set()
-
-        if resolver is None:
-            resolver = RefResolver.from_schema(definition, store={})
-        self._resolver = resolver
-
-        # add main function to `self._needed_validation_functions`
-        self._needed_validation_functions[self._resolver.get_uri()] = self._resolver.get_scope_name()
-
-        self._json_keywords_to_function = OrderedDict()
-
-    @property
-    def func_code(self):
-        """
-        Returns generated code of whole validation function as string.
-        """
-        self._generate_func_code()
-
-        return '\n'.join(self._code)
-
-    @property
-    def global_state(self):
-        """
-        Returns global variables for generating function from ``func_code``. Includes
-        compiled regular expressions and imports, so it does not have to do it every
-        time when validation function is called.
-        """
-        self._generate_func_code()
-
-        return dict(
-            **self._extra_imports_objects,
-            REGEX_PATTERNS=self._compile_regexps,
-            re=re,
-            JsonSchemaValueException=JsonSchemaValueException,
-        )
-
-    @property
-    def global_state_code(self):
-        """
-        Returns global variables for generating function from ``func_code`` as code.
-        Includes compiled regular expressions and imports.
-        """
-        self._generate_func_code()
-
-        if not self._compile_regexps:
-            return '\n'.join(self._extra_imports_lines + [
-                'from fastjsonschema import JsonSchemaValueException',
-                '',
-                '',
-            ])
-        return '\n'.join(self._extra_imports_lines + [
-            'import re',
-            'from fastjsonschema import JsonSchemaValueException',
-            '',
-            '',
-            'REGEX_PATTERNS = ' + serialize_regexes(self._compile_regexps),
-            '',
-        ])
-
-
-    def _generate_func_code(self):
-        if not self._code:
-            self.generate_func_code()
-
-    def generate_func_code(self):
-        """
-        Creates base code of validation function and calls helper
-        for creating code by definition.
-        """
-        self.l('NoneType = type(None)')
-        # Generate parts that are referenced and not yet generated
-        while self._needed_validation_functions:
-            # During generation of validation function, could be needed to generate
-            # new one that is added again to `_needed_validation_functions`.
-            # Therefore usage of while instead of for loop.
-            uri, name = self._needed_validation_functions.popitem()
-            self.generate_validation_function(uri, name)
-
-    def generate_validation_function(self, uri, name):
-        """
-        Generate validation function for given uri with given name
-        """
-        self._validation_functions_done.add(uri)
-        self.l('')
-        with self._resolver.resolving(uri) as definition:
-            with self.l('def {}(data, custom_formats={{}}, name_prefix=None):', name):
-                self.generate_func_code_block(definition, 'data', 'data', clear_variables=True)
-                self.l('return data')
-
-    def generate_func_code_block(self, definition, variable, variable_name, clear_variables=False):
-        """
-        Creates validation rules for current definition.
-
-        Returns the number of validation rules generated as code.
-        """
-        backup = self._definition, self._variable, self._variable_name
-        self._definition, self._variable, self._variable_name = definition, variable, variable_name
-        if clear_variables:
-            backup_variables = self._variables
-            self._variables = set()
-
-        count = self._generate_func_code_block(definition)
-
-        self._definition, self._variable, self._variable_name = backup
-        if clear_variables:
-            self._variables = backup_variables
-
-        return count
-
-    def _generate_func_code_block(self, definition):
-        if not isinstance(definition, dict):
-            raise JsonSchemaDefinitionException("definition must be an object")
-        if '$ref' in definition:
-            # needed because ref overrides any sibling keywords
-            return self.generate_ref()
-        else:
-            return self.run_generate_functions(definition)
-
-    def run_generate_functions(self, definition):
-        """Returns the number of generate functions that were executed."""
-        count = 0
-        for key, func in self._json_keywords_to_function.items():
-            if key in definition:
-                func()
-                count += 1
-        return count
-
-    def generate_ref(self):
-        """
-        Ref can be link to remote or local definition.
-
-        .. code-block:: python
-
-            {'$ref': 'http://json-schema.org/draft-04/schema#'}
-            {
-                'properties': {
-                    'foo': {'type': 'integer'},
-                    'bar': {'$ref': '#/properties/foo'}
-                }
-            }
-        """
-        with self._resolver.in_scope(self._definition['$ref']):
-            name = self._resolver.get_scope_name()
-            uri = self._resolver.get_uri()
-            if uri not in self._validation_functions_done:
-                self._needed_validation_functions[uri] = name
-            # call validation function
-            assert self._variable_name.startswith("data")
-            path = self._variable_name[4:]
-            name_arg = '(name_prefix or "data") + "{}"'.format(path)
-            if '{' in name_arg:
-                name_arg = name_arg + '.format(**locals())'
-            self.l('{}({variable}, custom_formats, {name_arg})', name, name_arg=name_arg)
-
-
-    # pylint: disable=invalid-name
-    @indent
-    def l(self, line, *args, **kwds):
-        """
-        Short-cut of line. Used for inserting line. It's formated with parameters
-        ``variable``, ``variable_name`` (as ``name`` for short-cut), all keys from
-        current JSON schema ``definition`` and also passed arguments in ``args``
-        and named ``kwds``.
-
-        .. code-block:: python
-
-            self.l('if {variable} not in {enum}: raise JsonSchemaValueException("Wrong!")')
-
-        When you want to indent block, use it as context manager. For example:
-
-        .. code-block:: python
-
-            with self.l('if {variable} not in {enum}:'):
-                self.l('raise JsonSchemaValueException("Wrong!")')
-        """
-        spaces = ' ' * self.INDENT * self._indent
-
-        name = self._variable_name
-        if name:
-            # Add name_prefix to the name when it is being outputted.
-            assert name.startswith('data')
-            name = '" + (name_prefix or "data") + "' + name[4:]
-            if '{' in name:
-                name = name + '".format(**locals()) + "'
-
-        context = dict(
-            self._definition if self._definition and self._definition is not True else {},
-            variable=self._variable,
-            name=name,
-            **kwds
-        )
-        line = line.format(*args, **context)
-        line = line.replace('\n', '\\n').replace('\r', '\\r')
-        self._code.append(spaces + line)
-        return line
-
-    def e(self, string):
-        """
-        Short-cut of escape. Used for inserting user values into a string message.
-
-        .. code-block:: python
-
-            self.l('raise JsonSchemaValueException("Variable: {}")', self.e(variable))
-        """
-        return str(string).replace('"', '\\"')
-
-    def exc(self, msg, *args, append_to_msg=None, rule=None):
-        """
-        Short-cut for creating raising exception in the code.
-        """
-        arg = '"'+msg+'"'
-        if append_to_msg:
-            arg += ' + (' + append_to_msg + ')'
-        msg = 'raise JsonSchemaValueException('+arg+', value={variable}, name="{name}", definition={definition}, rule={rule})'
-        definition = self._expand_refs(self._definition)
-        definition_rule = self.e(definition.get(rule) if isinstance(definition, dict) else None)
-        self.l(msg, *args, definition=repr(definition), rule=repr(rule), definition_rule=definition_rule)
-
-    def _expand_refs(self, definition):
-        if isinstance(definition, list):
-            return [self._expand_refs(v) for v in definition]
-        if not isinstance(definition, dict):
-            return definition
-        if "$ref" in definition and isinstance(definition["$ref"], str):
-            with self._resolver.resolving(definition["$ref"]) as schema:
-                return schema
-        return {k: self._expand_refs(v) for k, v in definition.items()}
-
-    def create_variable_with_length(self):
-        """
-        Append code for creating variable with length of that variable
-        (for example length of list or dictionary) with name ``{variable}_len``.
-        It can be called several times and always it's done only when that variable
-        still does not exists.
-        """
-        variable_name = '{}_len'.format(self._variable)
-        if variable_name in self._variables:
-            return
-        self._variables.add(variable_name)
-        self.l('{variable}_len = len({variable})')
-
-    def create_variable_keys(self):
-        """
-        Append code for creating variable with keys of that variable (dictionary)
-        with a name ``{variable}_keys``. Similar to `create_variable_with_length`.
-        """
-        variable_name = '{}_keys'.format(self._variable)
-        if variable_name in self._variables:
-            return
-        self._variables.add(variable_name)
-        self.l('{variable}_keys = set({variable}.keys())')
-
-    def create_variable_is_list(self):
-        """
-        Append code for creating variable with bool if it's instance of list
-        with a name ``{variable}_is_list``. Similar to `create_variable_with_length`.
-        """
-        variable_name = '{}_is_list'.format(self._variable)
-        if variable_name in self._variables:
-            return
-        self._variables.add(variable_name)
-        self.l('{variable}_is_list = isinstance({variable}, (list, tuple))')
-
-    def create_variable_is_dict(self):
-        """
-        Append code for creating variable with bool if it's instance of list
-        with a name ``{variable}_is_dict``. Similar to `create_variable_with_length`.
-        """
-        variable_name = '{}_is_dict'.format(self._variable)
-        if variable_name in self._variables:
-            return
-        self._variables.add(variable_name)
-        self.l('{variable}_is_dict = isinstance({variable}, dict)')
-
-
-def serialize_regexes(patterns_dict):
-    # Unfortunately using `pprint.pformat` is causing errors
-    # specially with big regexes
-    regex_patterns = (
-        repr(k) + ": " + repr_regex(v)
-        for k, v in patterns_dict.items()
-    )
-    return '{\n    ' + ",\n    ".join(regex_patterns) + "\n}"
-
-
-def repr_regex(regex):
-    all_flags = ("A", "I", "DEBUG", "L", "M", "S", "X")
-    flags = " | ".join(f"re.{f}" for f in all_flags if regex.flags & getattr(re, f))
-    flags = ", " + flags if flags else ""
-    return "re.compile({!r}{})".format(regex.pattern, flags)
diff --git a/src/poetry/core/_vendor/fastjsonschema/indent.py b/src/poetry/core/_vendor/fastjsonschema/indent.py
deleted file mode 100644
index 411c69f..0000000
--- a/src/poetry/core/_vendor/fastjsonschema/indent.py
+++ /dev/null
@@ -1,28 +0,0 @@
-def indent(func):
-    """
-    Decorator for allowing to use method as normal method or with
-    context manager for auto-indenting code blocks.
-    """
-    def wrapper(self, line, *args, optimize=True, **kwds):
-        last_line = self._indent_last_line
-        line = func(self, line, *args, **kwds)
-        # When two blocks have the same condition (such as value has to be dict),
-        # do the check only once and keep it under one block.
-        if optimize and last_line == line:
-            self._code.pop()
-        self._indent_last_line = line
-        return Indent(self, line)
-    return wrapper
-
-
-class Indent:
-    def __init__(self, instance, line):
-        self.instance = instance
-        self.line = line
-
-    def __enter__(self):
-        self.instance._indent += 1
-
-    def __exit__(self, type_, value, traceback):
-        self.instance._indent -= 1
-        self.instance._indent_last_line = self.line
diff --git a/src/poetry/core/_vendor/fastjsonschema/ref_resolver.py b/src/poetry/core/_vendor/fastjsonschema/ref_resolver.py
deleted file mode 100644
index b94813a..0000000
--- a/src/poetry/core/_vendor/fastjsonschema/ref_resolver.py
+++ /dev/null
@@ -1,176 +0,0 @@
-"""
-JSON Schema URI resolution scopes and dereferencing
-
-https://tools.ietf.org/id/draft-zyp-json-schema-04.html#rfc.section.7
-
-Code adapted from https://github.com/Julian/jsonschema
-"""
-
-import contextlib
-import json
-import re
-from urllib import parse as urlparse
-from urllib.parse import unquote
-
-from .exceptions import JsonSchemaDefinitionException
-
-
-def get_id(schema):
-    """
-    Originally ID was `id` and since v7 it's `$id`.
-    """
-    return schema.get('$id', schema.get('id', ''))
-
-
-def resolve_path(schema, fragment):
-    """
-    Return definition from path.
-
-    Path is unescaped according https://tools.ietf.org/html/rfc6901
-    """
-    fragment = fragment.lstrip('/')
-    parts = unquote(fragment).split('/') if fragment else []
-    for part in parts:
-        part = part.replace('~1', '/').replace('~0', '~')
-        if isinstance(schema, list):
-            schema = schema[int(part)]
-        elif part in schema:
-            schema = schema[part]
-        else:
-            raise JsonSchemaDefinitionException('Unresolvable ref: {}'.format(part))
-    return schema
-
-
-def normalize(uri):
-    return urlparse.urlsplit(uri).geturl()
-
-
-def resolve_remote(uri, handlers):
-    """
-    Resolve a remote ``uri``.
-
-    .. note::
-
-        urllib library is used to fetch requests from the remote ``uri``
-        if handlers does notdefine otherwise.
-    """
-    scheme = urlparse.urlsplit(uri).scheme
-    if scheme in handlers:
-        result = handlers[scheme](uri)
-    else:
-        from urllib.request import urlopen
-
-        req = urlopen(uri)
-        encoding = req.info().get_content_charset() or 'utf-8'
-        try:
-            result = json.loads(req.read().decode(encoding),)
-        except ValueError as exc:
-            raise JsonSchemaDefinitionException('{} failed to decode: {}'.format(uri, exc))
-    return result
-
-
-class RefResolver:
-    """
-    Resolve JSON References.
-    """
-
-    # pylint: disable=dangerous-default-value,too-many-arguments
-    def __init__(self, base_uri, schema, store={}, cache=True, handlers={}):
-        """
-        `base_uri` is URI of the referring document from the `schema`.
-        `store` is an dictionary that will be used to cache the fetched schemas
-        (if `cache=True`).
-
-        Please notice that you can have caching problems when compiling schemas
-        with colliding `$ref`. To force overwriting use `cache=False` or
-        explicitly pass the `store` argument (with a brand new dictionary)
-        """
-        self.base_uri = base_uri
-        self.resolution_scope = base_uri
-        self.schema = schema
-        self.store = store
-        self.cache = cache
-        self.handlers = handlers
-        self.walk(schema)
-
-    @classmethod
-    def from_schema(cls, schema, handlers={}, **kwargs):
-        """
-        Construct a resolver from a JSON schema object.
-        """
-        return cls(
-            get_id(schema) if isinstance(schema, dict) else '',
-            schema,
-            handlers=handlers,
-            **kwargs
-        )
-
-    @contextlib.contextmanager
-    def in_scope(self, scope: str):
-        """
-        Context manager to handle current scope.
-        """
-        old_scope = self.resolution_scope
-        self.resolution_scope = urlparse.urljoin(old_scope, scope)
-        try:
-            yield
-        finally:
-            self.resolution_scope = old_scope
-
-    @contextlib.contextmanager
-    def resolving(self, ref: str):
-        """
-        Context manager which resolves a JSON ``ref`` and enters the
-        resolution scope of this ref.
-        """
-        new_uri = urlparse.urljoin(self.resolution_scope, ref)
-        uri, fragment = urlparse.urldefrag(new_uri)
-
-        if uri and normalize(uri) in self.store:
-            schema = self.store[normalize(uri)]
-        elif not uri or uri == self.base_uri:
-            schema = self.schema
-        else:
-            schema = resolve_remote(uri, self.handlers)
-            if self.cache:
-                self.store[normalize(uri)] = schema
-
-        old_base_uri, old_schema = self.base_uri, self.schema
-        self.base_uri, self.schema = uri, schema
-        try:
-            with self.in_scope(uri):
-                yield resolve_path(schema, fragment)
-        finally:
-            self.base_uri, self.schema = old_base_uri, old_schema
-
-    def get_uri(self):
-        return normalize(self.resolution_scope)
-
-    def get_scope_name(self):
-        """
-        Get current scope and return it as a valid function name.
-        """
-        name = 'validate_' + unquote(self.resolution_scope).replace('~1', '_').replace('~0', '_').replace('"', '')
-        name = re.sub(r'($[^a-zA-Z]|[^a-zA-Z0-9])', '_', name)
-        name = name.lower().rstrip('_')
-        return name
-
-    def walk(self, node: dict):
-        """
-        Walk thru schema and dereferencing ``id`` and ``$ref`` instances
-        """
-        if isinstance(node, bool):
-            pass
-        elif '$ref' in node and isinstance(node['$ref'], str):
-            ref = node['$ref']
-            node['$ref'] = urlparse.urljoin(self.resolution_scope, ref)
-        elif ('$id' in node or 'id' in node) and isinstance(get_id(node), str):
-            with self.in_scope(get_id(node)):
-                self.store[normalize(self.resolution_scope)] = node
-                for _, item in node.items():
-                    if isinstance(item, dict):
-                        self.walk(item)
-        else:
-            for _, item in node.items():
-                if isinstance(item, dict):
-                    self.walk(item)
diff --git a/src/poetry/core/_vendor/fastjsonschema/version.py b/src/poetry/core/_vendor/fastjsonschema/version.py
deleted file mode 100644
index 3335adc..0000000
--- a/src/poetry/core/_vendor/fastjsonschema/version.py
+++ /dev/null
@@ -1 +0,0 @@
-VERSION = '2.18.1'
diff --git a/src/poetry/core/_vendor/lark/LICENSE b/src/poetry/core/_vendor/lark/LICENSE
deleted file mode 100644
index 9201da2..0000000
--- a/src/poetry/core/_vendor/lark/LICENSE
+++ /dev/null
@@ -1,18 +0,0 @@
-Copyright © 2017 Erez Shinan
-
-Permission is hereby granted, free of charge, to any person obtaining a copy of
-this software and associated documentation files (the "Software"), to deal in
-the Software without restriction, including without limitation the rights to
-use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
-the Software, and to permit persons to whom the Software is furnished to do so,
-subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
-FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE AUTHORS OR
-COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
-IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
-CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
diff --git a/src/poetry/core/_vendor/lark/__init__.py b/src/poetry/core/_vendor/lark/__init__.py
deleted file mode 100644
index e316f99..0000000
--- a/src/poetry/core/_vendor/lark/__init__.py
+++ /dev/null
@@ -1,38 +0,0 @@
-from .exceptions import (
-    GrammarError,
-    LarkError,
-    LexError,
-    ParseError,
-    UnexpectedCharacters,
-    UnexpectedEOF,
-    UnexpectedInput,
-    UnexpectedToken,
-)
-from .lark import Lark
-from .lexer import Token
-from .tree import ParseTree, Tree
-from .utils import logger
-from .visitors import Discard, Transformer, Transformer_NonRecursive, Visitor, v_args
-
-__version__: str = "1.1.8"
-
-__all__ = (
-    "GrammarError",
-    "LarkError",
-    "LexError",
-    "ParseError",
-    "UnexpectedCharacters",
-    "UnexpectedEOF",
-    "UnexpectedInput",
-    "UnexpectedToken",
-    "Lark",
-    "Token",
-    "ParseTree",
-    "Tree",
-    "logger",
-    "Discard",
-    "Transformer",
-    "Transformer_NonRecursive",
-    "Visitor",
-    "v_args",
-)
diff --git a/src/poetry/core/_vendor/lark/__pyinstaller/__init__.py b/src/poetry/core/_vendor/lark/__pyinstaller/__init__.py
deleted file mode 100644
index 9da62a3..0000000
--- a/src/poetry/core/_vendor/lark/__pyinstaller/__init__.py
+++ /dev/null
@@ -1,6 +0,0 @@
-# For usage of lark with PyInstaller. See https://pyinstaller-sample-hook.readthedocs.io/en/latest/index.html
-
-import os
-
-def get_hook_dirs():
-    return [os.path.dirname(__file__)]
diff --git a/src/poetry/core/_vendor/lark/__pyinstaller/hook-lark.py b/src/poetry/core/_vendor/lark/__pyinstaller/hook-lark.py
deleted file mode 100644
index cf3d8e3..0000000
--- a/src/poetry/core/_vendor/lark/__pyinstaller/hook-lark.py
+++ /dev/null
@@ -1,14 +0,0 @@
-#-----------------------------------------------------------------------------
-# Copyright (c) 2017-2020, PyInstaller Development Team.
-#
-# Distributed under the terms of the GNU General Public License (version 2
-# or later) with exception for distributing the bootloader.
-#
-# The full license is in the file COPYING.txt, distributed with this software.
-#
-# SPDX-License-Identifier: (GPL-2.0-or-later WITH Bootloader-exception)
-#-----------------------------------------------------------------------------
-
-from PyInstaller.utils.hooks import collect_data_files
-
-datas = collect_data_files('lark')
diff --git a/src/poetry/core/_vendor/lark/ast_utils.py b/src/poetry/core/_vendor/lark/ast_utils.py
deleted file mode 100644
index a5460f3..0000000
--- a/src/poetry/core/_vendor/lark/ast_utils.py
+++ /dev/null
@@ -1,59 +0,0 @@
-"""
-    Module of utilities for transforming a lark.Tree into a custom Abstract Syntax Tree (AST defined in classes)
-"""
-
-import inspect, re
-import types
-from typing import Optional, Callable
-
-from lark import Transformer, v_args
-
-class Ast:
-    """Abstract class
-
-    Subclasses will be collected by `create_transformer()`
-    """
-    pass
-
-class AsList:
-    """Abstract class
-
-    Subclasses will be instantiated with the parse results as a single list, instead of as arguments.
-    """
-
-class WithMeta:
-    """Abstract class
-
-    Subclasses will be instantiated with the Meta instance of the tree. (see ``v_args`` for more detail)
-    """
-    pass
-
-def camel_to_snake(name):
-    return re.sub(r'(?<!^)(?=[A-Z])', '_', name).lower()
-
-def create_transformer(ast_module: types.ModuleType,
-                       transformer: Optional[Transformer]=None,
-                       decorator_factory: Callable=v_args) -> Transformer:
-    """Collects `Ast` subclasses from the given module, and creates a Lark transformer that builds the AST.
-
-    For each class, we create a corresponding rule in the transformer, with a matching name.
-    CamelCase names will be converted into snake_case. Example: "CodeBlock" -> "code_block".
-
-    Classes starting with an underscore (`_`) will be skipped.
-
-    Parameters:
-        ast_module: A Python module containing all the subclasses of ``ast_utils.Ast``
-        transformer (Optional[Transformer]): An initial transformer. Its attributes may be overwritten.
-        decorator_factory (Callable): An optional callable accepting two booleans, inline, and meta,
-            and returning a decorator for the methods of ``transformer``. (default: ``v_args``).
-    """
-    t = transformer or Transformer()
-
-    for name, obj in inspect.getmembers(ast_module):
-        if not name.startswith('_') and inspect.isclass(obj):
-            if issubclass(obj, Ast):
-                wrapper = decorator_factory(inline=not issubclass(obj, AsList), meta=issubclass(obj, WithMeta))
-                obj = wrapper(obj).__get__(t)
-                setattr(t, camel_to_snake(name), obj)
-
-    return t
diff --git a/src/poetry/core/_vendor/lark/common.py b/src/poetry/core/_vendor/lark/common.py
deleted file mode 100644
index d6be890..0000000
--- a/src/poetry/core/_vendor/lark/common.py
+++ /dev/null
@@ -1,89 +0,0 @@
-from copy import deepcopy
-import sys
-from types import ModuleType
-from typing import Callable, Collection, Dict, Optional, TYPE_CHECKING, List
-
-if TYPE_CHECKING:
-    from .lark import PostLex
-    from .lexer import Lexer
-    from .grammar import Rule
-    from typing import Union, Type
-    if sys.version_info >= (3, 8):
-        from typing import Literal
-    else:
-        from typing_extensions import Literal
-    if sys.version_info >= (3, 10):
-        from typing import TypeAlias
-    else:
-        from typing_extensions import TypeAlias
-
-from .utils import Serialize
-from .lexer import TerminalDef, Token
-
-###{standalone
-
-_ParserArgType: 'TypeAlias' = 'Literal["earley", "lalr", "cyk", "auto"]'
-_LexerArgType: 'TypeAlias' = 'Union[Literal["auto", "basic", "contextual", "dynamic", "dynamic_complete"], Type[Lexer]]'
-_LexerCallback = Callable[[Token], Token]
-ParserCallbacks = Dict[str, Callable]
-
-class LexerConf(Serialize):
-    __serialize_fields__ = 'terminals', 'ignore', 'g_regex_flags', 'use_bytes', 'lexer_type'
-    __serialize_namespace__ = TerminalDef,
-
-    terminals: Collection[TerminalDef]
-    re_module: ModuleType
-    ignore: Collection[str]
-    postlex: 'Optional[PostLex]'
-    callbacks: Dict[str, _LexerCallback]
-    g_regex_flags: int
-    skip_validation: bool
-    use_bytes: bool
-    lexer_type: Optional[_LexerArgType]
-    strict: bool
-
-    def __init__(self, terminals: Collection[TerminalDef], re_module: ModuleType, ignore: Collection[str]=(), postlex: 'Optional[PostLex]'=None,
-                 callbacks: Optional[Dict[str, _LexerCallback]]=None, g_regex_flags: int=0, skip_validation: bool=False, use_bytes: bool=False, strict: bool=False):
-        self.terminals = terminals
-        self.terminals_by_name = {t.name: t for t in self.terminals}
-        assert len(self.terminals) == len(self.terminals_by_name)
-        self.ignore = ignore
-        self.postlex = postlex
-        self.callbacks = callbacks or {}
-        self.g_regex_flags = g_regex_flags
-        self.re_module = re_module
-        self.skip_validation = skip_validation
-        self.use_bytes = use_bytes
-        self.strict = strict
-        self.lexer_type = None
-
-    def _deserialize(self):
-        self.terminals_by_name = {t.name: t for t in self.terminals}
-
-    def __deepcopy__(self, memo=None):
-        return type(self)(
-            deepcopy(self.terminals, memo),
-            self.re_module,
-            deepcopy(self.ignore, memo),
-            deepcopy(self.postlex, memo),
-            deepcopy(self.callbacks, memo),
-            deepcopy(self.g_regex_flags, memo),
-            deepcopy(self.skip_validation, memo),
-            deepcopy(self.use_bytes, memo),
-        )
-
-class ParserConf(Serialize):
-    __serialize_fields__ = 'rules', 'start', 'parser_type'
-
-    rules: List['Rule']
-    callbacks: ParserCallbacks
-    start: List[str]
-    parser_type: _ParserArgType
-
-    def __init__(self, rules: List['Rule'], callbacks: ParserCallbacks, start: List[str]):
-        assert isinstance(start, list)
-        self.rules = rules
-        self.callbacks = callbacks
-        self.start = start
-
-###}
diff --git a/src/poetry/core/_vendor/lark/exceptions.py b/src/poetry/core/_vendor/lark/exceptions.py
deleted file mode 100644
index e099d59..0000000
--- a/src/poetry/core/_vendor/lark/exceptions.py
+++ /dev/null
@@ -1,292 +0,0 @@
-from .utils import logger, NO_VALUE
-from typing import Mapping, Iterable, Callable, Union, TypeVar, Tuple, Any, List, Set, Optional, Collection, TYPE_CHECKING
-
-if TYPE_CHECKING:
-    from .lexer import Token
-    from .parsers.lalr_interactive_parser import InteractiveParser
-    from .tree import Tree
-
-###{standalone
-
-class LarkError(Exception):
-    pass
-
-
-class ConfigurationError(LarkError, ValueError):
-    pass
-
-
-def assert_config(value, options: Collection, msg='Got %r, expected one of %s'):
-    if value not in options:
-        raise ConfigurationError(msg % (value, options))
-
-
-class GrammarError(LarkError):
-    pass
-
-
-class ParseError(LarkError):
-    pass
-
-
-class LexError(LarkError):
-    pass
-
-T = TypeVar('T')
-
-class UnexpectedInput(LarkError):
-    """UnexpectedInput Error.
-
-    Used as a base class for the following exceptions:
-
-    - ``UnexpectedCharacters``: The lexer encountered an unexpected string
-    - ``UnexpectedToken``: The parser received an unexpected token
-    - ``UnexpectedEOF``: The parser expected a token, but the input ended
-
-    After catching one of these exceptions, you may call the following helper methods to create a nicer error message.
-    """
-    line: int
-    column: int
-    pos_in_stream = None
-    state: Any
-    _terminals_by_name = None
-    interactive_parser: 'InteractiveParser'
-
-    def get_context(self, text: str, span: int=40) -> str:
-        """Returns a pretty string pinpointing the error in the text,
-        with span amount of context characters around it.
-
-        Note:
-            The parser doesn't hold a copy of the text it has to parse,
-            so you have to provide it again
-        """
-        assert self.pos_in_stream is not None, self
-        pos = self.pos_in_stream
-        start = max(pos - span, 0)
-        end = pos + span
-        if not isinstance(text, bytes):
-            before = text[start:pos].rsplit('\n', 1)[-1]
-            after = text[pos:end].split('\n', 1)[0]
-            return before + after + '\n' + ' ' * len(before.expandtabs()) + '^\n'
-        else:
-            before = text[start:pos].rsplit(b'\n', 1)[-1]
-            after = text[pos:end].split(b'\n', 1)[0]
-            return (before + after + b'\n' + b' ' * len(before.expandtabs()) + b'^\n').decode("ascii", "backslashreplace")
-
-    def match_examples(self, parse_fn: 'Callable[[str], Tree]',
-                             examples: Union[Mapping[T, Iterable[str]], Iterable[Tuple[T, Iterable[str]]]],
-                             token_type_match_fallback: bool=False,
-                             use_accepts: bool=True
-                         ) -> Optional[T]:
-        """Allows you to detect what's wrong in the input text by matching
-        against example errors.
-
-        Given a parser instance and a dictionary mapping some label with
-        some malformed syntax examples, it'll return the label for the
-        example that bests matches the current error. The function will
-        iterate the dictionary until it finds a matching error, and
-        return the corresponding value.
-
-        For an example usage, see `examples/error_reporting_lalr.py`
-
-        Parameters:
-            parse_fn: parse function (usually ``lark_instance.parse``)
-            examples: dictionary of ``{'example_string': value}``.
-            use_accepts: Recommended to keep this as ``use_accepts=True``.
-        """
-        assert self.state is not None, "Not supported for this exception"
-
-        if isinstance(examples, Mapping):
-            examples = examples.items()
-
-        candidate = (None, False)
-        for i, (label, example) in enumerate(examples):
-            assert not isinstance(example, str), "Expecting a list"
-
-            for j, malformed in enumerate(example):
-                try:
-                    parse_fn(malformed)
-                except UnexpectedInput as ut:
-                    if ut.state == self.state:
-                        if (
-                            use_accepts
-                            and isinstance(self, UnexpectedToken)
-                            and isinstance(ut, UnexpectedToken)
-                            and ut.accepts != self.accepts
-                        ):
-                            logger.debug("Different accepts with same state[%d]: %s != %s at example [%s][%s]" %
-                                         (self.state, self.accepts, ut.accepts, i, j))
-                            continue
-                        if (
-                            isinstance(self, (UnexpectedToken, UnexpectedEOF))
-                            and isinstance(ut, (UnexpectedToken, UnexpectedEOF))
-                        ):
-                            if ut.token == self.token:  # Try exact match first
-                                logger.debug("Exact Match at example [%s][%s]" % (i, j))
-                                return label
-
-                            if token_type_match_fallback:
-                                # Fallback to token types match
-                                if (ut.token.type == self.token.type) and not candidate[-1]:
-                                    logger.debug("Token Type Fallback at example [%s][%s]" % (i, j))
-                                    candidate = label, True
-
-                        if candidate[0] is None:
-                            logger.debug("Same State match at example [%s][%s]" % (i, j))
-                            candidate = label, False
-
-        return candidate[0]
-
-    def _format_expected(self, expected):
-        if self._terminals_by_name:
-            d = self._terminals_by_name
-            expected = [d[t_name].user_repr() if t_name in d else t_name for t_name in expected]
-        return "Expected one of: \n\t* %s\n" % '\n\t* '.join(expected)
-
-
-class UnexpectedEOF(ParseError, UnexpectedInput):
-    """An exception that is raised by the parser, when the input ends while it still expects a token.
-    """
-    expected: 'List[Token]'
-
-    def __init__(self, expected, state=None, terminals_by_name=None):
-        super(UnexpectedEOF, self).__init__()
-
-        self.expected = expected
-        self.state = state
-        from .lexer import Token
-        self.token = Token("<EOF>", "")  # , line=-1, column=-1, pos_in_stream=-1)
-        self.pos_in_stream = -1
-        self.line = -1
-        self.column = -1
-        self._terminals_by_name = terminals_by_name
-
-
-    def __str__(self):
-        message = "Unexpected end-of-input. "
-        message += self._format_expected(self.expected)
-        return message
-
-
-class UnexpectedCharacters(LexError, UnexpectedInput):
-    """An exception that is raised by the lexer, when it cannot match the next
-    string of characters to any of its terminals.
-    """
-
-    allowed: Set[str]
-    considered_tokens: Set[Any]
-
-    def __init__(self, seq, lex_pos, line, column, allowed=None, considered_tokens=None, state=None, token_history=None,
-                 terminals_by_name=None, considered_rules=None):
-        super(UnexpectedCharacters, self).__init__()
-
-        # TODO considered_tokens and allowed can be figured out using state
-        self.line = line
-        self.column = column
-        self.pos_in_stream = lex_pos
-        self.state = state
-        self._terminals_by_name = terminals_by_name
-
-        self.allowed = allowed
-        self.considered_tokens = considered_tokens
-        self.considered_rules = considered_rules
-        self.token_history = token_history
-
-        if isinstance(seq, bytes):
-            self.char = seq[lex_pos:lex_pos + 1].decode("ascii", "backslashreplace")
-        else:
-            self.char = seq[lex_pos]
-        self._context = self.get_context(seq)
-
-
-    def __str__(self):
-        message = "No terminal matches '%s' in the current parser context, at line %d col %d" % (self.char, self.line, self.column)
-        message += '\n\n' + self._context
-        if self.allowed:
-            message += self._format_expected(self.allowed)
-        if self.token_history:
-            message += '\nPrevious tokens: %s\n' % ', '.join(repr(t) for t in self.token_history)
-        return message
-
-
-class UnexpectedToken(ParseError, UnexpectedInput):
-    """An exception that is raised by the parser, when the token it received
-    doesn't match any valid step forward.
-
-    Parameters:
-        token: The mismatched token
-        expected: The set of expected tokens
-        considered_rules: Which rules were considered, to deduce the expected tokens
-        state: A value representing the parser state. Do not rely on its value or type.
-        interactive_parser: An instance of ``InteractiveParser``, that is initialized to the point of failure,
-                            and can be used for debugging and error handling.
-
-    Note: These parameters are available as attributes of the instance.
-    """
-
-    expected: Set[str]
-    considered_rules: Set[str]
-
-    def __init__(self, token, expected, considered_rules=None, state=None, interactive_parser=None, terminals_by_name=None, token_history=None):
-        super(UnexpectedToken, self).__init__()
-
-        # TODO considered_rules and expected can be figured out using state
-        self.line = getattr(token, 'line', '?')
-        self.column = getattr(token, 'column', '?')
-        self.pos_in_stream = getattr(token, 'start_pos', None)
-        self.state = state
-
-        self.token = token
-        self.expected = expected  # XXX deprecate? `accepts` is better
-        self._accepts = NO_VALUE
-        self.considered_rules = considered_rules
-        self.interactive_parser = interactive_parser
-        self._terminals_by_name = terminals_by_name
-        self.token_history = token_history
-
-
-    @property
-    def accepts(self) -> Set[str]:
-        if self._accepts is NO_VALUE:
-            self._accepts = self.interactive_parser and self.interactive_parser.accepts()
-        return self._accepts
-
-    def __str__(self):
-        message = ("Unexpected token %r at line %s, column %s.\n%s"
-                   % (self.token, self.line, self.column, self._format_expected(self.accepts or self.expected)))
-        if self.token_history:
-            message += "Previous tokens: %r\n" % self.token_history
-
-        return message
-
-
-
-class VisitError(LarkError):
-    """VisitError is raised when visitors are interrupted by an exception
-
-    It provides the following attributes for inspection:
-
-    Parameters:
-        rule: the name of the visit rule that failed
-        obj: the tree-node or token that was being processed
-        orig_exc: the exception that cause it to fail
-
-    Note: These parameters are available as attributes
-    """
-
-    obj: 'Union[Tree, Token]'
-    orig_exc: Exception
-
-    def __init__(self, rule, obj, orig_exc):
-        message = 'Error trying to process rule "%s":\n\n%s' % (rule, orig_exc)
-        super(VisitError, self).__init__(message)
-
-        self.rule = rule
-        self.obj = obj
-        self.orig_exc = orig_exc
-
-
-class MissingVariableError(LarkError):
-    pass
-
-###}
diff --git a/src/poetry/core/_vendor/lark/grammar.py b/src/poetry/core/_vendor/lark/grammar.py
deleted file mode 100644
index 1d226d9..0000000
--- a/src/poetry/core/_vendor/lark/grammar.py
+++ /dev/null
@@ -1,130 +0,0 @@
-from typing import Optional, Tuple, ClassVar, Sequence
-
-from .utils import Serialize
-
-###{standalone
-TOKEN_DEFAULT_PRIORITY = 0
-
-
-class Symbol(Serialize):
-    __slots__ = ('name',)
-
-    name: str
-    is_term: ClassVar[bool] = NotImplemented
-
-    def __init__(self, name: str) -> None:
-        self.name = name
-
-    def __eq__(self, other):
-        assert isinstance(other, Symbol), other
-        return self.is_term == other.is_term and self.name == other.name
-
-    def __ne__(self, other):
-        return not (self == other)
-
-    def __hash__(self):
-        return hash(self.name)
-
-    def __repr__(self):
-        return '%s(%r)' % (type(self).__name__, self.name)
-
-    fullrepr = property(__repr__)
-
-    def renamed(self, f):
-        return type(self)(f(self.name))
-
-
-class Terminal(Symbol):
-    __serialize_fields__ = 'name', 'filter_out'
-
-    is_term: ClassVar[bool] = True
-
-    def __init__(self, name, filter_out=False):
-        self.name = name
-        self.filter_out = filter_out
-
-    @property
-    def fullrepr(self):
-        return '%s(%r, %r)' % (type(self).__name__, self.name, self.filter_out)
-
-    def renamed(self, f):
-        return type(self)(f(self.name), self.filter_out)
-
-
-class NonTerminal(Symbol):
-    __serialize_fields__ = 'name',
-
-    is_term: ClassVar[bool] = False
-
-
-class RuleOptions(Serialize):
-    __serialize_fields__ = 'keep_all_tokens', 'expand1', 'priority', 'template_source', 'empty_indices'
-
-    keep_all_tokens: bool
-    expand1: bool
-    priority: Optional[int]
-    template_source: Optional[str]
-    empty_indices: Tuple[bool, ...]
-
-    def __init__(self, keep_all_tokens: bool=False, expand1: bool=False, priority: Optional[int]=None, template_source: Optional[str]=None, empty_indices: Tuple[bool, ...]=()) -> None:
-        self.keep_all_tokens = keep_all_tokens
-        self.expand1 = expand1
-        self.priority = priority
-        self.template_source = template_source
-        self.empty_indices = empty_indices
-
-    def __repr__(self):
-        return 'RuleOptions(%r, %r, %r, %r)' % (
-            self.keep_all_tokens,
-            self.expand1,
-            self.priority,
-            self.template_source
-        )
-
-
-class Rule(Serialize):
-    """
-        origin : a symbol
-        expansion : a list of symbols
-        order : index of this expansion amongst all rules of the same name
-    """
-    __slots__ = ('origin', 'expansion', 'alias', 'options', 'order', '_hash')
-
-    __serialize_fields__ = 'origin', 'expansion', 'order', 'alias', 'options'
-    __serialize_namespace__ = Terminal, NonTerminal, RuleOptions
-
-    origin: NonTerminal
-    expansion: Sequence[Symbol]
-    order: int
-    alias: Optional[str]
-    options: RuleOptions
-    _hash: int
-
-    def __init__(self, origin: NonTerminal, expansion: Sequence[Symbol],
-                 order: int=0, alias: Optional[str]=None, options: Optional[RuleOptions]=None):
-        self.origin = origin
-        self.expansion = expansion
-        self.alias = alias
-        self.order = order
-        self.options = options or RuleOptions()
-        self._hash = hash((self.origin, tuple(self.expansion)))
-
-    def _deserialize(self):
-        self._hash = hash((self.origin, tuple(self.expansion)))
-
-    def __str__(self):
-        return '<%s : %s>' % (self.origin.name, ' '.join(x.name for x in self.expansion))
-
-    def __repr__(self):
-        return 'Rule(%r, %r, %r, %r)' % (self.origin, self.expansion, self.alias, self.options)
-
-    def __hash__(self):
-        return self._hash
-
-    def __eq__(self, other):
-        if not isinstance(other, Rule):
-            return False
-        return self.origin == other.origin and self.expansion == other.expansion
-
-
-###}
diff --git a/src/poetry/core/_vendor/lark/grammars/common.lark b/src/poetry/core/_vendor/lark/grammars/common.lark
deleted file mode 100644
index e15b163..0000000
--- a/src/poetry/core/_vendor/lark/grammars/common.lark
+++ /dev/null
@@ -1,59 +0,0 @@
-// Basic terminals for common use
-
-
-//
-// Numbers
-//
-
-DIGIT: "0".."9"
-HEXDIGIT: "a".."f"|"A".."F"|DIGIT
-
-INT: DIGIT+
-SIGNED_INT: ["+"|"-"] INT
-DECIMAL: INT "." INT? | "." INT
-
-// float = /-?\d+(\.\d+)?([eE][+-]?\d+)?/
-_EXP: ("e"|"E") SIGNED_INT
-FLOAT: INT _EXP | DECIMAL _EXP?
-SIGNED_FLOAT: ["+"|"-"] FLOAT
-
-NUMBER: FLOAT | INT
-SIGNED_NUMBER: ["+"|"-"] NUMBER
-
-//
-// Strings
-//
-_STRING_INNER: /.*?/
-_STRING_ESC_INNER: _STRING_INNER /(?<!\\)(\\\\)*?/
-
-ESCAPED_STRING : "\"" _STRING_ESC_INNER "\""
-
-
-//
-// Names (Variables)
-//
-LCASE_LETTER: "a".."z"
-UCASE_LETTER: "A".."Z"
-
-LETTER: UCASE_LETTER | LCASE_LETTER
-WORD: LETTER+
-
-CNAME: ("_"|LETTER) ("_"|LETTER|DIGIT)*
-
-
-//
-// Whitespace
-//
-WS_INLINE: (" "|/\t/)+
-WS: /[ \t\f\r\n]/+
-
-CR : /\r/
-LF : /\n/
-NEWLINE: (CR? LF)+
-
-
-// Comments
-SH_COMMENT: /#[^\n]*/
-CPP_COMMENT: /\/\/[^\n]*/
-C_COMMENT: "/*" /(.|\n)*?/ "*/"
-SQL_COMMENT: /--[^\n]*/
diff --git a/src/poetry/core/_vendor/lark/grammars/lark.lark b/src/poetry/core/_vendor/lark/grammars/lark.lark
deleted file mode 100644
index fbcf88a..0000000
--- a/src/poetry/core/_vendor/lark/grammars/lark.lark
+++ /dev/null
@@ -1,62 +0,0 @@
-# Lark grammar of Lark's syntax
-# Note: Lark is not bootstrapped, its parser is implemented in load_grammar.py
-
-start: (_item? _NL)* _item?
-
-_item: rule
-     | token
-     | statement
-
-rule: RULE rule_params priority? ":" expansions
-token: TOKEN token_params priority? ":" expansions
-
-rule_params: ["{" RULE ("," RULE)* "}"]
-token_params: ["{" TOKEN ("," TOKEN)* "}"]
-
-priority: "." NUMBER
-
-statement: "%ignore" expansions                    -> ignore
-         | "%import" import_path ["->" name]       -> import
-         | "%import" import_path name_list         -> multi_import
-         | "%override" rule                        -> override_rule
-         | "%declare" name+                        -> declare
-
-!import_path: "."? name ("." name)*
-name_list: "(" name ("," name)* ")"
-
-?expansions: alias (_VBAR alias)*
-
-?alias: expansion ["->" RULE]
-
-?expansion: expr*
-
-?expr: atom [OP | "~" NUMBER [".." NUMBER]]
-
-?atom: "(" expansions ")"
-     | "[" expansions "]" -> maybe
-     | value
-
-?value: STRING ".." STRING -> literal_range
-      | name
-      | (REGEXP | STRING) -> literal
-      | name "{" value ("," value)* "}" -> template_usage
-
-name: RULE
-    | TOKEN
-
-_VBAR: _NL? "|"
-OP: /[+*]|[?](?![a-z])/
-RULE: /!?[_?]?[a-z][_a-z0-9]*/
-TOKEN: /_?[A-Z][_A-Z0-9]*/
-STRING: _STRING "i"?
-REGEXP: /\/(?!\/)(\\\/|\\\\|[^\/])*?\/[imslux]*/
-_NL: /(\r?\n)+\s*/
-
-%import common.ESCAPED_STRING -> _STRING
-%import common.SIGNED_INT -> NUMBER
-%import common.WS_INLINE
-
-COMMENT: /\s*/ "//" /[^\n]/* | /\s*/ "#" /[^\n]/*
-
-%ignore WS_INLINE
-%ignore COMMENT
diff --git a/src/poetry/core/_vendor/lark/grammars/python.lark b/src/poetry/core/_vendor/lark/grammars/python.lark
deleted file mode 100644
index 70ffad7..0000000
--- a/src/poetry/core/_vendor/lark/grammars/python.lark
+++ /dev/null
@@ -1,302 +0,0 @@
-// Python 3 grammar for Lark
-
-// This grammar should parse all python 3.x code successfully.
-
-// Adapted from: https://docs.python.org/3/reference/grammar.html
-
-// Start symbols for the grammar:
-//       single_input is a single interactive statement;
-//       file_input is a module or sequence of commands read from an input file;
-//       eval_input is the input for the eval() functions.
-// NB: compound_stmt in single_input is followed by extra NEWLINE!
-//
-
-single_input: _NEWLINE | simple_stmt | compound_stmt _NEWLINE
-file_input: (_NEWLINE | stmt)*
-eval_input: testlist _NEWLINE*
-
-decorator: "@" dotted_name [ "(" [arguments] ")" ] _NEWLINE
-decorators: decorator+
-decorated: decorators (classdef | funcdef | async_funcdef)
-
-async_funcdef: "async" funcdef
-funcdef: "def" name "(" [parameters] ")" ["->" test] ":" suite
-
-parameters: paramvalue ("," paramvalue)* ["," SLASH ("," paramvalue)*] ["," [starparams | kwparams]]
-          | starparams
-          | kwparams
-
-SLASH: "/" // Otherwise the it will completely disappear and it will be undisguisable in the result
-starparams: (starparam | starguard) poststarparams
-starparam: "*" typedparam
-starguard: "*"
-poststarparams: ("," paramvalue)* ["," kwparams]
-kwparams: "**" typedparam ","?
-
-?paramvalue: typedparam ("=" test)?
-?typedparam: name (":" test)?
-
-
-lambdef: "lambda" [lambda_params] ":" test
-lambdef_nocond: "lambda" [lambda_params] ":" test_nocond
-lambda_params: lambda_paramvalue ("," lambda_paramvalue)* ["," [lambda_starparams | lambda_kwparams]]
-          | lambda_starparams
-          | lambda_kwparams
-?lambda_paramvalue: name ("=" test)?
-lambda_starparams: "*" [name]  ("," lambda_paramvalue)* ["," [lambda_kwparams]]
-lambda_kwparams: "**" name ","?
-
-
-?stmt: simple_stmt | compound_stmt
-?simple_stmt: small_stmt (";" small_stmt)* [";"] _NEWLINE
-?small_stmt: (expr_stmt | assign_stmt | del_stmt | pass_stmt | flow_stmt | import_stmt | global_stmt | nonlocal_stmt | assert_stmt)
-expr_stmt: testlist_star_expr
-assign_stmt: annassign | augassign | assign
-
-annassign: testlist_star_expr ":" test ["=" test]
-assign: testlist_star_expr ("=" (yield_expr|testlist_star_expr))+
-augassign: testlist_star_expr augassign_op (yield_expr|testlist)
-!augassign_op: "+=" | "-=" | "*=" | "@=" | "/=" | "%=" | "&=" | "|=" | "^=" | "<<=" | ">>=" | "**=" | "//="
-?testlist_star_expr: test_or_star_expr
-                   | test_or_star_expr ("," test_or_star_expr)+ ","?  -> tuple
-                   | test_or_star_expr ","  -> tuple
-
-// For normal and annotated assignments, additional restrictions enforced by the interpreter
-del_stmt: "del" exprlist
-pass_stmt: "pass"
-?flow_stmt: break_stmt | continue_stmt | return_stmt | raise_stmt | yield_stmt
-break_stmt: "break"
-continue_stmt: "continue"
-return_stmt: "return" [testlist]
-yield_stmt: yield_expr
-raise_stmt: "raise" [test ["from" test]]
-import_stmt: import_name | import_from
-import_name: "import" dotted_as_names
-// note below: the ("." | "...") is necessary because "..." is tokenized as ELLIPSIS
-import_from: "from" (dots? dotted_name | dots) "import" ("*" | "(" import_as_names ")" | import_as_names)
-!dots: "."+
-import_as_name: name ["as" name]
-dotted_as_name: dotted_name ["as" name]
-import_as_names: import_as_name ("," import_as_name)* [","]
-dotted_as_names: dotted_as_name ("," dotted_as_name)*
-dotted_name: name ("." name)*
-global_stmt: "global" name ("," name)*
-nonlocal_stmt: "nonlocal" name ("," name)*
-assert_stmt: "assert" test ["," test]
-
-?compound_stmt: if_stmt | while_stmt | for_stmt | try_stmt | match_stmt
-              | with_stmt | funcdef | classdef | decorated | async_stmt
-async_stmt: "async" (funcdef | with_stmt | for_stmt)
-if_stmt: "if" test ":" suite elifs ["else" ":" suite]
-elifs: elif_*
-elif_: "elif" test ":" suite
-while_stmt: "while" test ":" suite ["else" ":" suite]
-for_stmt: "for" exprlist "in" testlist ":" suite ["else" ":" suite]
-try_stmt: "try" ":" suite except_clauses ["else" ":" suite] [finally]
-        | "try" ":" suite finally   -> try_finally
-finally: "finally" ":" suite
-except_clauses: except_clause+
-except_clause: "except" [test ["as" name]] ":" suite
-// NB compile.c makes sure that the default except clause is last
-
-
-with_stmt: "with" with_items ":" suite
-with_items: with_item ("," with_item)*
-with_item: test ["as" name]
-
-match_stmt: "match" test ":" _NEWLINE _INDENT case+ _DEDENT
-
-case: "case" pattern ["if" test] ":" suite
-
-?pattern: sequence_item_pattern "," _sequence_pattern -> sequence_pattern
-        | as_pattern
-?as_pattern: or_pattern ("as" NAME)?
-?or_pattern: closed_pattern ("|" closed_pattern)*
-?closed_pattern: literal_pattern
-               | NAME -> capture_pattern
-               | "_" -> any_pattern
-               | attr_pattern
-               | "(" as_pattern ")"
-               | "[" _sequence_pattern "]" -> sequence_pattern
-               | "(" (sequence_item_pattern "," _sequence_pattern)? ")" -> sequence_pattern
-               | "{" (mapping_item_pattern ("," mapping_item_pattern)* ","?)?"}" -> mapping_pattern
-               | "{" (mapping_item_pattern ("," mapping_item_pattern)* ",")? "**" NAME ","? "}" -> mapping_star_pattern
-               | class_pattern
-
-literal_pattern: inner_literal_pattern
-
-?inner_literal_pattern: "None" -> const_none
-                      | "True" -> const_true
-                      | "False" -> const_false
-                      | STRING -> string
-                      | number
-
-attr_pattern: NAME ("." NAME)+ -> value
-
-name_or_attr_pattern: NAME ("." NAME)* -> value
-
-mapping_item_pattern: (literal_pattern|attr_pattern) ":" as_pattern
-
-_sequence_pattern: (sequence_item_pattern ("," sequence_item_pattern)* ","?)?
-?sequence_item_pattern: as_pattern
-                      | "*" NAME -> star_pattern
-
-class_pattern: name_or_attr_pattern "(" [arguments_pattern ","?] ")"
-arguments_pattern: pos_arg_pattern ["," keyws_arg_pattern]
-                 | keyws_arg_pattern -> no_pos_arguments
-
-pos_arg_pattern: as_pattern ("," as_pattern)*
-keyws_arg_pattern: keyw_arg_pattern ("," keyw_arg_pattern)*
-keyw_arg_pattern: NAME "=" as_pattern
-
-
-
-suite: simple_stmt | _NEWLINE _INDENT stmt+ _DEDENT
-
-?test: or_test ("if" or_test "else" test)?
-     | lambdef
-     | assign_expr
-
-assign_expr: name ":=" test
-
-?test_nocond: or_test | lambdef_nocond
-
-?or_test: and_test ("or" and_test)*
-?and_test: not_test_ ("and" not_test_)*
-?not_test_: "not" not_test_ -> not_test
-         | comparison
-?comparison: expr (comp_op expr)*
-star_expr: "*" expr
-
-?expr: or_expr
-?or_expr: xor_expr ("|" xor_expr)*
-?xor_expr: and_expr ("^" and_expr)*
-?and_expr: shift_expr ("&" shift_expr)*
-?shift_expr: arith_expr (_shift_op arith_expr)*
-?arith_expr: term (_add_op term)*
-?term: factor (_mul_op factor)*
-?factor: _unary_op factor | power
-
-!_unary_op: "+"|"-"|"~"
-!_add_op: "+"|"-"
-!_shift_op: "<<"|">>"
-!_mul_op: "*"|"@"|"/"|"%"|"//"
-// <> isn't actually a valid comparison operator in Python. It's here for the
-// sake of a __future__ import described in PEP 401 (which really works :-)
-!comp_op: "<"|">"|"=="|">="|"<="|"<>"|"!="|"in"|"not" "in"|"is"|"is" "not"
-
-?power: await_expr ("**" factor)?
-?await_expr: AWAIT? atom_expr
-AWAIT: "await"
-
-?atom_expr: atom_expr "(" [arguments] ")"      -> funccall
-          | atom_expr "[" subscriptlist "]"  -> getitem
-          | atom_expr "." name               -> getattr
-          | atom
-
-?atom: "(" yield_expr ")"
-     | "(" _tuple_inner? ")" -> tuple
-     | "(" comprehension{test_or_star_expr} ")" -> tuple_comprehension
-     | "[" _exprlist? "]"  -> list
-     | "[" comprehension{test_or_star_expr} "]"  -> list_comprehension
-     | "{" _dict_exprlist? "}" -> dict
-     | "{" comprehension{key_value} "}" -> dict_comprehension
-     | "{" _exprlist "}" -> set
-     | "{" comprehension{test} "}" -> set_comprehension
-     | name -> var
-     | number
-     | string_concat
-     | "(" test ")"
-     | "..." -> ellipsis
-     | "None"    -> const_none
-     | "True"    -> const_true
-     | "False"   -> const_false
-
-
-?string_concat: string+
-
-_tuple_inner: test_or_star_expr (("," test_or_star_expr)+ [","] | ",")
-
-?test_or_star_expr: test
-                 | star_expr
-
-?subscriptlist: subscript
-              | subscript (("," subscript)+ [","] | ",") -> subscript_tuple
-?subscript: test | ([test] ":" [test] [sliceop]) -> slice
-sliceop: ":" [test]
-?exprlist: (expr|star_expr)
-         | (expr|star_expr) (("," (expr|star_expr))+ [","]|",")
-?testlist: test | testlist_tuple
-testlist_tuple: test (("," test)+ [","] | ",")
-_dict_exprlist: (key_value | "**" expr) ("," (key_value | "**" expr))* [","]
-
-key_value: test ":"  test
-
-_exprlist: test_or_star_expr (","  test_or_star_expr)* [","]
-
-classdef: "class" name ["(" [arguments] ")"] ":" suite
-
-
-
-arguments: argvalue ("," argvalue)*  ("," [ starargs | kwargs])?
-         | starargs
-         | kwargs
-         | comprehension{test}
-
-starargs: stararg ("," stararg)* ("," argvalue)* ["," kwargs]
-stararg: "*" test
-kwargs: "**" test ("," argvalue)*
-
-?argvalue: test ("=" test)?
-
-
-comprehension{comp_result}: comp_result comp_fors [comp_if]
-comp_fors: comp_for+
-comp_for: [ASYNC] "for" exprlist "in" or_test
-ASYNC: "async"
-?comp_if: "if" test_nocond
-
-// not used in grammar, but may appear in "node" passed from Parser to Compiler
-encoding_decl: name
-
-yield_expr: "yield" [testlist]
-          | "yield" "from" test -> yield_from
-
-number: DEC_NUMBER | HEX_NUMBER | BIN_NUMBER | OCT_NUMBER | FLOAT_NUMBER | IMAG_NUMBER
-string: STRING | LONG_STRING
-
-// Other terminals
-
-_NEWLINE: ( /\r?\n[\t ]*/ | COMMENT )+
-
-%ignore /[\t \f]+/  // WS
-%ignore /\\[\t \f]*\r?\n/   // LINE_CONT
-%ignore COMMENT
-%declare _INDENT _DEDENT
-
-
-// Python terminals
-
-!name: NAME | "match" | "case"
-NAME: /[^\W\d]\w*/
-COMMENT: /#[^\n]*/
-
-STRING: /([ubf]?r?|r[ubf])("(?!"").*?(?<!\\)(\\\\)*?"|'(?!'').*?(?<!\\)(\\\\)*?')/i
-LONG_STRING: /([ubf]?r?|r[ubf])(""".*?(?<!\\)(\\\\)*?"""|'''.*?(?<!\\)(\\\\)*?''')/is
-
-_SPECIAL_DEC: "0".."9"        ("_"?  "0".."9"                       )*
-DEC_NUMBER:   "1".."9"        ("_"?  "0".."9"                       )*
-          |   "0"             ("_"?  "0"                            )* /(?![1-9])/
-HEX_NUMBER.2: "0" ("x" | "X") ("_"? ("0".."9" | "a".."f" | "A".."F"))+
-OCT_NUMBER.2: "0" ("o" | "O") ("_"?  "0".."7"                       )+
-BIN_NUMBER.2: "0" ("b" | "B") ("_"?  "0".."1"                       )+
-
-_EXP: ("e"|"E") ["+" | "-"] _SPECIAL_DEC
-DECIMAL: "." _SPECIAL_DEC | _SPECIAL_DEC "." _SPECIAL_DEC?
-FLOAT_NUMBER.2: _SPECIAL_DEC _EXP | DECIMAL _EXP?
-IMAG_NUMBER.2: (_SPECIAL_DEC      | FLOAT_NUMBER) ("J" | "j")
-
-
-// Comma-separated list (with an optional trailing comma)
-cs_list{item}: item ("," item)* ","?
-_cs_list{item}: item ("," item)* ","?
diff --git a/src/poetry/core/_vendor/lark/grammars/unicode.lark b/src/poetry/core/_vendor/lark/grammars/unicode.lark
deleted file mode 100644
index fb139e2..0000000
--- a/src/poetry/core/_vendor/lark/grammars/unicode.lark
+++ /dev/null
@@ -1,7 +0,0 @@
-// TODO: LETTER, WORD, etc.
-
-//
-// Whitespace
-//
-WS_INLINE: /[ \t\xa0]/+
-WS: /[ \t\xa0\f\r\n]/+
diff --git a/src/poetry/core/_vendor/lark/indenter.py b/src/poetry/core/_vendor/lark/indenter.py
deleted file mode 100644
index 7e027f2..0000000
--- a/src/poetry/core/_vendor/lark/indenter.py
+++ /dev/null
@@ -1,112 +0,0 @@
-"Provides a post-lexer for implementing Python-style indentation."
-
-from abc import ABC, abstractmethod
-from typing import List, Iterator
-
-from .exceptions import LarkError
-from .lark import PostLex
-from .lexer import Token
-
-###{standalone
-
-class DedentError(LarkError):
-    pass
-
-class Indenter(PostLex, ABC):
-    paren_level: int
-    indent_level: List[int]
-
-    def __init__(self) -> None:
-        self.paren_level = 0
-        self.indent_level = [0]
-        assert self.tab_len > 0
-
-    def handle_NL(self, token: Token) -> Iterator[Token]:
-        if self.paren_level > 0:
-            return
-
-        yield token
-
-        indent_str = token.rsplit('\n', 1)[1] # Tabs and spaces
-        indent = indent_str.count(' ') + indent_str.count('\t') * self.tab_len
-
-        if indent > self.indent_level[-1]:
-            self.indent_level.append(indent)
-            yield Token.new_borrow_pos(self.INDENT_type, indent_str, token)
-        else:
-            while indent < self.indent_level[-1]:
-                self.indent_level.pop()
-                yield Token.new_borrow_pos(self.DEDENT_type, indent_str, token)
-
-            if indent != self.indent_level[-1]:
-                raise DedentError('Unexpected dedent to column %s. Expected dedent to %s' % (indent, self.indent_level[-1]))
-
-    def _process(self, stream):
-        for token in stream:
-            if token.type == self.NL_type:
-                yield from self.handle_NL(token)
-            else:
-                yield token
-
-            if token.type in self.OPEN_PAREN_types:
-                self.paren_level += 1
-            elif token.type in self.CLOSE_PAREN_types:
-                self.paren_level -= 1
-                assert self.paren_level >= 0
-
-        while len(self.indent_level) > 1:
-            self.indent_level.pop()
-            yield Token(self.DEDENT_type, '')
-
-        assert self.indent_level == [0], self.indent_level
-
-    def process(self, stream):
-        self.paren_level = 0
-        self.indent_level = [0]
-        return self._process(stream)
-
-    # XXX Hack for ContextualLexer. Maybe there's a more elegant solution?
-    @property
-    def always_accept(self):
-        return (self.NL_type,)
-
-    @property
-    @abstractmethod
-    def NL_type(self) -> str:
-        raise NotImplementedError()
-
-    @property
-    @abstractmethod
-    def OPEN_PAREN_types(self) -> List[str]:
-        raise NotImplementedError()
-
-    @property
-    @abstractmethod
-    def CLOSE_PAREN_types(self) -> List[str]:
-        raise NotImplementedError()
-
-    @property
-    @abstractmethod
-    def INDENT_type(self) -> str:
-        raise NotImplementedError()
-
-    @property
-    @abstractmethod
-    def DEDENT_type(self) -> str:
-        raise NotImplementedError()
-
-    @property
-    @abstractmethod
-    def tab_len(self) -> int:
-        raise NotImplementedError()
-
-
-class PythonIndenter(Indenter):
-    NL_type = '_NEWLINE'
-    OPEN_PAREN_types = ['LPAR', 'LSQB', 'LBRACE']
-    CLOSE_PAREN_types = ['RPAR', 'RSQB', 'RBRACE']
-    INDENT_type = '_INDENT'
-    DEDENT_type = '_DEDENT'
-    tab_len = 8
-
-###}
diff --git a/src/poetry/core/_vendor/lark/lark.py b/src/poetry/core/_vendor/lark/lark.py
deleted file mode 100644
index 6d34aa6..0000000
--- a/src/poetry/core/_vendor/lark/lark.py
+++ /dev/null
@@ -1,661 +0,0 @@
-from abc import ABC, abstractmethod
-import getpass
-import sys, os, pickle
-import tempfile
-import types
-import re
-from typing import (
-    TypeVar, Type, List, Dict, Iterator, Callable, Union, Optional, Sequence,
-    Tuple, Iterable, IO, Any, TYPE_CHECKING, Collection
-)
-if TYPE_CHECKING:
-    from .parsers.lalr_interactive_parser import InteractiveParser
-    from .tree import ParseTree
-    from .visitors import Transformer
-    if sys.version_info >= (3, 8):
-        from typing import Literal
-    else:
-        from typing_extensions import Literal
-    from .parser_frontends import ParsingFrontend
-
-from .exceptions import ConfigurationError, assert_config, UnexpectedInput
-from .utils import Serialize, SerializeMemoizer, FS, isascii, logger
-from .load_grammar import load_grammar, FromPackageLoader, Grammar, verify_used_files, PackageResource, sha256_digest
-from .tree import Tree
-from .common import LexerConf, ParserConf, _ParserArgType, _LexerArgType
-
-from .lexer import Lexer, BasicLexer, TerminalDef, LexerThread, Token
-from .parse_tree_builder import ParseTreeBuilder
-from .parser_frontends import _validate_frontend_args, _get_lexer_callbacks, _deserialize_parsing_frontend, _construct_parsing_frontend
-from .grammar import Rule
-
-
-try:
-    import regex
-    _has_regex = True
-except ImportError:
-    _has_regex = False
-
-
-###{standalone
-
-
-class PostLex(ABC):
-    @abstractmethod
-    def process(self, stream: Iterator[Token]) -> Iterator[Token]:
-        return stream
-
-    always_accept: Iterable[str] = ()
-
-class LarkOptions(Serialize):
-    """Specifies the options for Lark
-
-    """
-
-    start: List[str]
-    debug: bool
-    strict: bool
-    transformer: 'Optional[Transformer]'
-    propagate_positions: Union[bool, str]
-    maybe_placeholders: bool
-    cache: Union[bool, str]
-    regex: bool
-    g_regex_flags: int
-    keep_all_tokens: bool
-    tree_class: Optional[Callable[[str, List], Any]]
-    parser: _ParserArgType
-    lexer: _LexerArgType
-    ambiguity: 'Literal["auto", "resolve", "explicit", "forest"]'
-    postlex: Optional[PostLex]
-    priority: 'Optional[Literal["auto", "normal", "invert"]]'
-    lexer_callbacks: Dict[str, Callable[[Token], Token]]
-    use_bytes: bool
-    ordered_sets: bool
-    edit_terminals: Optional[Callable[[TerminalDef], TerminalDef]]
-    import_paths: 'List[Union[str, Callable[[Union[None, str, PackageResource], str], Tuple[str, str]]]]'
-    source_path: Optional[str]
-
-    OPTIONS_DOC = r"""
-    **===  General Options  ===**
-
-    start
-            The start symbol. Either a string, or a list of strings for multiple possible starts (Default: "start")
-    debug
-            Display debug information and extra warnings. Use only when debugging (Default: ``False``)
-            When used with Earley, it generates a forest graph as "sppf.png", if 'dot' is installed.
-    strict
-            Throw an exception on any potential ambiguity, including shift/reduce conflicts, and regex collisions.
-    transformer
-            Applies the transformer to every parse tree (equivalent to applying it after the parse, but faster)
-    propagate_positions
-            Propagates positional attributes into the 'meta' attribute of all tree branches.
-            Sets attributes: (line, column, end_line, end_column, start_pos, end_pos,
-                              container_line, container_column, container_end_line, container_end_column)
-            Accepts ``False``, ``True``, or a callable, which will filter which nodes to ignore when propagating.
-    maybe_placeholders
-            When ``True``, the ``[]`` operator returns ``None`` when not matched.
-            When ``False``,  ``[]`` behaves like the ``?`` operator, and returns no value at all.
-            (default= ``True``)
-    cache
-            Cache the results of the Lark grammar analysis, for x2 to x3 faster loading. LALR only for now.
-
-            - When ``False``, does nothing (default)
-            - When ``True``, caches to a temporary file in the local directory
-            - When given a string, caches to the path pointed by the string
-    regex
-            When True, uses the ``regex`` module instead of the stdlib ``re``.
-    g_regex_flags
-            Flags that are applied to all terminals (both regex and strings)
-    keep_all_tokens
-            Prevent the tree builder from automagically removing "punctuation" tokens (Default: ``False``)
-    tree_class
-            Lark will produce trees comprised of instances of this class instead of the default ``lark.Tree``.
-
-    **=== Algorithm Options ===**
-
-    parser
-            Decides which parser engine to use. Accepts "earley" or "lalr". (Default: "earley").
-            (there is also a "cyk" option for legacy)
-    lexer
-            Decides whether or not to use a lexer stage
-
-            - "auto" (default): Choose for me based on the parser
-            - "basic": Use a basic lexer
-            - "contextual": Stronger lexer (only works with parser="lalr")
-            - "dynamic": Flexible and powerful (only with parser="earley")
-            - "dynamic_complete": Same as dynamic, but tries *every* variation of tokenizing possible.
-    ambiguity
-            Decides how to handle ambiguity in the parse. Only relevant if parser="earley"
-
-            - "resolve": The parser will automatically choose the simplest derivation
-              (it chooses consistently: greedy for tokens, non-greedy for rules)
-            - "explicit": The parser will return all derivations wrapped in "_ambig" tree nodes (i.e. a forest).
-            - "forest": The parser will return the root of the shared packed parse forest.
-
-    **=== Misc. / Domain Specific Options ===**
-
-    postlex
-            Lexer post-processing (Default: ``None``) Only works with the basic and contextual lexers.
-    priority
-            How priorities should be evaluated - "auto", ``None``, "normal", "invert" (Default: "auto")
-    lexer_callbacks
-            Dictionary of callbacks for the lexer. May alter tokens during lexing. Use with caution.
-    use_bytes
-            Accept an input of type ``bytes`` instead of ``str``.
-    ordered_sets
-            Should Earley use ordered-sets to achieve stable output (~10% slower than regular sets. Default: True)
-    edit_terminals
-            A callback for editing the terminals before parse.
-    import_paths
-            A List of either paths or loader functions to specify from where grammars are imported
-    source_path
-            Override the source of from where the grammar was loaded. Useful for relative imports and unconventional grammar loading
-    **=== End of Options ===**
-    """
-    if __doc__:
-        __doc__ += OPTIONS_DOC
-
-
-    # Adding a new option needs to be done in multiple places:
-    # - In the dictionary below. This is the primary truth of which options `Lark.__init__` accepts
-    # - In the docstring above. It is used both for the docstring of `LarkOptions` and `Lark`, and in readthedocs
-    # - As an attribute of `LarkOptions` above
-    # - Potentially in `_LOAD_ALLOWED_OPTIONS` below this class, when the option doesn't change how the grammar is loaded
-    # - Potentially in `lark.tools.__init__`, if it makes sense, and it can easily be passed as a cmd argument
-    _defaults: Dict[str, Any] = {
-        'debug': False,
-        'strict': False,
-        'keep_all_tokens': False,
-        'tree_class': None,
-        'cache': False,
-        'postlex': None,
-        'parser': 'earley',
-        'lexer': 'auto',
-        'transformer': None,
-        'start': 'start',
-        'priority': 'auto',
-        'ambiguity': 'auto',
-        'regex': False,
-        'propagate_positions': False,
-        'lexer_callbacks': {},
-        'maybe_placeholders': True,
-        'edit_terminals': None,
-        'g_regex_flags': 0,
-        'use_bytes': False,
-        'ordered_sets': True,
-        'import_paths': [],
-        'source_path': None,
-        '_plugins': {},
-    }
-
-    def __init__(self, options_dict: Dict[str, Any]) -> None:
-        o = dict(options_dict)
-
-        options = {}
-        for name, default in self._defaults.items():
-            if name in o:
-                value = o.pop(name)
-                if isinstance(default, bool) and name not in ('cache', 'use_bytes', 'propagate_positions'):
-                    value = bool(value)
-            else:
-                value = default
-
-            options[name] = value
-
-        if isinstance(options['start'], str):
-            options['start'] = [options['start']]
-
-        self.__dict__['options'] = options
-
-
-        assert_config(self.parser, ('earley', 'lalr', 'cyk', None))
-
-        if self.parser == 'earley' and self.transformer:
-            raise ConfigurationError('Cannot specify an embedded transformer when using the Earley algorithm. '
-                             'Please use your transformer on the resulting parse tree, or use a different algorithm (i.e. LALR)')
-
-        if o:
-            raise ConfigurationError("Unknown options: %s" % o.keys())
-
-    def __getattr__(self, name: str) -> Any:
-        try:
-            return self.__dict__['options'][name]
-        except KeyError as e:
-            raise AttributeError(e)
-
-    def __setattr__(self, name: str, value: str) -> None:
-        assert_config(name, self.options.keys(), "%r isn't a valid option. Expected one of: %s")
-        self.options[name] = value
-
-    def serialize(self, memo = None) -> Dict[str, Any]:
-        return self.options
-
-    @classmethod
-    def deserialize(cls, data: Dict[str, Any], memo: Dict[int, Union[TerminalDef, Rule]]) -> "LarkOptions":
-        return cls(data)
-
-
-# Options that can be passed to the Lark parser, even when it was loaded from cache/standalone.
-# These options are only used outside of `load_grammar`.
-_LOAD_ALLOWED_OPTIONS = {'postlex', 'transformer', 'lexer_callbacks', 'use_bytes', 'debug', 'g_regex_flags', 'regex', 'propagate_positions', 'tree_class', '_plugins'}
-
-_VALID_PRIORITY_OPTIONS = ('auto', 'normal', 'invert', None)
-_VALID_AMBIGUITY_OPTIONS = ('auto', 'resolve', 'explicit', 'forest')
-
-
-_T = TypeVar('_T', bound="Lark")
-
-class Lark(Serialize):
-    """Main interface for the library.
-
-    It's mostly a thin wrapper for the many different parsers, and for the tree constructor.
-
-    Parameters:
-        grammar: a string or file-object containing the grammar spec (using Lark's ebnf syntax)
-        options: a dictionary controlling various aspects of Lark.
-
-    Example:
-        >>> Lark(r'''start: "foo" ''')
-        Lark(...)
-    """
-
-    source_path: str
-    source_grammar: str
-    grammar: 'Grammar'
-    options: LarkOptions
-    lexer: Lexer
-    parser: 'ParsingFrontend'
-    terminals: Collection[TerminalDef]
-
-    def __init__(self, grammar: 'Union[Grammar, str, IO[str]]', **options) -> None:
-        self.options = LarkOptions(options)
-        re_module: types.ModuleType
-
-        # Set regex or re module
-        use_regex = self.options.regex
-        if use_regex:
-            if _has_regex:
-                re_module = regex
-            else:
-                raise ImportError('`regex` module must be installed if calling `Lark(regex=True)`.')
-        else:
-            re_module = re
-
-        # Some, but not all file-like objects have a 'name' attribute
-        if self.options.source_path is None:
-            try:
-                self.source_path = grammar.name  # type: ignore[union-attr]
-            except AttributeError:
-                self.source_path = '<string>'
-        else:
-            self.source_path = self.options.source_path
-
-        # Drain file-like objects to get their contents
-        try:
-            read = grammar.read  # type: ignore[union-attr]
-        except AttributeError:
-            pass
-        else:
-            grammar = read()
-
-        cache_fn = None
-        cache_sha256 = None
-        if isinstance(grammar, str):
-            self.source_grammar = grammar
-            if self.options.use_bytes:
-                if not isascii(grammar):
-                    raise ConfigurationError("Grammar must be ascii only, when use_bytes=True")
-
-            if self.options.cache:
-                if self.options.parser != 'lalr':
-                    raise ConfigurationError("cache only works with parser='lalr' for now")
-
-                unhashable = ('transformer', 'postlex', 'lexer_callbacks', 'edit_terminals', '_plugins')
-                options_str = ''.join(k+str(v) for k, v in options.items() if k not in unhashable)
-                from . import __version__
-                s = grammar + options_str + __version__ + str(sys.version_info[:2])
-                cache_sha256 = sha256_digest(s)
-
-                if isinstance(self.options.cache, str):
-                    cache_fn = self.options.cache
-                else:
-                    if self.options.cache is not True:
-                        raise ConfigurationError("cache argument must be bool or str")
-
-                    try:
-                        username = getpass.getuser()
-                    except Exception:
-                        # The exception raised may be ImportError or OSError in
-                        # the future.  For the cache, we don't care about the
-                        # specific reason - we just want a username.
-                        username = "unknown"
-
-                    cache_fn = tempfile.gettempdir() + "/.lark_cache_%s_%s_%s_%s.tmp" % (username, cache_sha256, *sys.version_info[:2])
-
-                old_options = self.options
-                try:
-                    with FS.open(cache_fn, 'rb') as f:
-                        logger.debug('Loading grammar from cache: %s', cache_fn)
-                        # Remove options that aren't relevant for loading from cache
-                        for name in (set(options) - _LOAD_ALLOWED_OPTIONS):
-                            del options[name]
-                        file_sha256 = f.readline().rstrip(b'\n')
-                        cached_used_files = pickle.load(f)
-                        if file_sha256 == cache_sha256.encode('utf8') and verify_used_files(cached_used_files):
-                            cached_parser_data = pickle.load(f)
-                            self._load(cached_parser_data, **options)
-                            return
-                except FileNotFoundError:
-                    # The cache file doesn't exist; parse and compose the grammar as normal
-                    pass
-                except Exception: # We should probably narrow done which errors we catch here.
-                    logger.exception("Failed to load Lark from cache: %r. We will try to carry on.", cache_fn)
-
-                    # In theory, the Lark instance might have been messed up by the call to `_load`.
-                    # In practice the only relevant thing that might have been overwritten should be `options`
-                    self.options = old_options
-
-
-            # Parse the grammar file and compose the grammars
-            self.grammar, used_files = load_grammar(grammar, self.source_path, self.options.import_paths, self.options.keep_all_tokens)
-        else:
-            assert isinstance(grammar, Grammar)
-            self.grammar = grammar
-
-
-        if self.options.lexer == 'auto':
-            if self.options.parser == 'lalr':
-                self.options.lexer = 'contextual'
-            elif self.options.parser == 'earley':
-                if self.options.postlex is not None:
-                    logger.info("postlex can't be used with the dynamic lexer, so we use 'basic' instead. "
-                                "Consider using lalr with contextual instead of earley")
-                    self.options.lexer = 'basic'
-                else:
-                    self.options.lexer = 'dynamic'
-            elif self.options.parser == 'cyk':
-                self.options.lexer = 'basic'
-            else:
-                assert False, self.options.parser
-        lexer = self.options.lexer
-        if isinstance(lexer, type):
-            assert issubclass(lexer, Lexer)     # XXX Is this really important? Maybe just ensure interface compliance
-        else:
-            assert_config(lexer, ('basic', 'contextual', 'dynamic', 'dynamic_complete'))
-            if self.options.postlex is not None and 'dynamic' in lexer:
-                raise ConfigurationError("Can't use postlex with a dynamic lexer. Use basic or contextual instead")
-
-        if self.options.ambiguity == 'auto':
-            if self.options.parser == 'earley':
-                self.options.ambiguity = 'resolve'
-        else:
-            assert_config(self.options.parser, ('earley', 'cyk'), "%r doesn't support disambiguation. Use one of these parsers instead: %s")
-
-        if self.options.priority == 'auto':
-            self.options.priority = 'normal'
-
-        if self.options.priority not in _VALID_PRIORITY_OPTIONS:
-            raise ConfigurationError("invalid priority option: %r. Must be one of %r" % (self.options.priority, _VALID_PRIORITY_OPTIONS))
-        if self.options.ambiguity not in _VALID_AMBIGUITY_OPTIONS:
-            raise ConfigurationError("invalid ambiguity option: %r. Must be one of %r" % (self.options.ambiguity, _VALID_AMBIGUITY_OPTIONS))
-
-        if self.options.parser is None:
-            terminals_to_keep = '*'
-        elif self.options.postlex is not None:
-            terminals_to_keep = set(self.options.postlex.always_accept)
-        else:
-            terminals_to_keep = set()
-
-        # Compile the EBNF grammar into BNF
-        self.terminals, self.rules, self.ignore_tokens = self.grammar.compile(self.options.start, terminals_to_keep)
-
-        if self.options.edit_terminals:
-            for t in self.terminals:
-                self.options.edit_terminals(t)
-
-        self._terminals_dict = {t.name: t for t in self.terminals}
-
-        # If the user asked to invert the priorities, negate them all here.
-        if self.options.priority == 'invert':
-            for rule in self.rules:
-                if rule.options.priority is not None:
-                    rule.options.priority = -rule.options.priority
-            for term in self.terminals:
-                term.priority = -term.priority
-        # Else, if the user asked to disable priorities, strip them from the
-        # rules and terminals. This allows the Earley parsers to skip an extra forest walk
-        # for improved performance, if you don't need them (or didn't specify any).
-        elif self.options.priority is None:
-            for rule in self.rules:
-                if rule.options.priority is not None:
-                    rule.options.priority = None
-            for term in self.terminals:
-                term.priority = 0
-
-        # TODO Deprecate lexer_callbacks?
-        self.lexer_conf = LexerConf(
-                self.terminals, re_module, self.ignore_tokens, self.options.postlex,
-                self.options.lexer_callbacks, self.options.g_regex_flags, use_bytes=self.options.use_bytes, strict=self.options.strict
-            )
-
-        if self.options.parser:
-            self.parser = self._build_parser()
-        elif lexer:
-            self.lexer = self._build_lexer()
-
-        if cache_fn:
-            logger.debug('Saving grammar to cache: %s', cache_fn)
-            try:
-                with FS.open(cache_fn, 'wb') as f:
-                    assert cache_sha256 is not None
-                    f.write(cache_sha256.encode('utf8') + b'\n')
-                    pickle.dump(used_files, f)
-                    self.save(f, _LOAD_ALLOWED_OPTIONS)
-            except IOError as e:
-                logger.exception("Failed to save Lark to cache: %r.", cache_fn, e)
-
-    if __doc__:
-        __doc__ += "\n\n" + LarkOptions.OPTIONS_DOC
-
-    __serialize_fields__ = 'parser', 'rules', 'options'
-
-    def _build_lexer(self, dont_ignore: bool=False) -> BasicLexer:
-        lexer_conf = self.lexer_conf
-        if dont_ignore:
-            from copy import copy
-            lexer_conf = copy(lexer_conf)
-            lexer_conf.ignore = ()
-        return BasicLexer(lexer_conf)
-
-    def _prepare_callbacks(self) -> None:
-        self._callbacks = {}
-        # we don't need these callbacks if we aren't building a tree
-        if self.options.ambiguity != 'forest':
-            self._parse_tree_builder = ParseTreeBuilder(
-                    self.rules,
-                    self.options.tree_class or Tree,
-                    self.options.propagate_positions,
-                    self.options.parser != 'lalr' and self.options.ambiguity == 'explicit',
-                    self.options.maybe_placeholders
-                )
-            self._callbacks = self._parse_tree_builder.create_callback(self.options.transformer)
-        self._callbacks.update(_get_lexer_callbacks(self.options.transformer, self.terminals))
-
-    def _build_parser(self) -> "ParsingFrontend":
-        self._prepare_callbacks()
-        _validate_frontend_args(self.options.parser, self.options.lexer)
-        parser_conf = ParserConf(self.rules, self._callbacks, self.options.start)
-        return _construct_parsing_frontend(
-            self.options.parser,
-            self.options.lexer,
-            self.lexer_conf,
-            parser_conf,
-            options=self.options
-        )
-
-    def save(self, f, exclude_options: Collection[str] = ()) -> None:
-        """Saves the instance into the given file object
-
-        Useful for caching and multiprocessing.
-        """
-        if self.options.parser != 'lalr':
-            raise NotImplementedError("Lark.save() is only implemented for the LALR(1) parser.")
-        data, m = self.memo_serialize([TerminalDef, Rule])
-        if exclude_options:
-            data["options"] = {n: v for n, v in data["options"].items() if n not in exclude_options}
-        pickle.dump({'data': data, 'memo': m}, f, protocol=pickle.HIGHEST_PROTOCOL)
-
-    @classmethod
-    def load(cls: Type[_T], f) -> _T:
-        """Loads an instance from the given file object
-
-        Useful for caching and multiprocessing.
-        """
-        inst = cls.__new__(cls)
-        return inst._load(f)
-
-    def _deserialize_lexer_conf(self, data: Dict[str, Any], memo: Dict[int, Union[TerminalDef, Rule]], options: LarkOptions) -> LexerConf:
-        lexer_conf = LexerConf.deserialize(data['lexer_conf'], memo)
-        lexer_conf.callbacks = options.lexer_callbacks or {}
-        lexer_conf.re_module = regex if options.regex else re
-        lexer_conf.use_bytes = options.use_bytes
-        lexer_conf.g_regex_flags = options.g_regex_flags
-        lexer_conf.skip_validation = True
-        lexer_conf.postlex = options.postlex
-        return lexer_conf
-
-    def _load(self: _T, f: Any, **kwargs) -> _T:
-        if isinstance(f, dict):
-            d = f
-        else:
-            d = pickle.load(f)
-        memo_json = d['memo']
-        data = d['data']
-
-        assert memo_json
-        memo = SerializeMemoizer.deserialize(memo_json, {'Rule': Rule, 'TerminalDef': TerminalDef}, {})
-        options = dict(data['options'])
-        if (set(kwargs) - _LOAD_ALLOWED_OPTIONS) & set(LarkOptions._defaults):
-            raise ConfigurationError("Some options are not allowed when loading a Parser: {}"
-                             .format(set(kwargs) - _LOAD_ALLOWED_OPTIONS))
-        options.update(kwargs)
-        self.options = LarkOptions.deserialize(options, memo)
-        self.rules = [Rule.deserialize(r, memo) for r in data['rules']]
-        self.source_path = '<deserialized>'
-        _validate_frontend_args(self.options.parser, self.options.lexer)
-        self.lexer_conf = self._deserialize_lexer_conf(data['parser'], memo, self.options)
-        self.terminals = self.lexer_conf.terminals
-        self._prepare_callbacks()
-        self._terminals_dict = {t.name: t for t in self.terminals}
-        self.parser = _deserialize_parsing_frontend(
-            data['parser'],
-            memo,
-            self.lexer_conf,
-            self._callbacks,
-            self.options,  # Not all, but multiple attributes are used
-        )
-        return self
-
-    @classmethod
-    def _load_from_dict(cls, data, memo, **kwargs):
-        inst = cls.__new__(cls)
-        return inst._load({'data': data, 'memo': memo}, **kwargs)
-
-    @classmethod
-    def open(cls: Type[_T], grammar_filename: str, rel_to: Optional[str]=None, **options) -> _T:
-        """Create an instance of Lark with the grammar given by its filename
-
-        If ``rel_to`` is provided, the function will find the grammar filename in relation to it.
-
-        Example:
-
-            >>> Lark.open("grammar_file.lark", rel_to=__file__, parser="lalr")
-            Lark(...)
-
-        """
-        if rel_to:
-            basepath = os.path.dirname(rel_to)
-            grammar_filename = os.path.join(basepath, grammar_filename)
-        with open(grammar_filename, encoding='utf8') as f:
-            return cls(f, **options)
-
-    @classmethod
-    def open_from_package(cls: Type[_T], package: str, grammar_path: str, search_paths: 'Sequence[str]'=[""], **options) -> _T:
-        """Create an instance of Lark with the grammar loaded from within the package `package`.
-        This allows grammar loading from zipapps.
-
-        Imports in the grammar will use the `package` and `search_paths` provided, through `FromPackageLoader`
-
-        Example:
-
-            Lark.open_from_package(__name__, "example.lark", ("grammars",), parser=...)
-        """
-        package_loader = FromPackageLoader(package, search_paths)
-        full_path, text = package_loader(None, grammar_path)
-        options.setdefault('source_path', full_path)
-        options.setdefault('import_paths', [])
-        options['import_paths'].append(package_loader)
-        return cls(text, **options)
-
-    def __repr__(self):
-        return 'Lark(open(%r), parser=%r, lexer=%r, ...)' % (self.source_path, self.options.parser, self.options.lexer)
-
-
-    def lex(self, text: str, dont_ignore: bool=False) -> Iterator[Token]:
-        """Only lex (and postlex) the text, without parsing it. Only relevant when lexer='basic'
-
-        When dont_ignore=True, the lexer will return all tokens, even those marked for %ignore.
-
-        :raises UnexpectedCharacters: In case the lexer cannot find a suitable match.
-        """
-        lexer: Lexer
-        if not hasattr(self, 'lexer') or dont_ignore:
-            lexer = self._build_lexer(dont_ignore)
-        else:
-            lexer = self.lexer
-        lexer_thread = LexerThread.from_text(lexer, text)
-        stream = lexer_thread.lex(None)
-        if self.options.postlex:
-            return self.options.postlex.process(stream)
-        return stream
-
-    def get_terminal(self, name: str) -> TerminalDef:
-        """Get information about a terminal"""
-        return self._terminals_dict[name]
-
-    def parse_interactive(self, text: Optional[str]=None, start: Optional[str]=None) -> 'InteractiveParser':
-        """Start an interactive parsing session.
-
-        Parameters:
-            text (str, optional): Text to be parsed. Required for ``resume_parse()``.
-            start (str, optional): Start symbol
-
-        Returns:
-            A new InteractiveParser instance.
-
-        See Also: ``Lark.parse()``
-        """
-        return self.parser.parse_interactive(text, start=start)
-
-    def parse(self, text: str, start: Optional[str]=None, on_error: 'Optional[Callable[[UnexpectedInput], bool]]'=None) -> 'ParseTree':
-        """Parse the given text, according to the options provided.
-
-        Parameters:
-            text (str): Text to be parsed.
-            start (str, optional): Required if Lark was given multiple possible start symbols (using the start option).
-            on_error (function, optional): if provided, will be called on UnexpectedToken error. Return true to resume parsing.
-                LALR only. See examples/advanced/error_handling.py for an example of how to use on_error.
-
-        Returns:
-            If a transformer is supplied to ``__init__``, returns whatever is the
-            result of the transformation. Otherwise, returns a Tree instance.
-
-        :raises UnexpectedInput: On a parse error, one of these sub-exceptions will rise:
-                ``UnexpectedCharacters``, ``UnexpectedToken``, or ``UnexpectedEOF``.
-                For convenience, these sub-exceptions also inherit from ``ParserError`` and ``LexerError``.
-
-        """
-        return self.parser.parse(text, start=start, on_error=on_error)
-
-
-###}
diff --git a/src/poetry/core/_vendor/lark/lexer.py b/src/poetry/core/_vendor/lark/lexer.py
deleted file mode 100644
index 9061d60..0000000
--- a/src/poetry/core/_vendor/lark/lexer.py
+++ /dev/null
@@ -1,678 +0,0 @@
-# Lexer Implementation
-
-from abc import abstractmethod, ABC
-import re
-from contextlib import suppress
-from typing import (
-    TypeVar, Type, Dict, Iterator, Collection, Callable, Optional, FrozenSet, Any,
-    ClassVar, TYPE_CHECKING, overload
-)
-from types import ModuleType
-import warnings
-try:
-    import interegular
-except ImportError:
-    pass
-if TYPE_CHECKING:
-    from .common import LexerConf
-    from .parsers.lalr_parser_state import ParserState
-
-from .utils import classify, get_regexp_width, Serialize, logger
-from .exceptions import UnexpectedCharacters, LexError, UnexpectedToken
-from .grammar import TOKEN_DEFAULT_PRIORITY
-
-
-###{standalone
-from copy import copy
-
-try:  # For the standalone parser, we need to make sure that has_interegular is False to avoid NameErrors later on
-    has_interegular = bool(interegular)
-except NameError:
-    has_interegular = False
-
-class Pattern(Serialize, ABC):
-    "An abstraction over regular expressions."
-
-    value: str
-    flags: Collection[str]
-    raw: Optional[str]
-    type: ClassVar[str]
-
-    def __init__(self, value: str, flags: Collection[str] = (), raw: Optional[str] = None) -> None:
-        self.value = value
-        self.flags = frozenset(flags)
-        self.raw = raw
-
-    def __repr__(self):
-        return repr(self.to_regexp())
-
-    # Pattern Hashing assumes all subclasses have a different priority!
-    def __hash__(self):
-        return hash((type(self), self.value, self.flags))
-
-    def __eq__(self, other):
-        return type(self) == type(other) and self.value == other.value and self.flags == other.flags
-
-    @abstractmethod
-    def to_regexp(self) -> str:
-        raise NotImplementedError()
-
-    @property
-    @abstractmethod
-    def min_width(self) -> int:
-        raise NotImplementedError()
-
-    @property
-    @abstractmethod
-    def max_width(self) -> int:
-        raise NotImplementedError()
-
-    def _get_flags(self, value):
-        for f in self.flags:
-            value = ('(?%s:%s)' % (f, value))
-        return value
-
-
-class PatternStr(Pattern):
-    __serialize_fields__ = 'value', 'flags', 'raw'
-
-    type: ClassVar[str] = "str"
-
-    def to_regexp(self) -> str:
-        return self._get_flags(re.escape(self.value))
-
-    @property
-    def min_width(self) -> int:
-        return len(self.value)
-
-    @property
-    def max_width(self) -> int:
-        return len(self.value)
-
-
-class PatternRE(Pattern):
-    __serialize_fields__ = 'value', 'flags', 'raw', '_width'
-
-    type: ClassVar[str] = "re"
-
-    def to_regexp(self) -> str:
-        return self._get_flags(self.value)
-
-    _width = None
-    def _get_width(self):
-        if self._width is None:
-            self._width = get_regexp_width(self.to_regexp())
-        return self._width
-
-    @property
-    def min_width(self) -> int:
-        return self._get_width()[0]
-
-    @property
-    def max_width(self) -> int:
-        return self._get_width()[1]
-
-
-class TerminalDef(Serialize):
-    "A definition of a terminal"
-    __serialize_fields__ = 'name', 'pattern', 'priority'
-    __serialize_namespace__ = PatternStr, PatternRE
-
-    name: str
-    pattern: Pattern
-    priority: int
-
-    def __init__(self, name: str, pattern: Pattern, priority: int = TOKEN_DEFAULT_PRIORITY) -> None:
-        assert isinstance(pattern, Pattern), pattern
-        self.name = name
-        self.pattern = pattern
-        self.priority = priority
-
-    def __repr__(self):
-        return '%s(%r, %r)' % (type(self).__name__, self.name, self.pattern)
-
-    def user_repr(self) -> str:
-        if self.name.startswith('__'):  # We represent a generated terminal
-            return self.pattern.raw or self.name
-        else:
-            return self.name
-
-_T = TypeVar('_T', bound="Token")
-
-class Token(str):
-    """A string with meta-information, that is produced by the lexer.
-
-    When parsing text, the resulting chunks of the input that haven't been discarded,
-    will end up in the tree as Token instances. The Token class inherits from Python's ``str``,
-    so normal string comparisons and operations will work as expected.
-
-    Attributes:
-        type: Name of the token (as specified in grammar)
-        value: Value of the token (redundant, as ``token.value == token`` will always be true)
-        start_pos: The index of the token in the text
-        line: The line of the token in the text (starting with 1)
-        column: The column of the token in the text (starting with 1)
-        end_line: The line where the token ends
-        end_column: The next column after the end of the token. For example,
-            if the token is a single character with a column value of 4,
-            end_column will be 5.
-        end_pos: the index where the token ends (basically ``start_pos + len(token)``)
-    """
-    __slots__ = ('type', 'start_pos', 'value', 'line', 'column', 'end_line', 'end_column', 'end_pos')
-
-    __match_args__ = ('type', 'value')
-
-    type: str
-    start_pos: Optional[int]
-    value: Any
-    line: Optional[int]
-    column: Optional[int]
-    end_line: Optional[int]
-    end_column: Optional[int]
-    end_pos: Optional[int]
-
-
-    @overload
-    def __new__(
-            cls,
-            type: str,
-            value: Any,
-            start_pos: Optional[int] = None,
-            line: Optional[int] = None,
-            column: Optional[int] = None,
-            end_line: Optional[int] = None,
-            end_column: Optional[int] = None,
-            end_pos: Optional[int] = None
-    ) -> 'Token':
-        ...
-
-    @overload
-    def __new__(
-            cls,
-            type_: str,
-            value: Any,
-            start_pos: Optional[int] = None,
-            line: Optional[int] = None,
-            column: Optional[int] = None,
-            end_line: Optional[int] = None,
-            end_column: Optional[int] = None,
-            end_pos: Optional[int] = None
-    ) -> 'Token':        ...
-
-    def __new__(cls, *args, **kwargs):
-        if "type_" in kwargs:
-            warnings.warn("`type_` is deprecated use `type` instead", DeprecationWarning)
-
-            if "type" in kwargs:
-                raise TypeError("Error: using both 'type' and the deprecated 'type_' as arguments.")
-            kwargs["type"] = kwargs.pop("type_")
-
-        return cls._future_new(*args, **kwargs)
-
-
-    @classmethod
-    def _future_new(cls, type, value, start_pos=None, line=None, column=None, end_line=None, end_column=None, end_pos=None):
-        inst = super(Token, cls).__new__(cls, value)
-
-        inst.type = type
-        inst.start_pos = start_pos
-        inst.value = value
-        inst.line = line
-        inst.column = column
-        inst.end_line = end_line
-        inst.end_column = end_column
-        inst.end_pos = end_pos
-        return inst
-
-    @overload
-    def update(self, type: Optional[str] = None, value: Optional[Any] = None) -> 'Token':
-        ...
-
-    @overload
-    def update(self, type_: Optional[str] = None, value: Optional[Any] = None) -> 'Token':
-        ...
-
-    def update(self, *args, **kwargs):
-        if "type_" in kwargs:
-            warnings.warn("`type_` is deprecated use `type` instead", DeprecationWarning)
-
-            if "type" in kwargs:
-                raise TypeError("Error: using both 'type' and the deprecated 'type_' as arguments.")
-            kwargs["type"] = kwargs.pop("type_")
-
-        return self._future_update(*args, **kwargs)
-
-    def _future_update(self, type: Optional[str] = None, value: Optional[Any] = None) -> 'Token':
-        return Token.new_borrow_pos(
-            type if type is not None else self.type,
-            value if value is not None else self.value,
-            self
-        )
-
-    @classmethod
-    def new_borrow_pos(cls: Type[_T], type_: str, value: Any, borrow_t: 'Token') -> _T:
-        return cls(type_, value, borrow_t.start_pos, borrow_t.line, borrow_t.column, borrow_t.end_line, borrow_t.end_column, borrow_t.end_pos)
-
-    def __reduce__(self):
-        return (self.__class__, (self.type, self.value, self.start_pos, self.line, self.column))
-
-    def __repr__(self):
-        return 'Token(%r, %r)' % (self.type, self.value)
-
-    def __deepcopy__(self, memo):
-        return Token(self.type, self.value, self.start_pos, self.line, self.column)
-
-    def __eq__(self, other):
-        if isinstance(other, Token) and self.type != other.type:
-            return False
-
-        return str.__eq__(self, other)
-
-    __hash__ = str.__hash__
-
-
-class LineCounter:
-    "A utility class for keeping track of line & column information"
-
-    __slots__ = 'char_pos', 'line', 'column', 'line_start_pos', 'newline_char'
-
-    def __init__(self, newline_char):
-        self.newline_char = newline_char
-        self.char_pos = 0
-        self.line = 1
-        self.column = 1
-        self.line_start_pos = 0
-
-    def __eq__(self, other):
-        if not isinstance(other, LineCounter):
-            return NotImplemented
-
-        return self.char_pos == other.char_pos and self.newline_char == other.newline_char
-
-    def feed(self, token: Token, test_newline=True):
-        """Consume a token and calculate the new line & column.
-
-        As an optional optimization, set test_newline=False if token doesn't contain a newline.
-        """
-        if test_newline:
-            newlines = token.count(self.newline_char)
-            if newlines:
-                self.line += newlines
-                self.line_start_pos = self.char_pos + token.rindex(self.newline_char) + 1
-
-        self.char_pos += len(token)
-        self.column = self.char_pos - self.line_start_pos + 1
-
-
-class UnlessCallback:
-    def __init__(self, scanner):
-        self.scanner = scanner
-
-    def __call__(self, t):
-        res = self.scanner.match(t.value, 0)
-        if res:
-            _value, t.type = res
-        return t
-
-
-class CallChain:
-    def __init__(self, callback1, callback2, cond):
-        self.callback1 = callback1
-        self.callback2 = callback2
-        self.cond = cond
-
-    def __call__(self, t):
-        t2 = self.callback1(t)
-        return self.callback2(t) if self.cond(t2) else t2
-
-
-def _get_match(re_, regexp, s, flags):
-    m = re_.match(regexp, s, flags)
-    if m:
-        return m.group(0)
-
-def _create_unless(terminals, g_regex_flags, re_, use_bytes):
-    tokens_by_type = classify(terminals, lambda t: type(t.pattern))
-    assert len(tokens_by_type) <= 2, tokens_by_type.keys()
-    embedded_strs = set()
-    callback = {}
-    for retok in tokens_by_type.get(PatternRE, []):
-        unless = []
-        for strtok in tokens_by_type.get(PatternStr, []):
-            if strtok.priority != retok.priority:
-                continue
-            s = strtok.pattern.value
-            if s == _get_match(re_, retok.pattern.to_regexp(), s, g_regex_flags):
-                unless.append(strtok)
-                if strtok.pattern.flags <= retok.pattern.flags:
-                    embedded_strs.add(strtok)
-        if unless:
-            callback[retok.name] = UnlessCallback(Scanner(unless, g_regex_flags, re_, match_whole=True, use_bytes=use_bytes))
-
-    new_terminals = [t for t in terminals if t not in embedded_strs]
-    return new_terminals, callback
-
-
-class Scanner:
-    def __init__(self, terminals, g_regex_flags, re_, use_bytes, match_whole=False):
-        self.terminals = terminals
-        self.g_regex_flags = g_regex_flags
-        self.re_ = re_
-        self.use_bytes = use_bytes
-        self.match_whole = match_whole
-
-        self.allowed_types = {t.name for t in self.terminals}
-
-        self._mres = self._build_mres(terminals, len(terminals))
-
-    def _build_mres(self, terminals, max_size):
-        # Python sets an unreasonable group limit (currently 100) in its re module
-        # Worse, the only way to know we reached it is by catching an AssertionError!
-        # This function recursively tries less and less groups until it's successful.
-        postfix = '$' if self.match_whole else ''
-        mres = []
-        while terminals:
-            pattern = u'|'.join(u'(?P<%s>%s)' % (t.name, t.pattern.to_regexp() + postfix) for t in terminals[:max_size])
-            if self.use_bytes:
-                pattern = pattern.encode('latin-1')
-            try:
-                mre = self.re_.compile(pattern, self.g_regex_flags)
-            except AssertionError:  # Yes, this is what Python provides us.. :/
-                return self._build_mres(terminals, max_size // 2)
-
-            mres.append(mre)
-            terminals = terminals[max_size:]
-        return mres
-
-    def match(self, text, pos):
-        for mre in self._mres:
-            m = mre.match(text, pos)
-            if m:
-                return m.group(0), m.lastgroup
-
-
-def _regexp_has_newline(r: str):
-    r"""Expressions that may indicate newlines in a regexp:
-        - newlines (\n)
-        - escaped newline (\\n)
-        - anything but ([^...])
-        - any-char (.) when the flag (?s) exists
-        - spaces (\s)
-    """
-    return '\n' in r or '\\n' in r or '\\s' in r or '[^' in r or ('(?s' in r and '.' in r)
-
-
-class LexerState:
-    """Represents the current state of the lexer as it scans the text
-    (Lexer objects are only instantiated per grammar, not per text)
-    """
-
-    __slots__ = 'text', 'line_ctr', 'last_token'
-
-    text: str
-    line_ctr: LineCounter
-    last_token: Optional[Token]
-
-    def __init__(self, text: str, line_ctr: Optional[LineCounter]=None, last_token: Optional[Token]=None):
-        self.text = text
-        self.line_ctr = line_ctr or LineCounter(b'\n' if isinstance(text, bytes) else '\n')
-        self.last_token = last_token
-
-    def __eq__(self, other):
-        if not isinstance(other, LexerState):
-            return NotImplemented
-
-        return self.text is other.text and self.line_ctr == other.line_ctr and self.last_token == other.last_token
-
-    def __copy__(self):
-        return type(self)(self.text, copy(self.line_ctr), self.last_token)
-
-
-class LexerThread:
-    """A thread that ties a lexer instance and a lexer state, to be used by the parser
-    """
-
-    def __init__(self, lexer: 'Lexer', lexer_state: LexerState):
-        self.lexer = lexer
-        self.state = lexer_state
-
-    @classmethod
-    def from_text(cls, lexer: 'Lexer', text: str) -> 'LexerThread':
-        return cls(lexer, LexerState(text))
-
-    def lex(self, parser_state):
-        return self.lexer.lex(self.state, parser_state)
-
-    def __copy__(self):
-        return type(self)(self.lexer, copy(self.state))
-
-    _Token = Token
-
-
-_Callback = Callable[[Token], Token]
-
-class Lexer(ABC):
-    """Lexer interface
-
-    Method Signatures:
-        lex(self, lexer_state, parser_state) -> Iterator[Token]
-    """
-    @abstractmethod
-    def lex(self, lexer_state: LexerState, parser_state: Any) -> Iterator[Token]:
-        return NotImplemented
-
-    def make_lexer_state(self, text):
-        "Deprecated"
-        return LexerState(text)
-
-
-def _check_regex_collisions(terminal_to_regexp: Dict[TerminalDef, str], comparator, strict_mode, max_collisions_to_show=8):
-    if not comparator:
-        comparator = interegular.Comparator.from_regexes(terminal_to_regexp)
-
-    # When in strict mode, we only ever try to provide one example, so taking
-    # a long time for that should be fine
-    max_time = 2 if strict_mode else 0.2
-
-    # We don't want to show too many collisions.
-    if comparator.count_marked_pairs() >= max_collisions_to_show:
-        return
-    for group in classify(terminal_to_regexp, lambda t: t.priority).values():
-        for a, b in comparator.check(group, skip_marked=True):
-            assert a.priority == b.priority
-            # Mark this pair to not repeat warnings when multiple different BasicLexers see the same collision
-            comparator.mark(a, b)
-
-            # Notify the user
-            message = f"Collision between Terminals {a.name} and {b.name}. "
-            try:
-                example = comparator.get_example_overlap(a, b, max_time).format_multiline()
-            except ValueError:
-                # Couldn't find an example within max_time steps.
-                example = "No example could be found fast enough. However, the collision does still exists"
-            if strict_mode:
-                raise LexError(f"{message}\n{example}")
-            logger.warning("%s The lexer will choose between them arbitrarily.\n%s", message, example)
-            if comparator.count_marked_pairs() >= max_collisions_to_show:
-                logger.warning("Found 8 regex collisions, will not check for more.")
-                return
-
-
-class AbstractBasicLexer(Lexer):
-    terminals_by_name: Dict[str, TerminalDef]
-
-    @abstractmethod
-    def __init__(self, conf: 'LexerConf', comparator=None) -> None:
-        ...
-
-    @abstractmethod
-    def next_token(self, lex_state: LexerState, parser_state: Any = None) -> Token:
-        ...
-
-    def lex(self, state: LexerState, parser_state: Any) -> Iterator[Token]:
-        with suppress(EOFError):
-            while True:
-                yield self.next_token(state, parser_state)
-
-
-class BasicLexer(AbstractBasicLexer):
-    terminals: Collection[TerminalDef]
-    ignore_types: FrozenSet[str]
-    newline_types: FrozenSet[str]
-    user_callbacks: Dict[str, _Callback]
-    callback: Dict[str, _Callback]
-    re: ModuleType
-
-    def __init__(self, conf: 'LexerConf', comparator=None) -> None:
-        terminals = list(conf.terminals)
-        assert all(isinstance(t, TerminalDef) for t in terminals), terminals
-
-        self.re = conf.re_module
-
-        if not conf.skip_validation:
-            # Sanitization
-            terminal_to_regexp = {}
-            for t in terminals:
-                regexp = t.pattern.to_regexp()
-                try:
-                    self.re.compile(regexp, conf.g_regex_flags)
-                except self.re.error:
-                    raise LexError("Cannot compile token %s: %s" % (t.name, t.pattern))
-
-                if t.pattern.min_width == 0:
-                    raise LexError("Lexer does not allow zero-width terminals. (%s: %s)" % (t.name, t.pattern))
-                if t.pattern.type == "re":
-                    terminal_to_regexp[t] = regexp
-
-            if not (set(conf.ignore) <= {t.name for t in terminals}):
-                raise LexError("Ignore terminals are not defined: %s" % (set(conf.ignore) - {t.name for t in terminals}))
-
-            if has_interegular:
-                _check_regex_collisions(terminal_to_regexp, comparator, conf.strict)
-            elif conf.strict:
-                raise LexError("interegular must be installed for strict mode. Use `pip install 'lark[interegular]'`.")
-
-        # Init
-        self.newline_types = frozenset(t.name for t in terminals if _regexp_has_newline(t.pattern.to_regexp()))
-        self.ignore_types = frozenset(conf.ignore)
-
-        terminals.sort(key=lambda x: (-x.priority, -x.pattern.max_width, -len(x.pattern.value), x.name))
-        self.terminals = terminals
-        self.user_callbacks = conf.callbacks
-        self.g_regex_flags = conf.g_regex_flags
-        self.use_bytes = conf.use_bytes
-        self.terminals_by_name = conf.terminals_by_name
-
-        self._scanner = None
-
-    def _build_scanner(self):
-        terminals, self.callback = _create_unless(self.terminals, self.g_regex_flags, self.re, self.use_bytes)
-        assert all(self.callback.values())
-
-        for type_, f in self.user_callbacks.items():
-            if type_ in self.callback:
-                # Already a callback there, probably UnlessCallback
-                self.callback[type_] = CallChain(self.callback[type_], f, lambda t: t.type == type_)
-            else:
-                self.callback[type_] = f
-
-        self._scanner = Scanner(terminals, self.g_regex_flags, self.re, self.use_bytes)
-
-    @property
-    def scanner(self):
-        if self._scanner is None:
-            self._build_scanner()
-        return self._scanner
-
-    def match(self, text, pos):
-        return self.scanner.match(text, pos)
-
-    def next_token(self, lex_state: LexerState, parser_state: Any = None) -> Token:
-        line_ctr = lex_state.line_ctr
-        while line_ctr.char_pos < len(lex_state.text):
-            res = self.match(lex_state.text, line_ctr.char_pos)
-            if not res:
-                allowed = self.scanner.allowed_types - self.ignore_types
-                if not allowed:
-                    allowed = {"<END-OF-FILE>"}
-                raise UnexpectedCharacters(lex_state.text, line_ctr.char_pos, line_ctr.line, line_ctr.column,
-                                           allowed=allowed, token_history=lex_state.last_token and [lex_state.last_token],
-                                           state=parser_state, terminals_by_name=self.terminals_by_name)
-
-            value, type_ = res
-
-            ignored = type_ in self.ignore_types
-            t = None
-            if not ignored or type_ in self.callback:
-                t = Token(type_, value, line_ctr.char_pos, line_ctr.line, line_ctr.column)
-            line_ctr.feed(value, type_ in self.newline_types)
-            if t is not None:
-                t.end_line = line_ctr.line
-                t.end_column = line_ctr.column
-                t.end_pos = line_ctr.char_pos
-                if t.type in self.callback:
-                    t = self.callback[t.type](t)
-                if not ignored:
-                    if not isinstance(t, Token):
-                        raise LexError("Callbacks must return a token (returned %r)" % t)
-                    lex_state.last_token = t
-                    return t
-
-        # EOF
-        raise EOFError(self)
-
-
-class ContextualLexer(Lexer):
-    lexers: Dict[int, AbstractBasicLexer]
-    root_lexer: AbstractBasicLexer
-
-    BasicLexer: Type[AbstractBasicLexer] = BasicLexer
-
-    def __init__(self, conf: 'LexerConf', states: Dict[int, Collection[str]], always_accept: Collection[str]=()) -> None:
-        terminals = list(conf.terminals)
-        terminals_by_name = conf.terminals_by_name
-
-        trad_conf = copy(conf)
-        trad_conf.terminals = terminals
-
-        if has_interegular and not conf.skip_validation:
-            comparator = interegular.Comparator.from_regexes({t: t.pattern.to_regexp() for t in terminals})
-        else:
-            comparator = None
-        lexer_by_tokens: Dict[FrozenSet[str], AbstractBasicLexer] = {}
-        self.lexers = {}
-        for state, accepts in states.items():
-            key = frozenset(accepts)
-            try:
-                lexer = lexer_by_tokens[key]
-            except KeyError:
-                accepts = set(accepts) | set(conf.ignore) | set(always_accept)
-                lexer_conf = copy(trad_conf)
-                lexer_conf.terminals = [terminals_by_name[n] for n in accepts if n in terminals_by_name]
-                lexer = self.BasicLexer(lexer_conf, comparator)
-                lexer_by_tokens[key] = lexer
-
-            self.lexers[state] = lexer
-
-        assert trad_conf.terminals is terminals
-        trad_conf.skip_validation = True  # We don't need to verify all terminals again
-        self.root_lexer = self.BasicLexer(trad_conf, comparator)
-
-    def lex(self, lexer_state: LexerState, parser_state: 'ParserState') -> Iterator[Token]:
-        try:
-            while True:
-                lexer = self.lexers[parser_state.position]
-                yield lexer.next_token(lexer_state, parser_state)
-        except EOFError:
-            pass
-        except UnexpectedCharacters as e:
-            # In the contextual lexer, UnexpectedCharacters can mean that the terminal is defined, but not in the current context.
-            # This tests the input against the global context, to provide a nicer error.
-            try:
-                last_token = lexer_state.last_token  # Save last_token. Calling root_lexer.next_token will change this to the wrong token
-                token = self.root_lexer.next_token(lexer_state, parser_state)
-                raise UnexpectedToken(token, e.allowed, state=parser_state, token_history=[last_token], terminals_by_name=self.root_lexer.terminals_by_name)
-            except UnexpectedCharacters:
-                raise e  # Raise the original UnexpectedCharacters. The root lexer raises it with the wrong expected set.
-
-###}
diff --git a/src/poetry/core/_vendor/lark/load_grammar.py b/src/poetry/core/_vendor/lark/load_grammar.py
deleted file mode 100644
index 362a845..0000000
--- a/src/poetry/core/_vendor/lark/load_grammar.py
+++ /dev/null
@@ -1,1428 +0,0 @@
-"""Parses and compiles Lark grammars into an internal representation.
-"""
-
-import hashlib
-import os.path
-import sys
-from collections import namedtuple
-from copy import copy, deepcopy
-import pkgutil
-from ast import literal_eval
-from contextlib import suppress
-from typing import List, Tuple, Union, Callable, Dict, Optional, Sequence, Generator
-
-from .utils import bfs, logger, classify_bool, is_id_continue, is_id_start, bfs_all_unique, small_factors, OrderedSet
-from .lexer import Token, TerminalDef, PatternStr, PatternRE, Pattern
-
-from .parse_tree_builder import ParseTreeBuilder
-from .parser_frontends import ParsingFrontend
-from .common import LexerConf, ParserConf
-from .grammar import RuleOptions, Rule, Terminal, NonTerminal, Symbol, TOKEN_DEFAULT_PRIORITY
-from .utils import classify, dedup_list
-from .exceptions import GrammarError, UnexpectedCharacters, UnexpectedToken, ParseError, UnexpectedInput
-
-from .tree import Tree, SlottedTree as ST
-from .visitors import Transformer, Visitor, v_args, Transformer_InPlace, Transformer_NonRecursive
-inline_args = v_args(inline=True)
-
-IMPORT_PATHS = ['grammars']
-
-EXT = '.lark'
-
-_RE_FLAGS = 'imslux'
-
-_EMPTY = Symbol('__empty__')
-
-_TERMINAL_NAMES = {
-    '.' : 'DOT',
-    ',' : 'COMMA',
-    ':' : 'COLON',
-    ';' : 'SEMICOLON',
-    '+' : 'PLUS',
-    '-' : 'MINUS',
-    '*' : 'STAR',
-    '/' : 'SLASH',
-    '\\' : 'BACKSLASH',
-    '|' : 'VBAR',
-    '?' : 'QMARK',
-    '!' : 'BANG',
-    '@' : 'AT',
-    '#' : 'HASH',
-    '$' : 'DOLLAR',
-    '%' : 'PERCENT',
-    '^' : 'CIRCUMFLEX',
-    '&' : 'AMPERSAND',
-    '_' : 'UNDERSCORE',
-    '<' : 'LESSTHAN',
-    '>' : 'MORETHAN',
-    '=' : 'EQUAL',
-    '"' : 'DBLQUOTE',
-    '\'' : 'QUOTE',
-    '`' : 'BACKQUOTE',
-    '~' : 'TILDE',
-    '(' : 'LPAR',
-    ')' : 'RPAR',
-    '{' : 'LBRACE',
-    '}' : 'RBRACE',
-    '[' : 'LSQB',
-    ']' : 'RSQB',
-    '\n' : 'NEWLINE',
-    '\r\n' : 'CRLF',
-    '\t' : 'TAB',
-    ' ' : 'SPACE',
-}
-
-# Grammar Parser
-TERMINALS = {
-    '_LPAR': r'\(',
-    '_RPAR': r'\)',
-    '_LBRA': r'\[',
-    '_RBRA': r'\]',
-    '_LBRACE': r'\{',
-    '_RBRACE': r'\}',
-    'OP': '[+*]|[?](?![a-z_])',
-    '_COLON': ':',
-    '_COMMA': ',',
-    '_OR': r'\|',
-    '_DOT': r'\.(?!\.)',
-    '_DOTDOT': r'\.\.',
-    'TILDE': '~',
-    'RULE_MODIFIERS': '(!|![?]?|[?]!?)(?=[_a-z])',
-    'RULE': '_?[a-z][_a-z0-9]*',
-    'TERMINAL': '_?[A-Z][_A-Z0-9]*',
-    'STRING': r'"(\\"|\\\\|[^"\n])*?"i?',
-    'REGEXP': r'/(?!/)(\\/|\\\\|[^/])*?/[%s]*' % _RE_FLAGS,
-    '_NL': r'(\r?\n)+\s*',
-    '_NL_OR': r'(\r?\n)+\s*\|',
-    'WS': r'[ \t]+',
-    'COMMENT': r'\s*//[^\n]*|\s*#[^\n]*',
-    'BACKSLASH': r'\\[ ]*\n',
-    '_TO': '->',
-    '_IGNORE': r'%ignore',
-    '_OVERRIDE': r'%override',
-    '_DECLARE': r'%declare',
-    '_EXTEND': r'%extend',
-    '_IMPORT': r'%import',
-    'NUMBER': r'[+-]?\d+',
-}
-
-RULES = {
-    'start': ['_list'],
-    '_list':  ['_item', '_list _item'],
-    '_item':  ['rule', 'term', 'ignore', 'import', 'declare', 'override', 'extend', '_NL'],
-
-    'rule': ['rule_modifiers RULE template_params priority _COLON expansions _NL'],
-    'rule_modifiers': ['RULE_MODIFIERS',
-                       ''],
-    'priority': ['_DOT NUMBER',
-                 ''],
-    'template_params': ['_LBRACE _template_params _RBRACE',
-                        ''],
-    '_template_params': ['RULE',
-                         '_template_params _COMMA RULE'],
-    'expansions': ['_expansions'],
-    '_expansions': ['alias',
-                    '_expansions _OR alias',
-                    '_expansions _NL_OR alias'],
-
-    '?alias':     ['expansion _TO nonterminal', 'expansion'],
-    'expansion': ['_expansion'],
-
-    '_expansion': ['', '_expansion expr'],
-
-    '?expr': ['atom',
-              'atom OP',
-              'atom TILDE NUMBER',
-              'atom TILDE NUMBER _DOTDOT NUMBER',
-              ],
-
-    '?atom': ['_LPAR expansions _RPAR',
-              'maybe',
-              'value'],
-
-    'value': ['terminal',
-              'nonterminal',
-              'literal',
-              'range',
-              'template_usage'],
-
-    'terminal': ['TERMINAL'],
-    'nonterminal': ['RULE'],
-
-    '?name': ['RULE', 'TERMINAL'],
-    '?symbol': ['terminal', 'nonterminal'],
-
-    'maybe': ['_LBRA expansions _RBRA'],
-    'range': ['STRING _DOTDOT STRING'],
-
-    'template_usage': ['nonterminal _LBRACE _template_args _RBRACE'],
-    '_template_args': ['value',
-                       '_template_args _COMMA value'],
-
-    'term': ['TERMINAL _COLON expansions _NL',
-             'TERMINAL _DOT NUMBER _COLON expansions _NL'],
-    'override': ['_OVERRIDE rule',
-                 '_OVERRIDE term'],
-    'extend': ['_EXTEND rule',
-               '_EXTEND term'],
-    'ignore': ['_IGNORE expansions _NL'],
-    'declare': ['_DECLARE _declare_args _NL'],
-    'import': ['_IMPORT _import_path _NL',
-               '_IMPORT _import_path _LPAR name_list _RPAR _NL',
-               '_IMPORT _import_path _TO name _NL'],
-
-    '_import_path': ['import_lib', 'import_rel'],
-    'import_lib': ['_import_args'],
-    'import_rel': ['_DOT _import_args'],
-    '_import_args': ['name', '_import_args _DOT name'],
-
-    'name_list': ['_name_list'],
-    '_name_list': ['name', '_name_list _COMMA name'],
-
-    '_declare_args': ['symbol', '_declare_args symbol'],
-    'literal': ['REGEXP', 'STRING'],
-}
-
-
-# Value 5 keeps the number of states in the lalr parser somewhat minimal
-# It isn't optimal, but close to it. See PR #949
-SMALL_FACTOR_THRESHOLD = 5
-# The Threshold whether repeat via ~ are split up into different rules
-# 50 is chosen since it keeps the number of states low and therefore lalr analysis time low,
-# while not being to overaggressive and unnecessarily creating rules that might create shift/reduce conflicts.
-# (See PR #949)
-REPEAT_BREAK_THRESHOLD = 50
-
-
-class FindRuleSize(Transformer):
-    def __init__(self, keep_all_tokens: bool):
-        self.keep_all_tokens = keep_all_tokens
-
-    def _will_not_get_removed(self, sym: Symbol) -> bool:
-        if isinstance(sym, NonTerminal):
-            return not sym.name.startswith('_')
-        if isinstance(sym, Terminal):
-            return self.keep_all_tokens or not sym.filter_out
-        if sym is _EMPTY:
-            return False
-        assert False, sym
-
-    def _args_as_int(self, args: List[Union[int, Symbol]]) -> Generator[int, None, None]:
-        for a in args:
-            if isinstance(a, int):
-                yield a
-            elif isinstance(a, Symbol):
-                yield 1 if self._will_not_get_removed(a) else 0
-            else:
-                assert False
-
-    def expansion(self, args) -> int:
-        return sum(self._args_as_int(args))
-
-    def expansions(self, args) -> int:
-        return max(self._args_as_int(args))
-
-
-@inline_args
-class EBNF_to_BNF(Transformer_InPlace):
-    def __init__(self):
-        self.new_rules = []
-        self.rules_cache = {}
-        self.prefix = 'anon'
-        self.i = 0
-        self.rule_options = None
-
-    def _name_rule(self, inner: str):
-        new_name = '__%s_%s_%d' % (self.prefix, inner, self.i)
-        self.i += 1
-        return new_name
-
-    def _add_rule(self, key, name, expansions):
-        t = NonTerminal(name)
-        self.new_rules.append((name, expansions, self.rule_options))
-        self.rules_cache[key] = t
-        return t
-
-    def _add_recurse_rule(self, type_: str, expr: Tree):
-        try:
-            return self.rules_cache[expr]
-        except KeyError:
-            new_name = self._name_rule(type_)
-            t = NonTerminal(new_name)
-            tree = ST('expansions', [
-                ST('expansion', [expr]),
-                ST('expansion', [t, expr])
-            ])
-            return self._add_rule(expr, new_name, tree)
-
-    def _add_repeat_rule(self, a, b, target, atom):
-        """Generate a rule that repeats target ``a`` times, and repeats atom ``b`` times.
-
-        When called recursively (into target), it repeats atom for x(n) times, where:
-            x(0) = 1
-            x(n) = a(n) * x(n-1) + b
-
-        Example rule when a=3, b=4:
-
-            new_rule: target target target atom atom atom atom
-
-        """
-        key = (a, b, target, atom)
-        try:
-            return self.rules_cache[key]
-        except KeyError:
-            new_name = self._name_rule('repeat_a%d_b%d' % (a, b))
-            tree = ST('expansions', [ST('expansion', [target] * a + [atom] * b)])
-            return self._add_rule(key, new_name, tree)
-
-    def _add_repeat_opt_rule(self, a, b, target, target_opt, atom):
-        """Creates a rule that matches atom 0 to (a*n+b)-1 times.
-
-        When target matches n times atom, and target_opt 0 to n-1 times target_opt,
-
-        First we generate target * i followed by target_opt, for i from 0 to a-1
-        These match 0 to n*a - 1 times atom
-
-        Then we generate target * a followed by atom * i, for i from 0 to b-1
-        These match n*a to n*a + b-1 times atom
-
-        The created rule will not have any shift/reduce conflicts so that it can be used with lalr
-
-        Example rule when a=3, b=4:
-
-            new_rule: target_opt
-                    | target target_opt
-                    | target target target_opt
-
-                    | target target target
-                    | target target target atom
-                    | target target target atom atom
-                    | target target target atom atom atom
-
-        """
-        key = (a, b, target, atom, "opt")
-        try:
-            return self.rules_cache[key]
-        except KeyError:
-            new_name = self._name_rule('repeat_a%d_b%d_opt' % (a, b))
-            tree = ST('expansions', [
-                ST('expansion', [target]*i + [target_opt]) for i in range(a)
-            ] + [
-                ST('expansion', [target]*a + [atom]*i) for i in range(b)
-            ])
-            return self._add_rule(key, new_name, tree)
-
-    def _generate_repeats(self, rule: Tree, mn: int, mx: int):
-        """Generates a rule tree that repeats ``rule`` exactly between ``mn`` to ``mx`` times.
-        """
-        # For a small number of repeats, we can take the naive approach
-        if mx < REPEAT_BREAK_THRESHOLD:
-            return ST('expansions', [ST('expansion', [rule] * n) for n in range(mn, mx + 1)])
-
-        # For large repeat values, we break the repetition into sub-rules.
-        # We treat ``rule~mn..mx`` as ``rule~mn rule~0..(diff=mx-mn)``.
-        # We then use small_factors to split up mn and diff up into values [(a, b), ...]
-        # This values are used with the help of _add_repeat_rule and _add_repeat_rule_opt
-        # to generate a complete rule/expression that matches the corresponding number of repeats
-        mn_target = rule
-        for a, b in small_factors(mn, SMALL_FACTOR_THRESHOLD):
-            mn_target = self._add_repeat_rule(a, b, mn_target, rule)
-        if mx == mn:
-            return mn_target
-
-        diff = mx - mn + 1  # We add one because _add_repeat_opt_rule generates rules that match one less
-        diff_factors = small_factors(diff, SMALL_FACTOR_THRESHOLD)
-        diff_target = rule  # Match rule 1 times
-        diff_opt_target = ST('expansion', [])  # match rule 0 times (e.g. up to 1 -1 times)
-        for a, b in diff_factors[:-1]:
-            diff_opt_target = self._add_repeat_opt_rule(a, b, diff_target, diff_opt_target, rule)
-            diff_target = self._add_repeat_rule(a, b, diff_target, rule)
-
-        a, b = diff_factors[-1]
-        diff_opt_target = self._add_repeat_opt_rule(a, b, diff_target, diff_opt_target, rule)
-
-        return ST('expansions', [ST('expansion', [mn_target] + [diff_opt_target])])
-
-    def expr(self, rule: Tree, op: Token, *args):
-        if op.value == '?':
-            empty = ST('expansion', [])
-            return ST('expansions', [rule, empty])
-        elif op.value == '+':
-            # a : b c+ d
-            #   -->
-            # a : b _c d
-            # _c : _c c | c;
-            return self._add_recurse_rule('plus', rule)
-        elif op.value == '*':
-            # a : b c* d
-            #   -->
-            # a : b _c? d
-            # _c : _c c | c;
-            new_name = self._add_recurse_rule('star', rule)
-            return ST('expansions', [new_name, ST('expansion', [])])
-        elif op.value == '~':
-            if len(args) == 1:
-                mn = mx = int(args[0])
-            else:
-                mn, mx = map(int, args)
-                if mx < mn or mn < 0:
-                    raise GrammarError("Bad Range for %s (%d..%d isn't allowed)" % (rule, mn, mx))
-
-            return self._generate_repeats(rule, mn, mx)
-
-        assert False, op
-
-    def maybe(self, rule: Tree):
-        keep_all_tokens = self.rule_options and self.rule_options.keep_all_tokens
-        rule_size = FindRuleSize(keep_all_tokens).transform(rule)
-        empty = ST('expansion', [_EMPTY] * rule_size)
-        return ST('expansions', [rule, empty])
-
-
-class SimplifyRule_Visitor(Visitor):
-
-    @staticmethod
-    def _flatten(tree: Tree):
-        while tree.expand_kids_by_data(tree.data):
-            pass
-
-    def expansion(self, tree: Tree):
-        # rules_list unpacking
-        # a : b (c|d) e
-        #  -->
-        # a : b c e | b d e
-        #
-        # In AST terms:
-        # expansion(b, expansions(c, d), e)
-        #   -->
-        # expansions( expansion(b, c, e), expansion(b, d, e) )
-
-        self._flatten(tree)
-
-        for i, child in enumerate(tree.children):
-            if isinstance(child, Tree) and child.data == 'expansions':
-                tree.data = 'expansions'
-                tree.children = [self.visit(ST('expansion', [option if i == j else other
-                                                             for j, other in enumerate(tree.children)]))
-                                 for option in dedup_list(child.children)]
-                self._flatten(tree)
-                break
-
-    def alias(self, tree):
-        rule, alias_name = tree.children
-        if rule.data == 'expansions':
-            aliases = []
-            for child in tree.children[0].children:
-                aliases.append(ST('alias', [child, alias_name]))
-            tree.data = 'expansions'
-            tree.children = aliases
-
-    def expansions(self, tree: Tree):
-        self._flatten(tree)
-        # Ensure all children are unique
-        if len(set(tree.children)) != len(tree.children):
-            tree.children = dedup_list(tree.children)   # dedup is expensive, so try to minimize its use
-
-
-class RuleTreeToText(Transformer):
-    def expansions(self, x):
-        return x
-
-    def expansion(self, symbols):
-        return symbols, None
-
-    def alias(self, x):
-        (expansion, _alias), alias = x
-        assert _alias is None, (alias, expansion, '-', _alias)  # Double alias not allowed
-        return expansion, alias.name
-
-
-class PrepareAnonTerminals(Transformer_InPlace):
-    """Create a unique list of anonymous terminals. Attempt to give meaningful names to them when we add them"""
-
-    def __init__(self, terminals):
-        self.terminals = terminals
-        self.term_set = {td.name for td in self.terminals}
-        self.term_reverse = {td.pattern: td for td in terminals}
-        self.i = 0
-        self.rule_options = None
-
-    @inline_args
-    def pattern(self, p):
-        value = p.value
-        if p in self.term_reverse and p.flags != self.term_reverse[p].pattern.flags:
-            raise GrammarError(u'Conflicting flags for the same terminal: %s' % p)
-
-        term_name = None
-
-        if isinstance(p, PatternStr):
-            try:
-                # If already defined, use the user-defined terminal name
-                term_name = self.term_reverse[p].name
-            except KeyError:
-                # Try to assign an indicative anon-terminal name
-                try:
-                    term_name = _TERMINAL_NAMES[value]
-                except KeyError:
-                    if value and is_id_continue(value) and is_id_start(value[0]) and value.upper() not in self.term_set:
-                        term_name = value.upper()
-
-                if term_name in self.term_set:
-                    term_name = None
-
-        elif isinstance(p, PatternRE):
-            if p in self.term_reverse:  # Kind of a weird placement.name
-                term_name = self.term_reverse[p].name
-        else:
-            assert False, p
-
-        if term_name is None:
-            term_name = '__ANON_%d' % self.i
-            self.i += 1
-
-        if term_name not in self.term_set:
-            assert p not in self.term_reverse
-            self.term_set.add(term_name)
-            termdef = TerminalDef(term_name, p)
-            self.term_reverse[p] = termdef
-            self.terminals.append(termdef)
-
-        filter_out = False if self.rule_options and self.rule_options.keep_all_tokens else isinstance(p, PatternStr)
-
-        return Terminal(term_name, filter_out=filter_out)
-
-
-class _ReplaceSymbols(Transformer_InPlace):
-    """Helper for ApplyTemplates"""
-
-    def __init__(self):
-        self.names = {}
-
-    def value(self, c):
-        if len(c) == 1 and isinstance(c[0], Symbol) and c[0].name in self.names:
-            return self.names[c[0].name]
-        return self.__default__('value', c, None)
-
-    def template_usage(self, c):
-        name = c[0].name
-        if name in self.names:
-            return self.__default__('template_usage', [self.names[name]] + c[1:], None)
-        return self.__default__('template_usage', c, None)
-
-
-class ApplyTemplates(Transformer_InPlace):
-    """Apply the templates, creating new rules that represent the used templates"""
-
-    def __init__(self, rule_defs):
-        self.rule_defs = rule_defs
-        self.replacer = _ReplaceSymbols()
-        self.created_templates = set()
-
-    def template_usage(self, c):
-        name = c[0].name
-        args = c[1:]
-        result_name = "%s{%s}" % (name, ",".join(a.name for a in args))
-        if result_name not in self.created_templates:
-            self.created_templates.add(result_name)
-            (_n, params, tree, options) ,= (t for t in self.rule_defs if t[0] == name)
-            assert len(params) == len(args), args
-            result_tree = deepcopy(tree)
-            self.replacer.names = dict(zip(params, args))
-            self.replacer.transform(result_tree)
-            self.rule_defs.append((result_name, [], result_tree, deepcopy(options)))
-        return NonTerminal(result_name)
-
-
-def _rfind(s, choices):
-    return max(s.rfind(c) for c in choices)
-
-
-def eval_escaping(s):
-    w = ''
-    i = iter(s)
-    for n in i:
-        w += n
-        if n == '\\':
-            try:
-                n2 = next(i)
-            except StopIteration:
-                raise GrammarError("Literal ended unexpectedly (bad escaping): `%r`" % s)
-            if n2 == '\\':
-                w += '\\\\'
-            elif n2 not in 'Uuxnftr':
-                w += '\\'
-            w += n2
-    w = w.replace('\\"', '"').replace("'", "\\'")
-
-    to_eval = "u'''%s'''" % w
-    try:
-        s = literal_eval(to_eval)
-    except SyntaxError as e:
-        raise GrammarError(s, e)
-
-    return s
-
-
-def _literal_to_pattern(literal):
-    assert isinstance(literal, Token)
-    v = literal.value
-    flag_start = _rfind(v, '/"')+1
-    assert flag_start > 0
-    flags = v[flag_start:]
-    assert all(f in _RE_FLAGS for f in flags), flags
-
-    if literal.type == 'STRING' and '\n' in v:
-        raise GrammarError('You cannot put newlines in string literals')
-
-    if literal.type == 'REGEXP' and '\n' in v and 'x' not in flags:
-        raise GrammarError('You can only use newlines in regular expressions '
-                           'with the `x` (verbose) flag')
-
-    v = v[:flag_start]
-    assert v[0] == v[-1] and v[0] in '"/'
-    x = v[1:-1]
-
-    s = eval_escaping(x)
-
-    if s == "":
-        raise GrammarError("Empty terminals are not allowed (%s)" % literal)
-
-    if literal.type == 'STRING':
-        s = s.replace('\\\\', '\\')
-        return PatternStr(s, flags, raw=literal.value)
-    elif literal.type == 'REGEXP':
-        return PatternRE(s, flags, raw=literal.value)
-    else:
-        assert False, 'Invariant failed: literal.type not in ["STRING", "REGEXP"]'
-
-
-@inline_args
-class PrepareLiterals(Transformer_InPlace):
-    def literal(self, literal):
-        return ST('pattern', [_literal_to_pattern(literal)])
-
-    def range(self, start, end):
-        assert start.type == end.type == 'STRING'
-        start = start.value[1:-1]
-        end = end.value[1:-1]
-        assert len(eval_escaping(start)) == len(eval_escaping(end)) == 1
-        regexp = '[%s-%s]' % (start, end)
-        return ST('pattern', [PatternRE(regexp)])
-
-
-def _make_joined_pattern(regexp, flags_set) -> PatternRE:
-    return PatternRE(regexp, ())
-
-class TerminalTreeToPattern(Transformer_NonRecursive):
-    def pattern(self, ps):
-        p ,= ps
-        return p
-
-    def expansion(self, items: List[Pattern]) -> Pattern:
-        if not items:
-            return PatternStr('')
-
-        if len(items) == 1:
-            return items[0]
-
-        pattern = ''.join(i.to_regexp() for i in items)
-        return _make_joined_pattern(pattern, {i.flags for i in items})
-
-    def expansions(self, exps: List[Pattern]) -> Pattern:
-        if len(exps) == 1:
-            return exps[0]
-
-        # Do a bit of sorting to make sure that the longest option is returned
-        # (Python's re module otherwise prefers just 'l' when given (l|ll) and both could match)
-        exps.sort(key=lambda x: (-x.max_width, -x.min_width, -len(x.value)))
-
-        pattern = '(?:%s)' % ('|'.join(i.to_regexp() for i in exps))
-        return _make_joined_pattern(pattern, {i.flags for i in exps})
-
-    def expr(self, args) -> Pattern:
-        inner: Pattern
-        inner, op = args[:2]
-        if op == '~':
-            if len(args) == 3:
-                op = "{%d}" % int(args[2])
-            else:
-                mn, mx = map(int, args[2:])
-                if mx < mn:
-                    raise GrammarError("Bad Range for %s (%d..%d isn't allowed)" % (inner, mn, mx))
-                op = "{%d,%d}" % (mn, mx)
-        else:
-            assert len(args) == 2
-        return PatternRE('(?:%s)%s' % (inner.to_regexp(), op), inner.flags)
-
-    def maybe(self, expr):
-        return self.expr(expr + ['?'])
-
-    def alias(self, t):
-        raise GrammarError("Aliasing not allowed in terminals (You used -> in the wrong place)")
-
-    def value(self, v):
-        return v[0]
-
-
-class ValidateSymbols(Transformer_InPlace):
-    def value(self, v):
-        v ,= v
-        assert isinstance(v, (Tree, Symbol))
-        return v
-
-
-def nr_deepcopy_tree(t):
-    """Deepcopy tree `t` without recursion"""
-    return Transformer_NonRecursive(False).transform(t)
-
-
-class Grammar:
-
-    term_defs: List[Tuple[str, Tuple[Tree, int]]]
-    rule_defs: List[Tuple[str, Tuple[str, ...], Tree, RuleOptions]]
-    ignore: List[str]
-
-    def __init__(self, rule_defs: List[Tuple[str, Tuple[str, ...], Tree, RuleOptions]], term_defs: List[Tuple[str, Tuple[Tree, int]]], ignore: List[str]) -> None:
-        self.term_defs = term_defs
-        self.rule_defs = rule_defs
-        self.ignore = ignore
-
-    def compile(self, start, terminals_to_keep) -> Tuple[List[TerminalDef], List[Rule], List[str]]:
-        # We change the trees in-place (to support huge grammars)
-        # So deepcopy allows calling compile more than once.
-        term_defs = [(n, (nr_deepcopy_tree(t), p)) for n, (t, p) in self.term_defs]
-        rule_defs = [(n, p, nr_deepcopy_tree(t), o) for n, p, t, o in self.rule_defs]
-
-        # ===================
-        #  Compile Terminals
-        # ===================
-
-        # Convert terminal-trees to strings/regexps
-
-        for name, (term_tree, priority) in term_defs:
-            if term_tree is None:  # Terminal added through %declare
-                continue
-            expansions = list(term_tree.find_data('expansion'))
-            if len(expansions) == 1 and not expansions[0].children:
-                raise GrammarError("Terminals cannot be empty (%s)" % name)
-
-        transformer = PrepareLiterals() * TerminalTreeToPattern()
-        terminals = [TerminalDef(name, transformer.transform(term_tree), priority)
-                     for name, (term_tree, priority) in term_defs if term_tree]
-
-        # =================
-        #  Compile Rules
-        # =================
-
-        # 1. Pre-process terminals
-        anon_tokens_transf = PrepareAnonTerminals(terminals)
-        transformer = PrepareLiterals() * ValidateSymbols() * anon_tokens_transf  # Adds to terminals
-
-        # 2. Inline Templates
-
-        transformer *= ApplyTemplates(rule_defs)
-
-        # 3. Convert EBNF to BNF (and apply step 1 & 2)
-        ebnf_to_bnf = EBNF_to_BNF()
-        rules = []
-        i = 0
-        while i < len(rule_defs):  # We have to do it like this because rule_defs might grow due to templates
-            name, params, rule_tree, options = rule_defs[i]
-            i += 1
-            if len(params) != 0:  # Dont transform templates
-                continue
-            rule_options = RuleOptions(keep_all_tokens=True) if options and options.keep_all_tokens else None
-            ebnf_to_bnf.rule_options = rule_options
-            ebnf_to_bnf.prefix = name
-            anon_tokens_transf.rule_options = rule_options
-            tree = transformer.transform(rule_tree)
-            res: Tree = ebnf_to_bnf.transform(tree)
-            rules.append((name, res, options))
-        rules += ebnf_to_bnf.new_rules
-
-        assert len(rules) == len({name for name, _t, _o in rules}), "Whoops, name collision"
-
-        # 4. Compile tree to Rule objects
-        rule_tree_to_text = RuleTreeToText()
-
-        simplify_rule = SimplifyRule_Visitor()
-        compiled_rules: List[Rule] = []
-        for rule_content in rules:
-            name, tree, options = rule_content
-            simplify_rule.visit(tree)
-            expansions = rule_tree_to_text.transform(tree)
-
-            for i, (expansion, alias) in enumerate(expansions):
-                if alias and name.startswith('_'):
-                    raise GrammarError("Rule %s is marked for expansion (it starts with an underscore) and isn't allowed to have aliases (alias=%s)"% (name, alias))
-
-                empty_indices = tuple(x==_EMPTY for x in expansion)
-                if any(empty_indices):
-                    exp_options = copy(options) or RuleOptions()
-                    exp_options.empty_indices = empty_indices
-                    expansion = [x for x in expansion if x!=_EMPTY]
-                else:
-                    exp_options = options
-
-                for sym in expansion:
-                    assert isinstance(sym, Symbol)
-                    if sym.is_term and exp_options and exp_options.keep_all_tokens:
-                        assert isinstance(sym, Terminal)
-                        sym.filter_out = False
-                rule = Rule(NonTerminal(name), expansion, i, alias, exp_options)
-                compiled_rules.append(rule)
-
-        # Remove duplicates of empty rules, throw error for non-empty duplicates
-        if len(set(compiled_rules)) != len(compiled_rules):
-            duplicates = classify(compiled_rules, lambda x: x)
-            for dups in duplicates.values():
-                if len(dups) > 1:
-                    if dups[0].expansion:
-                        raise GrammarError("Rules defined twice: %s\n\n(Might happen due to colliding expansion of optionals: [] or ?)"
-                                           % ''.join('\n  * %s' % i for i in dups))
-
-                    # Empty rule; assert all other attributes are equal
-                    assert len({(r.alias, r.order, r.options) for r in dups}) == len(dups)
-
-            # Remove duplicates
-            compiled_rules = list(OrderedSet(compiled_rules))
-
-        # Filter out unused rules
-        while True:
-            c = len(compiled_rules)
-            used_rules = {s for r in compiled_rules
-                            for s in r.expansion
-                            if isinstance(s, NonTerminal)
-                            and s != r.origin}
-            used_rules |= {NonTerminal(s) for s in start}
-            compiled_rules, unused = classify_bool(compiled_rules, lambda r: r.origin in used_rules)
-            for r in unused:
-                logger.debug("Unused rule: %s", r)
-            if len(compiled_rules) == c:
-                break
-
-        # Filter out unused terminals
-        if terminals_to_keep != '*':
-            used_terms = {t.name for r in compiled_rules
-                                 for t in r.expansion
-                                 if isinstance(t, Terminal)}
-            terminals, unused = classify_bool(terminals, lambda t: t.name in used_terms or t.name in self.ignore or t.name in terminals_to_keep)
-            if unused:
-                logger.debug("Unused terminals: %s", [t.name for t in unused])
-
-        return terminals, compiled_rules, self.ignore
-
-
-PackageResource = namedtuple('PackageResource', 'pkg_name path')
-
-
-class FromPackageLoader:
-    """
-    Provides a simple way of creating custom import loaders that load from packages via ``pkgutil.get_data`` instead of using `open`.
-    This allows them to be compatible even from within zip files.
-
-    Relative imports are handled, so you can just freely use them.
-
-    pkg_name: The name of the package. You can probably provide `__name__` most of the time
-    search_paths: All the path that will be search on absolute imports.
-    """
-
-    pkg_name: str
-    search_paths: Sequence[str]
-
-    def __init__(self, pkg_name: str, search_paths: Sequence[str]=("", )) -> None:
-        self.pkg_name = pkg_name
-        self.search_paths = search_paths
-
-    def __repr__(self):
-        return "%s(%r, %r)" % (type(self).__name__, self.pkg_name, self.search_paths)
-
-    def __call__(self, base_path: Union[None, str, PackageResource], grammar_path: str) -> Tuple[PackageResource, str]:
-        if base_path is None:
-            to_try = self.search_paths
-        else:
-            # Check whether or not the importing grammar was loaded by this module.
-            if not isinstance(base_path, PackageResource) or base_path.pkg_name != self.pkg_name:
-                # Technically false, but FileNotFound doesn't exist in python2.7, and this message should never reach the end user anyway
-                raise IOError()
-            to_try = [base_path.path]
-
-        err = None
-        for path in to_try:
-            full_path = os.path.join(path, grammar_path)
-            try:
-                text: Optional[bytes] = pkgutil.get_data(self.pkg_name, full_path)
-            except IOError as e:
-                err = e
-                continue
-            else:
-                return PackageResource(self.pkg_name, full_path), (text.decode() if text else '')
-
-        raise IOError('Cannot find grammar in given paths') from err
-
-
-stdlib_loader = FromPackageLoader('lark', IMPORT_PATHS)
-
-
-
-def resolve_term_references(term_dict):
-    # TODO Solve with transitive closure (maybe)
-
-    while True:
-        changed = False
-        for name, token_tree in term_dict.items():
-            if token_tree is None:  # Terminal added through %declare
-                continue
-            for exp in token_tree.find_data('value'):
-                item ,= exp.children
-                if isinstance(item, NonTerminal):
-                    raise GrammarError("Rules aren't allowed inside terminals (%s in %s)" % (item, name))
-                elif isinstance(item, Terminal):
-                    try:
-                        term_value = term_dict[item.name]
-                    except KeyError:
-                        raise GrammarError("Terminal used but not defined: %s" % item.name)
-                    assert term_value is not None
-                    exp.children[0] = term_value
-                    changed = True
-                else:
-                    assert isinstance(item, Tree)
-        if not changed:
-            break
-
-    for name, term in term_dict.items():
-        if term:    # Not just declared
-            for child in term.children:
-                ids = [id(x) for x in child.iter_subtrees()]
-                if id(term) in ids:
-                    raise GrammarError("Recursion in terminal '%s' (recursion is only allowed in rules, not terminals)" % name)
-
-
-
-def symbol_from_strcase(s):
-    assert isinstance(s, str)
-    return Terminal(s, filter_out=s.startswith('_')) if s.isupper() else NonTerminal(s)
-
-@inline_args
-class PrepareGrammar(Transformer_InPlace):
-    def terminal(self, name):
-        return Terminal(str(name), filter_out=name.startswith('_'))
-
-    def nonterminal(self, name):
-        return NonTerminal(name.value)
-
-
-def _find_used_symbols(tree):
-    assert tree.data == 'expansions'
-    return {t.name for x in tree.find_data('expansion')
-            for t in x.scan_values(lambda t: isinstance(t, Symbol))}
-
-
-def _get_parser():
-    try:
-        return _get_parser.cache
-    except AttributeError:
-        terminals = [TerminalDef(name, PatternRE(value)) for name, value in TERMINALS.items()]
-
-        rules = [(name.lstrip('?'), x, RuleOptions(expand1=name.startswith('?')))
-                for name, x in RULES.items()]
-        rules = [Rule(NonTerminal(r), [symbol_from_strcase(s) for s in x.split()], i, None, o)
-                 for r, xs, o in rules for i, x in enumerate(xs)]
-
-        callback = ParseTreeBuilder(rules, ST).create_callback()
-        import re
-        lexer_conf = LexerConf(terminals, re, ['WS', 'COMMENT', 'BACKSLASH'])
-        parser_conf = ParserConf(rules, callback, ['start'])
-        lexer_conf.lexer_type = 'basic'
-        parser_conf.parser_type = 'lalr'
-        _get_parser.cache = ParsingFrontend(lexer_conf, parser_conf, None)
-        return _get_parser.cache
-
-GRAMMAR_ERRORS = [
-        ('Incorrect type of value', ['a: 1\n']),
-        ('Unclosed parenthesis', ['a: (\n']),
-        ('Unmatched closing parenthesis', ['a: )\n', 'a: [)\n', 'a: (]\n']),
-        ('Expecting rule or terminal definition (missing colon)', ['a\n', 'A\n', 'a->\n', 'A->\n', 'a A\n']),
-        ('Illegal name for rules or terminals', ['Aa:\n']),
-        ('Alias expects lowercase name', ['a: -> "a"\n']),
-        ('Unexpected colon', ['a::\n', 'a: b:\n', 'a: B:\n', 'a: "a":\n']),
-        ('Misplaced operator', ['a: b??', 'a: b(?)', 'a:+\n', 'a:?\n', 'a:*\n', 'a:|*\n']),
-        ('Expecting option ("|") or a new rule or terminal definition', ['a:a\n()\n']),
-        ('Terminal names cannot contain dots', ['A.B\n']),
-        ('Expecting rule or terminal definition', ['"a"\n']),
-        ('%import expects a name', ['%import "a"\n']),
-        ('%ignore expects a value', ['%ignore %import\n']),
-    ]
-
-def _translate_parser_exception(parse, e):
-        error = e.match_examples(parse, GRAMMAR_ERRORS, use_accepts=True)
-        if error:
-            return error
-        elif 'STRING' in e.expected:
-            return "Expecting a value"
-
-def _parse_grammar(text, name, start='start'):
-    try:
-        tree = _get_parser().parse(text + '\n', start)
-    except UnexpectedCharacters as e:
-        context = e.get_context(text)
-        raise GrammarError("Unexpected input at line %d column %d in %s: \n\n%s" %
-                           (e.line, e.column, name, context))
-    except UnexpectedToken as e:
-        context = e.get_context(text)
-        error = _translate_parser_exception(_get_parser().parse, e)
-        if error:
-            raise GrammarError("%s, at line %s column %s\n\n%s" % (error, e.line, e.column, context))
-        raise
-
-    return PrepareGrammar().transform(tree)
-
-
-def _error_repr(error):
-    if isinstance(error, UnexpectedToken):
-        error2 = _translate_parser_exception(_get_parser().parse, error)
-        if error2:
-            return error2
-        expected = ', '.join(error.accepts or error.expected)
-        return "Unexpected token %r. Expected one of: {%s}" % (str(error.token), expected)
-    else:
-        return str(error)
-
-def _search_interactive_parser(interactive_parser, predicate):
-    def expand(node):
-        path, p = node
-        for choice in p.choices():
-            t = Token(choice, '')
-            try:
-                new_p = p.feed_token(t)
-            except ParseError:    # Illegal
-                pass
-            else:
-                yield path + (choice,), new_p
-
-    for path, p in bfs_all_unique([((), interactive_parser)], expand):
-        if predicate(p):
-            return path, p
-
-def find_grammar_errors(text: str, start: str='start') -> List[Tuple[UnexpectedInput, str]]:
-    errors = []
-    def on_error(e):
-        errors.append((e, _error_repr(e)))
-
-        # recover to a new line
-        token_path, _ = _search_interactive_parser(e.interactive_parser.as_immutable(), lambda p: '_NL' in p.choices())
-        for token_type in token_path:
-            e.interactive_parser.feed_token(Token(token_type, ''))
-        e.interactive_parser.feed_token(Token('_NL', '\n'))
-        return True
-
-    _tree = _get_parser().parse(text + '\n', start, on_error=on_error)
-
-    errors_by_line = classify(errors, lambda e: e[0].line)
-    errors = [el[0] for el in errors_by_line.values()]      # already sorted
-
-    for e in errors:
-        e[0].interactive_parser = None
-    return errors
-
-
-def _get_mangle(prefix, aliases, base_mangle=None):
-    def mangle(s):
-        if s in aliases:
-            s = aliases[s]
-        else:
-            if s[0] == '_':
-                s = '_%s__%s' % (prefix, s[1:])
-            else:
-                s = '%s__%s' % (prefix, s)
-        if base_mangle is not None:
-            s = base_mangle(s)
-        return s
-    return mangle
-
-def _mangle_definition_tree(exp, mangle):
-    if mangle is None:
-        return exp
-    exp = deepcopy(exp) # TODO: is this needed?
-    for t in exp.iter_subtrees():
-        for i, c in enumerate(t.children):
-            if isinstance(c, Symbol):
-                t.children[i] = c.renamed(mangle)
-
-    return exp
-
-def _make_rule_tuple(modifiers_tree, name, params, priority_tree, expansions):
-    if modifiers_tree.children:
-        m ,= modifiers_tree.children
-        expand1 = '?' in m
-        if expand1 and name.startswith('_'):
-            raise GrammarError("Inlined rules (_rule) cannot use the ?rule modifier.")
-        keep_all_tokens = '!' in m
-    else:
-        keep_all_tokens = False
-        expand1 = False
-
-    if priority_tree.children:
-        p ,= priority_tree.children
-        priority = int(p)
-    else:
-        priority = None
-
-    if params is not None:
-        params = [t.value for t in params.children]  # For the grammar parser
-
-    return name, params, expansions, RuleOptions(keep_all_tokens, expand1, priority=priority,
-                                                 template_source=(name if params else None))
-
-
-class Definition:
-    def __init__(self, is_term, tree, params=(), options=None):
-        self.is_term = is_term
-        self.tree = tree
-        self.params = tuple(params)
-        self.options = options
-
-class GrammarBuilder:
-
-    global_keep_all_tokens: bool
-    import_paths: List[Union[str, Callable]]
-    used_files: Dict[str, str]
-
-    _definitions: Dict[str, Definition]
-    _ignore_names: List[str]
-
-    def __init__(self, global_keep_all_tokens: bool=False, import_paths: Optional[List[Union[str, Callable]]]=None, used_files: Optional[Dict[str, str]]=None) -> None:
-        self.global_keep_all_tokens = global_keep_all_tokens
-        self.import_paths = import_paths or []
-        self.used_files = used_files or {}
-
-        self._definitions: Dict[str, Definition] = {}
-        self._ignore_names: List[str] = []
-
-    def _grammar_error(self, is_term, msg, *names):
-        args = {}
-        for i, name in enumerate(names, start=1):
-            postfix = '' if i == 1 else str(i)
-            args['name' + postfix] = name
-            args['type' + postfix] = lowercase_type = ("rule", "terminal")[is_term]
-            args['Type' + postfix] = lowercase_type.title()
-        raise GrammarError(msg.format(**args))
-
-    def _check_options(self, is_term, options):
-        if is_term:
-            if options is None:
-                options = 1
-            elif not isinstance(options, int):
-                raise GrammarError("Terminal require a single int as 'options' (e.g. priority), got %s" % (type(options),))
-        else:
-            if options is None:
-                options = RuleOptions()
-            elif not isinstance(options, RuleOptions):
-                raise GrammarError("Rules require a RuleOptions instance as 'options'")
-            if self.global_keep_all_tokens:
-                options.keep_all_tokens = True
-        return options
-
-
-    def _define(self, name, is_term, exp, params=(), options=None, *, override=False):
-        if name in self._definitions:
-            if not override:
-                self._grammar_error(is_term, "{Type} '{name}' defined more than once", name)
-        elif override:
-            self._grammar_error(is_term, "Cannot override a nonexisting {type} {name}", name)
-
-        if name.startswith('__'):
-            self._grammar_error(is_term, 'Names starting with double-underscore are reserved (Error at {name})', name)
-
-        self._definitions[name] = Definition(is_term, exp, params, self._check_options(is_term, options))
-
-    def _extend(self, name, is_term, exp, params=(), options=None):
-        if name not in self._definitions:
-            self._grammar_error(is_term, "Can't extend {type} {name} as it wasn't defined before", name)
-
-        d = self._definitions[name]
-
-        if is_term != d.is_term:
-            self._grammar_error(is_term, "Cannot extend {type} {name} - one is a terminal, while the other is not.", name)
-        if tuple(params) != d.params:
-            self._grammar_error(is_term, "Cannot extend {type} with different parameters: {name}", name)
-
-        if d.tree is None:
-            self._grammar_error(is_term, "Can't extend {type} {name} - it is abstract.", name)
-
-        # TODO: think about what to do with 'options'
-        base = d.tree
-
-        assert isinstance(base, Tree) and base.data == 'expansions'
-        base.children.insert(0, exp)
-
-    def _ignore(self, exp_or_name):
-        if isinstance(exp_or_name, str):
-            self._ignore_names.append(exp_or_name)
-        else:
-            assert isinstance(exp_or_name, Tree)
-            t = exp_or_name
-            if t.data == 'expansions' and len(t.children) == 1:
-                t2 ,= t.children
-                if t2.data=='expansion' and len(t2.children) == 1:
-                    item ,= t2.children
-                    if item.data == 'value':
-                        item ,= item.children
-                        if isinstance(item, Terminal):
-                            # Keep terminal name, no need to create a new definition
-                            self._ignore_names.append(item.name)
-                            return
-
-            name = '__IGNORE_%d'% len(self._ignore_names)
-            self._ignore_names.append(name)
-            self._definitions[name] = Definition(True, t, options=TOKEN_DEFAULT_PRIORITY)
-
-    def _unpack_import(self, stmt, grammar_name):
-        if len(stmt.children) > 1:
-            path_node, arg1 = stmt.children
-        else:
-            path_node, = stmt.children
-            arg1 = None
-
-        if isinstance(arg1, Tree):  # Multi import
-            dotted_path = tuple(path_node.children)
-            names = arg1.children
-            aliases = dict(zip(names, names))  # Can't have aliased multi import, so all aliases will be the same as names
-        else:  # Single import
-            dotted_path = tuple(path_node.children[:-1])
-            if not dotted_path:
-                name ,= path_node.children
-                raise GrammarError("Nothing was imported from grammar `%s`" % name)
-            name = path_node.children[-1]  # Get name from dotted path
-            aliases = {name.value: (arg1 or name).value}  # Aliases if exist
-
-        if path_node.data == 'import_lib':  # Import from library
-            base_path = None
-        else:  # Relative import
-            if grammar_name == '<string>':  # Import relative to script file path if grammar is coded in script
-                try:
-                    base_file = os.path.abspath(sys.modules['__main__'].__file__)
-                except AttributeError:
-                    base_file = None
-            else:
-                base_file = grammar_name  # Import relative to grammar file path if external grammar file
-            if base_file:
-                if isinstance(base_file, PackageResource):
-                    base_path = PackageResource(base_file.pkg_name, os.path.split(base_file.path)[0])
-                else:
-                    base_path = os.path.split(base_file)[0]
-            else:
-                base_path = os.path.abspath(os.path.curdir)
-
-        return dotted_path, base_path, aliases
-
-    def _unpack_definition(self, tree, mangle):
-
-        if tree.data == 'rule':
-            name, params, exp, opts = _make_rule_tuple(*tree.children)
-            is_term = False
-        else:
-            name = tree.children[0].value
-            params = ()     # TODO terminal templates
-            opts = int(tree.children[1]) if len(tree.children) == 3 else TOKEN_DEFAULT_PRIORITY # priority
-            exp = tree.children[-1]
-            is_term = True
-
-        if mangle is not None:
-            params = tuple(mangle(p) for p in params)
-            name = mangle(name)
-
-        exp = _mangle_definition_tree(exp, mangle)
-        return name, is_term, exp, params, opts
-
-
-    def load_grammar(self, grammar_text: str, grammar_name: str="<?>", mangle: Optional[Callable[[str], str]]=None) -> None:
-        tree = _parse_grammar(grammar_text, grammar_name)
-
-        imports: Dict[Tuple[str, ...], Tuple[Optional[str], Dict[str, str]]] = {}
-
-        for stmt in tree.children:
-            if stmt.data == 'import':
-                dotted_path, base_path, aliases = self._unpack_import(stmt, grammar_name)
-                try:
-                    import_base_path, import_aliases = imports[dotted_path]
-                    assert base_path == import_base_path, 'Inconsistent base_path for %s.' % '.'.join(dotted_path)
-                    import_aliases.update(aliases)
-                except KeyError:
-                    imports[dotted_path] = base_path, aliases
-
-        for dotted_path, (base_path, aliases) in imports.items():
-            self.do_import(dotted_path, base_path, aliases, mangle)
-
-        for stmt in tree.children:
-            if stmt.data in ('term', 'rule'):
-                self._define(*self._unpack_definition(stmt, mangle))
-            elif stmt.data == 'override':
-                r ,= stmt.children
-                self._define(*self._unpack_definition(r, mangle), override=True)
-            elif stmt.data == 'extend':
-                r ,= stmt.children
-                self._extend(*self._unpack_definition(r, mangle))
-            elif stmt.data == 'ignore':
-                # if mangle is not None, we shouldn't apply ignore, since we aren't in a toplevel grammar
-                if mangle is None:
-                    self._ignore(*stmt.children)
-            elif stmt.data == 'declare':
-                for symbol in stmt.children:
-                    assert isinstance(symbol, Symbol), symbol
-                    is_term = isinstance(symbol, Terminal)
-                    if mangle is None:
-                        name = symbol.name
-                    else:
-                        name = mangle(symbol.name)
-                    self._define(name, is_term, None)
-            elif stmt.data == 'import':
-                pass
-            else:
-                assert False, stmt
-
-
-        term_defs = { name: d.tree
-            for name, d in self._definitions.items()
-            if d.is_term
-        }
-        resolve_term_references(term_defs)
-
-
-    def _remove_unused(self, used):
-        def rule_dependencies(symbol):
-            try:
-                d = self._definitions[symbol]
-            except KeyError:
-                return []
-            if d.is_term:
-                return []
-            return _find_used_symbols(d.tree) - set(d.params)
-
-        _used = set(bfs(used, rule_dependencies))
-        self._definitions = {k: v for k, v in self._definitions.items() if k in _used}
-
-
-    def do_import(self, dotted_path: Tuple[str, ...], base_path: Optional[str], aliases: Dict[str, str], base_mangle: Optional[Callable[[str], str]]=None) -> None:
-        assert dotted_path
-        mangle = _get_mangle('__'.join(dotted_path), aliases, base_mangle)
-        grammar_path = os.path.join(*dotted_path) + EXT
-        to_try = self.import_paths + ([base_path] if base_path is not None else []) + [stdlib_loader]
-        for source in to_try:
-            try:
-                if callable(source):
-                    joined_path, text = source(base_path, grammar_path)
-                else:
-                    joined_path = os.path.join(source, grammar_path)
-                    with open(joined_path, encoding='utf8') as f:
-                        text = f.read()
-            except IOError:
-                continue
-            else:
-                h = sha256_digest(text)
-                if self.used_files.get(joined_path, h) != h:
-                    raise RuntimeError("Grammar file was changed during importing")
-                self.used_files[joined_path] = h
-
-                gb = GrammarBuilder(self.global_keep_all_tokens, self.import_paths, self.used_files)
-                gb.load_grammar(text, joined_path, mangle)
-                gb._remove_unused(map(mangle, aliases))
-                for name in gb._definitions:
-                    if name in self._definitions:
-                        raise GrammarError("Cannot import '%s' from '%s': Symbol already defined." % (name, grammar_path))
-
-                self._definitions.update(**gb._definitions)
-                break
-        else:
-            # Search failed. Make Python throw a nice error.
-            open(grammar_path, encoding='utf8')
-            assert False, "Couldn't import grammar %s, but a corresponding file was found at a place where lark doesn't search for it" % (dotted_path,)
-
-
-    def validate(self) -> None:
-        for name, d in self._definitions.items():
-            params = d.params
-            exp = d.tree
-
-            for i, p in enumerate(params):
-                if p in self._definitions:
-                    raise GrammarError("Template Parameter conflicts with rule %s (in template %s)" % (p, name))
-                if p in params[:i]:
-                    raise GrammarError("Duplicate Template Parameter %s (in template %s)" % (p, name))
-
-            if exp is None: # Remaining checks don't apply to abstract rules/terminals (created with %declare)
-                continue
-
-            for temp in exp.find_data('template_usage'):
-                sym = temp.children[0].name
-                args = temp.children[1:]
-                if sym not in params:
-                    if sym not in self._definitions:
-                        self._grammar_error(d.is_term, "Template '%s' used but not defined (in {type} {name})" % sym, name)
-                    if len(args) != len(self._definitions[sym].params):
-                        expected, actual = len(self._definitions[sym].params), len(args)
-                        self._grammar_error(d.is_term, "Wrong number of template arguments used for {name} "
-                                            "(expected %s, got %s) (in {type2} {name2})" % (expected, actual), sym, name)
-
-            for sym in _find_used_symbols(exp):
-                if sym not in self._definitions and sym not in params:
-                    self._grammar_error(d.is_term, "{Type} '{name}' used but not defined (in {type2} {name2})", sym, name)
-
-        if not set(self._definitions).issuperset(self._ignore_names):
-            raise GrammarError("Terminals %s were marked to ignore but were not defined!" % (set(self._ignore_names) - set(self._definitions)))
-
-    def build(self) -> Grammar:
-        self.validate()
-        rule_defs = []
-        term_defs = []
-        for name, d in self._definitions.items():
-            (params, exp, options) = d.params, d.tree, d.options
-            if d.is_term:
-                assert len(params) == 0
-                term_defs.append((name, (exp, options)))
-            else:
-                rule_defs.append((name, params, exp, options))
-        # resolve_term_references(term_defs)
-        return Grammar(rule_defs, term_defs, self._ignore_names)
-
-
-def verify_used_files(file_hashes):
-    for path, old in file_hashes.items():
-        text = None
-        if isinstance(path, str) and os.path.exists(path):
-            with open(path, encoding='utf8') as f:
-                text = f.read()
-        elif isinstance(path, PackageResource):
-            with suppress(IOError):
-                text = pkgutil.get_data(*path).decode('utf-8')
-        if text is None: # We don't know how to load the path. ignore it.
-            continue
-
-        current = sha256_digest(text)
-        if old != current:
-            logger.info("File %r changed, rebuilding Parser" % path)
-            return False
-    return True
-
-def list_grammar_imports(grammar, import_paths=[]):
-    "Returns a list of paths to the lark grammars imported by the given grammar (recursively)"
-    builder = GrammarBuilder(False, import_paths)
-    builder.load_grammar(grammar, '<string>')
-    return list(builder.used_files.keys())
-
-def load_grammar(grammar, source, import_paths, global_keep_all_tokens):
-    builder = GrammarBuilder(global_keep_all_tokens, import_paths)
-    builder.load_grammar(grammar, source)
-    return builder.build(), builder.used_files
-
-
-def sha256_digest(s: str) -> str:
-    """Get the sha256 digest of a string
-
-    Supports the `usedforsecurity` argument for Python 3.9+ to allow running on
-    a FIPS-enabled system.
-    """
-    if sys.version_info >= (3, 9):
-        return hashlib.sha256(s.encode('utf8'), usedforsecurity=False).hexdigest()
-    else:
-        return hashlib.sha256(s.encode('utf8')).hexdigest()
diff --git a/src/poetry/core/_vendor/lark/parse_tree_builder.py b/src/poetry/core/_vendor/lark/parse_tree_builder.py
deleted file mode 100644
index e3a4171..0000000
--- a/src/poetry/core/_vendor/lark/parse_tree_builder.py
+++ /dev/null
@@ -1,391 +0,0 @@
-"""Provides functions for the automatic building and shaping of the parse-tree."""
-
-from typing import List
-
-from .exceptions import GrammarError, ConfigurationError
-from .lexer import Token
-from .tree import Tree
-from .visitors import Transformer_InPlace
-from .visitors import _vargs_meta, _vargs_meta_inline
-
-###{standalone
-from functools import partial, wraps
-from itertools import product
-
-
-class ExpandSingleChild:
-    def __init__(self, node_builder):
-        self.node_builder = node_builder
-
-    def __call__(self, children):
-        if len(children) == 1:
-            return children[0]
-        else:
-            return self.node_builder(children)
-
-
-
-class PropagatePositions:
-    def __init__(self, node_builder, node_filter=None):
-        self.node_builder = node_builder
-        self.node_filter = node_filter
-
-    def __call__(self, children):
-        res = self.node_builder(children)
-
-        if isinstance(res, Tree):
-            # Calculate positions while the tree is streaming, according to the rule:
-            # - nodes start at the start of their first child's container,
-            #   and end at the end of their last child's container.
-            # Containers are nodes that take up space in text, but have been inlined in the tree.
-
-            res_meta = res.meta
-
-            first_meta = self._pp_get_meta(children)
-            if first_meta is not None:
-                if not hasattr(res_meta, 'line'):
-                    # meta was already set, probably because the rule has been inlined (e.g. `?rule`)
-                    res_meta.line = getattr(first_meta, 'container_line', first_meta.line)
-                    res_meta.column = getattr(first_meta, 'container_column', first_meta.column)
-                    res_meta.start_pos = getattr(first_meta, 'container_start_pos', first_meta.start_pos)
-                    res_meta.empty = False
-
-                res_meta.container_line = getattr(first_meta, 'container_line', first_meta.line)
-                res_meta.container_column = getattr(first_meta, 'container_column', first_meta.column)
-                res_meta.container_start_pos = getattr(first_meta, 'container_start_pos', first_meta.start_pos)
-
-            last_meta = self._pp_get_meta(reversed(children))
-            if last_meta is not None:
-                if not hasattr(res_meta, 'end_line'):
-                    res_meta.end_line = getattr(last_meta, 'container_end_line', last_meta.end_line)
-                    res_meta.end_column = getattr(last_meta, 'container_end_column', last_meta.end_column)
-                    res_meta.end_pos = getattr(last_meta, 'container_end_pos', last_meta.end_pos)
-                    res_meta.empty = False
-
-                res_meta.container_end_line = getattr(last_meta, 'container_end_line', last_meta.end_line)
-                res_meta.container_end_column = getattr(last_meta, 'container_end_column', last_meta.end_column)
-                res_meta.container_end_pos = getattr(last_meta, 'container_end_pos', last_meta.end_pos)
-
-        return res
-
-    def _pp_get_meta(self, children):
-        for c in children:
-            if self.node_filter is not None and not self.node_filter(c):
-                continue
-            if isinstance(c, Tree):
-                if not c.meta.empty:
-                    return c.meta
-            elif isinstance(c, Token):
-                return c
-            elif hasattr(c, '__lark_meta__'):
-                return c.__lark_meta__()
-
-def make_propagate_positions(option):
-    if callable(option):
-        return partial(PropagatePositions, node_filter=option)
-    elif option is True:
-        return PropagatePositions
-    elif option is False:
-        return None
-
-    raise ConfigurationError('Invalid option for propagate_positions: %r' % option)
-
-
-class ChildFilter:
-    def __init__(self, to_include, append_none, node_builder):
-        self.node_builder = node_builder
-        self.to_include = to_include
-        self.append_none = append_none
-
-    def __call__(self, children):
-        filtered = []
-
-        for i, to_expand, add_none in self.to_include:
-            if add_none:
-                filtered += [None] * add_none
-            if to_expand:
-                filtered += children[i].children
-            else:
-                filtered.append(children[i])
-
-        if self.append_none:
-            filtered += [None] * self.append_none
-
-        return self.node_builder(filtered)
-
-
-class ChildFilterLALR(ChildFilter):
-    """Optimized childfilter for LALR (assumes no duplication in parse tree, so it's safe to change it)"""
-
-    def __call__(self, children):
-        filtered = []
-        for i, to_expand, add_none in self.to_include:
-            if add_none:
-                filtered += [None] * add_none
-            if to_expand:
-                if filtered:
-                    filtered += children[i].children
-                else:   # Optimize for left-recursion
-                    filtered = children[i].children
-            else:
-                filtered.append(children[i])
-
-        if self.append_none:
-            filtered += [None] * self.append_none
-
-        return self.node_builder(filtered)
-
-
-class ChildFilterLALR_NoPlaceholders(ChildFilter):
-    "Optimized childfilter for LALR (assumes no duplication in parse tree, so it's safe to change it)"
-    def __init__(self, to_include, node_builder):
-        self.node_builder = node_builder
-        self.to_include = to_include
-
-    def __call__(self, children):
-        filtered = []
-        for i, to_expand in self.to_include:
-            if to_expand:
-                if filtered:
-                    filtered += children[i].children
-                else:   # Optimize for left-recursion
-                    filtered = children[i].children
-            else:
-                filtered.append(children[i])
-        return self.node_builder(filtered)
-
-
-def _should_expand(sym):
-    return not sym.is_term and sym.name.startswith('_')
-
-
-def maybe_create_child_filter(expansion, keep_all_tokens, ambiguous, _empty_indices: List[bool]):
-    # Prepare empty_indices as: How many Nones to insert at each index?
-    if _empty_indices:
-        assert _empty_indices.count(False) == len(expansion)
-        s = ''.join(str(int(b)) for b in _empty_indices)
-        empty_indices = [len(ones) for ones in s.split('0')]
-        assert len(empty_indices) == len(expansion)+1, (empty_indices, len(expansion))
-    else:
-        empty_indices = [0] * (len(expansion)+1)
-
-    to_include = []
-    nones_to_add = 0
-    for i, sym in enumerate(expansion):
-        nones_to_add += empty_indices[i]
-        if keep_all_tokens or not (sym.is_term and sym.filter_out):
-            to_include.append((i, _should_expand(sym), nones_to_add))
-            nones_to_add = 0
-
-    nones_to_add += empty_indices[len(expansion)]
-
-    if _empty_indices or len(to_include) < len(expansion) or any(to_expand for i, to_expand,_ in to_include):
-        if _empty_indices or ambiguous:
-            return partial(ChildFilter if ambiguous else ChildFilterLALR, to_include, nones_to_add)
-        else:
-            # LALR without placeholders
-            return partial(ChildFilterLALR_NoPlaceholders, [(i, x) for i,x,_ in to_include])
-
-
-class AmbiguousExpander:
-    """Deal with the case where we're expanding children ('_rule') into a parent but the children
-       are ambiguous. i.e. (parent->_ambig->_expand_this_rule). In this case, make the parent itself
-       ambiguous with as many copies as there are ambiguous children, and then copy the ambiguous children
-       into the right parents in the right places, essentially shifting the ambiguity up the tree."""
-    def __init__(self, to_expand, tree_class, node_builder):
-        self.node_builder = node_builder
-        self.tree_class = tree_class
-        self.to_expand = to_expand
-
-    def __call__(self, children):
-        def _is_ambig_tree(t):
-            return hasattr(t, 'data') and t.data == '_ambig'
-
-        # -- When we're repeatedly expanding ambiguities we can end up with nested ambiguities.
-        #    All children of an _ambig node should be a derivation of that ambig node, hence
-        #    it is safe to assume that if we see an _ambig node nested within an ambig node
-        #    it is safe to simply expand it into the parent _ambig node as an alternative derivation.
-        ambiguous = []
-        for i, child in enumerate(children):
-            if _is_ambig_tree(child):
-                if i in self.to_expand:
-                    ambiguous.append(i)
-
-                child.expand_kids_by_data('_ambig')
-
-        if not ambiguous:
-            return self.node_builder(children)
-
-        expand = [child.children if i in ambiguous else (child,) for i, child in enumerate(children)]
-        return self.tree_class('_ambig', [self.node_builder(list(f)) for f in product(*expand)])
-
-
-def maybe_create_ambiguous_expander(tree_class, expansion, keep_all_tokens):
-    to_expand = [i for i, sym in enumerate(expansion)
-                 if keep_all_tokens or ((not (sym.is_term and sym.filter_out)) and _should_expand(sym))]
-    if to_expand:
-        return partial(AmbiguousExpander, to_expand, tree_class)
-
-
-class AmbiguousIntermediateExpander:
-    """
-    Propagate ambiguous intermediate nodes and their derivations up to the
-    current rule.
-
-    In general, converts
-
-    rule
-      _iambig
-        _inter
-          someChildren1
-          ...
-        _inter
-          someChildren2
-          ...
-      someChildren3
-      ...
-
-    to
-
-    _ambig
-      rule
-        someChildren1
-        ...
-        someChildren3
-        ...
-      rule
-        someChildren2
-        ...
-        someChildren3
-        ...
-      rule
-        childrenFromNestedIambigs
-        ...
-        someChildren3
-        ...
-      ...
-
-    propagating up any nested '_iambig' nodes along the way.
-    """
-
-    def __init__(self, tree_class, node_builder):
-        self.node_builder = node_builder
-        self.tree_class = tree_class
-
-    def __call__(self, children):
-        def _is_iambig_tree(child):
-            return hasattr(child, 'data') and child.data == '_iambig'
-
-        def _collapse_iambig(children):
-            """
-            Recursively flatten the derivations of the parent of an '_iambig'
-            node. Returns a list of '_inter' nodes guaranteed not
-            to contain any nested '_iambig' nodes, or None if children does
-            not contain an '_iambig' node.
-            """
-
-            # Due to the structure of the SPPF,
-            # an '_iambig' node can only appear as the first child
-            if children and _is_iambig_tree(children[0]):
-                iambig_node = children[0]
-                result = []
-                for grandchild in iambig_node.children:
-                    collapsed = _collapse_iambig(grandchild.children)
-                    if collapsed:
-                        for child in collapsed:
-                            child.children += children[1:]
-                        result += collapsed
-                    else:
-                        new_tree = self.tree_class('_inter', grandchild.children + children[1:])
-                        result.append(new_tree)
-                return result
-
-        collapsed = _collapse_iambig(children)
-        if collapsed:
-            processed_nodes = [self.node_builder(c.children) for c in collapsed]
-            return self.tree_class('_ambig', processed_nodes)
-
-        return self.node_builder(children)
-
-
-
-def inplace_transformer(func):
-    @wraps(func)
-    def f(children):
-        # function name in a Transformer is a rule name.
-        tree = Tree(func.__name__, children)
-        return func(tree)
-    return f
-
-
-def apply_visit_wrapper(func, name, wrapper):
-    if wrapper is _vargs_meta or wrapper is _vargs_meta_inline:
-        raise NotImplementedError("Meta args not supported for internal transformer")
-
-    @wraps(func)
-    def f(children):
-        return wrapper(func, name, children, None)
-    return f
-
-
-class ParseTreeBuilder:
-    def __init__(self, rules, tree_class, propagate_positions=False, ambiguous=False, maybe_placeholders=False):
-        self.tree_class = tree_class
-        self.propagate_positions = propagate_positions
-        self.ambiguous = ambiguous
-        self.maybe_placeholders = maybe_placeholders
-
-        self.rule_builders = list(self._init_builders(rules))
-
-    def _init_builders(self, rules):
-        propagate_positions = make_propagate_positions(self.propagate_positions)
-
-        for rule in rules:
-            options = rule.options
-            keep_all_tokens = options.keep_all_tokens
-            expand_single_child = options.expand1
-
-            wrapper_chain = list(filter(None, [
-                (expand_single_child and not rule.alias) and ExpandSingleChild,
-                maybe_create_child_filter(rule.expansion, keep_all_tokens, self.ambiguous, options.empty_indices if self.maybe_placeholders else None),
-                propagate_positions,
-                self.ambiguous and maybe_create_ambiguous_expander(self.tree_class, rule.expansion, keep_all_tokens),
-                self.ambiguous and partial(AmbiguousIntermediateExpander, self.tree_class)
-            ]))
-
-            yield rule, wrapper_chain
-
-    def create_callback(self, transformer=None):
-        callbacks = {}
-
-        default_handler = getattr(transformer, '__default__', None)
-        if default_handler:
-            def default_callback(data, children):
-                return default_handler(data, children, None)
-        else:
-            default_callback = self.tree_class
-
-        for rule, wrapper_chain in self.rule_builders:
-
-            user_callback_name = rule.alias or rule.options.template_source or rule.origin.name
-            try:
-                f = getattr(transformer, user_callback_name)
-                wrapper = getattr(f, 'visit_wrapper', None)
-                if wrapper is not None:
-                    f = apply_visit_wrapper(f, user_callback_name, wrapper)
-                elif isinstance(transformer, Transformer_InPlace):
-                    f = inplace_transformer(f)
-            except AttributeError:
-                f = partial(default_callback, user_callback_name)
-
-            for w in wrapper_chain:
-                f = w(f)
-
-            if rule in callbacks:
-                raise GrammarError("Rule '%s' already exists" % (rule,))
-
-            callbacks[rule] = f
-
-        return callbacks
-
-###}
diff --git a/src/poetry/core/_vendor/lark/parser_frontends.py b/src/poetry/core/_vendor/lark/parser_frontends.py
deleted file mode 100644
index 186058a..0000000
--- a/src/poetry/core/_vendor/lark/parser_frontends.py
+++ /dev/null
@@ -1,257 +0,0 @@
-from typing import Any, Callable, Dict, Optional, Collection, Union, TYPE_CHECKING
-
-from .exceptions import ConfigurationError, GrammarError, assert_config
-from .utils import get_regexp_width, Serialize
-from .lexer import LexerThread, BasicLexer, ContextualLexer, Lexer
-from .parsers import earley, xearley, cyk
-from .parsers.lalr_parser import LALR_Parser
-from .tree import Tree
-from .common import LexerConf, ParserConf, _ParserArgType, _LexerArgType
-
-if TYPE_CHECKING:
-    from .parsers.lalr_analysis import ParseTableBase
-
-
-###{standalone
-
-def _wrap_lexer(lexer_class):
-    future_interface = getattr(lexer_class, '__future_interface__', False)
-    if future_interface:
-        return lexer_class
-    else:
-        class CustomLexerWrapper(Lexer):
-            def __init__(self, lexer_conf):
-                self.lexer = lexer_class(lexer_conf)
-            def lex(self, lexer_state, parser_state):
-                return self.lexer.lex(lexer_state.text)
-        return CustomLexerWrapper
-
-
-def _deserialize_parsing_frontend(data, memo, lexer_conf, callbacks, options):
-    parser_conf = ParserConf.deserialize(data['parser_conf'], memo)
-    cls = (options and options._plugins.get('LALR_Parser')) or LALR_Parser
-    parser = cls.deserialize(data['parser'], memo, callbacks, options.debug)
-    parser_conf.callbacks = callbacks
-    return ParsingFrontend(lexer_conf, parser_conf, options, parser=parser)
-
-
-_parser_creators: 'Dict[str, Callable[[LexerConf, Any, Any], Any]]' = {}
-
-
-class ParsingFrontend(Serialize):
-    __serialize_fields__ = 'lexer_conf', 'parser_conf', 'parser'
-
-    lexer_conf: LexerConf
-    parser_conf: ParserConf
-    options: Any
-
-    def __init__(self, lexer_conf: LexerConf, parser_conf: ParserConf, options, parser=None):
-        self.parser_conf = parser_conf
-        self.lexer_conf = lexer_conf
-        self.options = options
-
-        # Set-up parser
-        if parser:  # From cache
-            self.parser = parser
-        else:
-            create_parser = _parser_creators.get(parser_conf.parser_type)
-            assert create_parser is not None, "{} is not supported in standalone mode".format(
-                    parser_conf.parser_type
-                )
-            self.parser = create_parser(lexer_conf, parser_conf, options)
-
-        # Set-up lexer
-        lexer_type = lexer_conf.lexer_type
-        self.skip_lexer = False
-        if lexer_type in ('dynamic', 'dynamic_complete'):
-            assert lexer_conf.postlex is None
-            self.skip_lexer = True
-            return
-
-        if isinstance(lexer_type, type):
-            assert issubclass(lexer_type, Lexer)
-            self.lexer = _wrap_lexer(lexer_type)(lexer_conf)
-        elif isinstance(lexer_type, str):
-            create_lexer = {
-                'basic': create_basic_lexer,
-                'contextual': create_contextual_lexer,
-            }[lexer_type]
-            self.lexer = create_lexer(lexer_conf, self.parser, lexer_conf.postlex, options)
-        else:
-            raise TypeError("Bad value for lexer_type: {lexer_type}")
-
-        if lexer_conf.postlex:
-            self.lexer = PostLexConnector(self.lexer, lexer_conf.postlex)
-
-    def _verify_start(self, start=None):
-        if start is None:
-            start_decls = self.parser_conf.start
-            if len(start_decls) > 1:
-                raise ConfigurationError("Lark initialized with more than 1 possible start rule. Must specify which start rule to parse", start_decls)
-            start ,= start_decls
-        elif start not in self.parser_conf.start:
-            raise ConfigurationError("Unknown start rule %s. Must be one of %r" % (start, self.parser_conf.start))
-        return start
-
-    def _make_lexer_thread(self, text: str) -> Union[str, LexerThread]:
-        cls = (self.options and self.options._plugins.get('LexerThread')) or LexerThread
-        return text if self.skip_lexer else cls.from_text(self.lexer, text)
-
-    def parse(self, text: str, start=None, on_error=None):
-        chosen_start = self._verify_start(start)
-        kw = {} if on_error is None else {'on_error': on_error}
-        stream = self._make_lexer_thread(text)
-        return self.parser.parse(stream, chosen_start, **kw)
-
-    def parse_interactive(self, text: Optional[str]=None, start=None):
-        # TODO BREAK - Change text from Optional[str] to text: str = ''.
-        #   Would break behavior of exhaust_lexer(), which currently raises TypeError, and after the change would just return []
-        chosen_start = self._verify_start(start)
-        if self.parser_conf.parser_type != 'lalr':
-            raise ConfigurationError("parse_interactive() currently only works with parser='lalr' ")
-        stream = self._make_lexer_thread(text)  # type: ignore[arg-type]
-        return self.parser.parse_interactive(stream, chosen_start)
-
-
-def _validate_frontend_args(parser, lexer) -> None:
-    assert_config(parser, ('lalr', 'earley', 'cyk'))
-    if not isinstance(lexer, type):     # not custom lexer?
-        expected = {
-            'lalr': ('basic', 'contextual'),
-            'earley': ('basic', 'dynamic', 'dynamic_complete'),
-            'cyk': ('basic', ),
-         }[parser]
-        assert_config(lexer, expected, 'Parser %r does not support lexer %%r, expected one of %%s' % parser)
-
-
-def _get_lexer_callbacks(transformer, terminals):
-    result = {}
-    for terminal in terminals:
-        callback = getattr(transformer, terminal.name, None)
-        if callback is not None:
-            result[terminal.name] = callback
-    return result
-
-class PostLexConnector:
-    def __init__(self, lexer, postlexer):
-        self.lexer = lexer
-        self.postlexer = postlexer
-
-    def lex(self, lexer_state, parser_state):
-        i = self.lexer.lex(lexer_state, parser_state)
-        return self.postlexer.process(i)
-
-
-
-def create_basic_lexer(lexer_conf, parser, postlex, options) -> BasicLexer:
-    cls = (options and options._plugins.get('BasicLexer')) or BasicLexer
-    return cls(lexer_conf)
-
-def create_contextual_lexer(lexer_conf: LexerConf, parser, postlex, options) -> ContextualLexer:
-    cls = (options and options._plugins.get('ContextualLexer')) or ContextualLexer
-    parse_table: ParseTableBase[int] = parser._parse_table
-    states: Dict[int, Collection[str]] = {idx:list(t.keys()) for idx, t in parse_table.states.items()}
-    always_accept: Collection[str] = postlex.always_accept if postlex else ()
-    return cls(lexer_conf, states, always_accept=always_accept)
-
-def create_lalr_parser(lexer_conf: LexerConf, parser_conf: ParserConf, options=None) -> LALR_Parser:
-    debug = options.debug if options else False
-    strict = options.strict if options else False
-    cls = (options and options._plugins.get('LALR_Parser')) or LALR_Parser
-    return cls(parser_conf, debug=debug, strict=strict)
-
-_parser_creators['lalr'] = create_lalr_parser
-
-###}
-
-class EarleyRegexpMatcher:
-    def __init__(self, lexer_conf):
-        self.regexps = {}
-        for t in lexer_conf.terminals:
-            regexp = t.pattern.to_regexp()
-            try:
-                width = get_regexp_width(regexp)[0]
-            except ValueError:
-                raise GrammarError("Bad regexp in token %s: %s" % (t.name, regexp))
-            else:
-                if width == 0:
-                    raise GrammarError("Dynamic Earley doesn't allow zero-width regexps", t)
-            if lexer_conf.use_bytes:
-                regexp = regexp.encode('utf-8')
-
-            self.regexps[t.name] = lexer_conf.re_module.compile(regexp, lexer_conf.g_regex_flags)
-
-    def match(self, term, text, index=0):
-        return self.regexps[term.name].match(text, index)
-
-
-def create_earley_parser__dynamic(lexer_conf: LexerConf, parser_conf: ParserConf, **kw):
-    if lexer_conf.callbacks:
-        raise GrammarError("Earley's dynamic lexer doesn't support lexer_callbacks.")
-
-    earley_matcher = EarleyRegexpMatcher(lexer_conf)
-    return xearley.Parser(lexer_conf, parser_conf, earley_matcher.match, **kw)
-
-def _match_earley_basic(term, token):
-    return term.name == token.type
-
-def create_earley_parser__basic(lexer_conf: LexerConf, parser_conf: ParserConf, **kw):
-    return earley.Parser(lexer_conf, parser_conf, _match_earley_basic, **kw)
-
-def create_earley_parser(lexer_conf: LexerConf, parser_conf: ParserConf, options) -> earley.Parser:
-    resolve_ambiguity = options.ambiguity == 'resolve'
-    debug = options.debug if options else False
-    tree_class = options.tree_class or Tree if options.ambiguity != 'forest' else None
-
-    extra = {}
-    if lexer_conf.lexer_type == 'dynamic':
-        f = create_earley_parser__dynamic
-    elif lexer_conf.lexer_type == 'dynamic_complete':
-        extra['complete_lex'] = True
-        f = create_earley_parser__dynamic
-    else:
-        f = create_earley_parser__basic
-
-    return f(lexer_conf, parser_conf, resolve_ambiguity=resolve_ambiguity,
-             debug=debug, tree_class=tree_class, ordered_sets=options.ordered_sets, **extra)
-
-
-
-class CYK_FrontEnd:
-    def __init__(self, lexer_conf, parser_conf, options=None):
-        self.parser = cyk.Parser(parser_conf.rules)
-
-        self.callbacks = parser_conf.callbacks
-
-    def parse(self, lexer_thread, start):
-        tokens = list(lexer_thread.lex(None))
-        tree = self.parser.parse(tokens, start)
-        return self._transform(tree)
-
-    def _transform(self, tree):
-        subtrees = list(tree.iter_subtrees())
-        for subtree in subtrees:
-            subtree.children = [self._apply_callback(c) if isinstance(c, Tree) else c for c in subtree.children]
-
-        return self._apply_callback(tree)
-
-    def _apply_callback(self, tree):
-        return self.callbacks[tree.rule](tree.children)
-
-
-_parser_creators['earley'] = create_earley_parser
-_parser_creators['cyk'] = CYK_FrontEnd
-
-
-def _construct_parsing_frontend(
-        parser_type: _ParserArgType,
-        lexer_type: _LexerArgType,
-        lexer_conf,
-        parser_conf,
-        options
-):
-    assert isinstance(lexer_conf, LexerConf)
-    assert isinstance(parser_conf, ParserConf)
-    parser_conf.parser_type = parser_type
-    lexer_conf.lexer_type = lexer_type
-    return ParsingFrontend(lexer_conf, parser_conf, options)
diff --git a/src/poetry/core/_vendor/lark/parsers/cyk.py b/src/poetry/core/_vendor/lark/parsers/cyk.py
deleted file mode 100644
index b5334f9..0000000
--- a/src/poetry/core/_vendor/lark/parsers/cyk.py
+++ /dev/null
@@ -1,340 +0,0 @@
-"""This module implements a CYK parser."""
-
-# Author: https://github.com/ehudt (2018)
-#
-# Adapted by Erez
-
-
-from collections import defaultdict
-import itertools
-
-from ..exceptions import ParseError
-from ..lexer import Token
-from ..tree import Tree
-from ..grammar import Terminal as T, NonTerminal as NT, Symbol
-
-def match(t, s):
-    assert isinstance(t, T)
-    return t.name == s.type
-
-
-class Rule:
-    """Context-free grammar rule."""
-
-    def __init__(self, lhs, rhs, weight, alias):
-        super(Rule, self).__init__()
-        assert isinstance(lhs, NT), lhs
-        assert all(isinstance(x, NT) or isinstance(x, T) for x in rhs), rhs
-        self.lhs = lhs
-        self.rhs = rhs
-        self.weight = weight
-        self.alias = alias
-
-    def __str__(self):
-        return '%s -> %s' % (str(self.lhs), ' '.join(str(x) for x in self.rhs))
-
-    def __repr__(self):
-        return str(self)
-
-    def __hash__(self):
-        return hash((self.lhs, tuple(self.rhs)))
-
-    def __eq__(self, other):
-        return self.lhs == other.lhs and self.rhs == other.rhs
-
-    def __ne__(self, other):
-        return not (self == other)
-
-
-class Grammar:
-    """Context-free grammar."""
-
-    def __init__(self, rules):
-        self.rules = frozenset(rules)
-
-    def __eq__(self, other):
-        return self.rules == other.rules
-
-    def __str__(self):
-        return '\n' + '\n'.join(sorted(repr(x) for x in self.rules)) + '\n'
-
-    def __repr__(self):
-        return str(self)
-
-
-# Parse tree data structures
-class RuleNode:
-    """A node in the parse tree, which also contains the full rhs rule."""
-
-    def __init__(self, rule, children, weight=0):
-        self.rule = rule
-        self.children = children
-        self.weight = weight
-
-    def __repr__(self):
-        return 'RuleNode(%s, [%s])' % (repr(self.rule.lhs), ', '.join(str(x) for x in self.children))
-
-
-
-class Parser:
-    """Parser wrapper."""
-
-    def __init__(self, rules):
-        super(Parser, self).__init__()
-        self.orig_rules = {rule: rule for rule in rules}
-        rules = [self._to_rule(rule) for rule in rules]
-        self.grammar = to_cnf(Grammar(rules))
-
-    def _to_rule(self, lark_rule):
-        """Converts a lark rule, (lhs, rhs, callback, options), to a Rule."""
-        assert isinstance(lark_rule.origin, NT)
-        assert all(isinstance(x, Symbol) for x in lark_rule.expansion)
-        return Rule(
-            lark_rule.origin, lark_rule.expansion,
-            weight=lark_rule.options.priority if lark_rule.options.priority else 0,
-            alias=lark_rule)
-
-    def parse(self, tokenized, start):  # pylint: disable=invalid-name
-        """Parses input, which is a list of tokens."""
-        assert start
-        start = NT(start)
-
-        table, trees = _parse(tokenized, self.grammar)
-        # Check if the parse succeeded.
-        if all(r.lhs != start for r in table[(0, len(tokenized) - 1)]):
-            raise ParseError('Parsing failed.')
-        parse = trees[(0, len(tokenized) - 1)][start]
-        return self._to_tree(revert_cnf(parse))
-
-    def _to_tree(self, rule_node):
-        """Converts a RuleNode parse tree to a lark Tree."""
-        orig_rule = self.orig_rules[rule_node.rule.alias]
-        children = []
-        for child in rule_node.children:
-            if isinstance(child, RuleNode):
-                children.append(self._to_tree(child))
-            else:
-                assert isinstance(child.name, Token)
-                children.append(child.name)
-        t = Tree(orig_rule.origin, children)
-        t.rule=orig_rule
-        return t
-
-
-def print_parse(node, indent=0):
-    if isinstance(node, RuleNode):
-        print(' ' * (indent * 2) + str(node.rule.lhs))
-        for child in node.children:
-            print_parse(child, indent + 1)
-    else:
-        print(' ' * (indent * 2) + str(node.s))
-
-
-def _parse(s, g):
-    """Parses sentence 's' using CNF grammar 'g'."""
-    # The CYK table. Indexed with a 2-tuple: (start pos, end pos)
-    table = defaultdict(set)
-    # Top-level structure is similar to the CYK table. Each cell is a dict from
-    # rule name to the best (lightest) tree for that rule.
-    trees = defaultdict(dict)
-    # Populate base case with existing terminal production rules
-    for i, w in enumerate(s):
-        for terminal, rules in g.terminal_rules.items():
-            if match(terminal, w):
-                for rule in rules:
-                    table[(i, i)].add(rule)
-                    if (rule.lhs not in trees[(i, i)] or
-                        rule.weight < trees[(i, i)][rule.lhs].weight):
-                        trees[(i, i)][rule.lhs] = RuleNode(rule, [T(w)], weight=rule.weight)
-
-    # Iterate over lengths of sub-sentences
-    for l in range(2, len(s) + 1):
-        # Iterate over sub-sentences with the given length
-        for i in range(len(s) - l + 1):
-            # Choose partition of the sub-sentence in [1, l)
-            for p in range(i + 1, i + l):
-                span1 = (i, p - 1)
-                span2 = (p, i + l - 1)
-                for r1, r2 in itertools.product(table[span1], table[span2]):
-                    for rule in g.nonterminal_rules.get((r1.lhs, r2.lhs), []):
-                        table[(i, i + l - 1)].add(rule)
-                        r1_tree = trees[span1][r1.lhs]
-                        r2_tree = trees[span2][r2.lhs]
-                        rule_total_weight = rule.weight + r1_tree.weight + r2_tree.weight
-                        if (rule.lhs not in trees[(i, i + l - 1)]
-                            or rule_total_weight < trees[(i, i + l - 1)][rule.lhs].weight):
-                            trees[(i, i + l - 1)][rule.lhs] = RuleNode(rule, [r1_tree, r2_tree], weight=rule_total_weight)
-    return table, trees
-
-
-# This section implements context-free grammar converter to Chomsky normal form.
-# It also implements a conversion of parse trees from its CNF to the original
-# grammar.
-# Overview:
-# Applies the following operations in this order:
-# * TERM: Eliminates non-solitary terminals from all rules
-# * BIN: Eliminates rules with more than 2 symbols on their right-hand-side.
-# * UNIT: Eliminates non-terminal unit rules
-#
-# The following grammar characteristics aren't featured:
-# * Start symbol appears on RHS
-# * Empty rules (epsilon rules)
-
-
-class CnfWrapper:
-    """CNF wrapper for grammar.
-
-  Validates that the input grammar is CNF and provides helper data structures.
-  """
-
-    def __init__(self, grammar):
-        super(CnfWrapper, self).__init__()
-        self.grammar = grammar
-        self.rules = grammar.rules
-        self.terminal_rules = defaultdict(list)
-        self.nonterminal_rules = defaultdict(list)
-        for r in self.rules:
-            # Validate that the grammar is CNF and populate auxiliary data structures.
-            assert isinstance(r.lhs, NT), r
-            if len(r.rhs) not in [1, 2]:
-                raise ParseError("CYK doesn't support empty rules")
-            if len(r.rhs) == 1 and isinstance(r.rhs[0], T):
-                self.terminal_rules[r.rhs[0]].append(r)
-            elif len(r.rhs) == 2 and all(isinstance(x, NT) for x in r.rhs):
-                self.nonterminal_rules[tuple(r.rhs)].append(r)
-            else:
-                assert False, r
-
-    def __eq__(self, other):
-        return self.grammar == other.grammar
-
-    def __repr__(self):
-        return repr(self.grammar)
-
-
-class UnitSkipRule(Rule):
-    """A rule that records NTs that were skipped during transformation."""
-
-    def __init__(self, lhs, rhs, skipped_rules, weight, alias):
-        super(UnitSkipRule, self).__init__(lhs, rhs, weight, alias)
-        self.skipped_rules = skipped_rules
-
-    def __eq__(self, other):
-        return isinstance(other, type(self)) and self.skipped_rules == other.skipped_rules
-
-    __hash__ = Rule.__hash__
-
-
-def build_unit_skiprule(unit_rule, target_rule):
-    skipped_rules = []
-    if isinstance(unit_rule, UnitSkipRule):
-        skipped_rules += unit_rule.skipped_rules
-    skipped_rules.append(target_rule)
-    if isinstance(target_rule, UnitSkipRule):
-        skipped_rules += target_rule.skipped_rules
-    return UnitSkipRule(unit_rule.lhs, target_rule.rhs, skipped_rules,
-                      weight=unit_rule.weight + target_rule.weight, alias=unit_rule.alias)
-
-
-def get_any_nt_unit_rule(g):
-    """Returns a non-terminal unit rule from 'g', or None if there is none."""
-    for rule in g.rules:
-        if len(rule.rhs) == 1 and isinstance(rule.rhs[0], NT):
-            return rule
-    return None
-
-
-def _remove_unit_rule(g, rule):
-    """Removes 'rule' from 'g' without changing the language produced by 'g'."""
-    new_rules = [x for x in g.rules if x != rule]
-    refs = [x for x in g.rules if x.lhs == rule.rhs[0]]
-    new_rules += [build_unit_skiprule(rule, ref) for ref in refs]
-    return Grammar(new_rules)
-
-
-def _split(rule):
-    """Splits a rule whose len(rhs) > 2 into shorter rules."""
-    rule_str = str(rule.lhs) + '__' + '_'.join(str(x) for x in rule.rhs)
-    rule_name = '__SP_%s' % (rule_str) + '_%d'
-    yield Rule(rule.lhs, [rule.rhs[0], NT(rule_name % 1)], weight=rule.weight, alias=rule.alias)
-    for i in range(1, len(rule.rhs) - 2):
-        yield Rule(NT(rule_name % i), [rule.rhs[i], NT(rule_name % (i + 1))], weight=0, alias='Split')
-    yield Rule(NT(rule_name % (len(rule.rhs) - 2)), rule.rhs[-2:], weight=0, alias='Split')
-
-
-def _term(g):
-    """Applies the TERM rule on 'g' (see top comment)."""
-    all_t = {x for rule in g.rules for x in rule.rhs if isinstance(x, T)}
-    t_rules = {t: Rule(NT('__T_%s' % str(t)), [t], weight=0, alias='Term') for t in all_t}
-    new_rules = []
-    for rule in g.rules:
-        if len(rule.rhs) > 1 and any(isinstance(x, T) for x in rule.rhs):
-            new_rhs = [t_rules[x].lhs if isinstance(x, T) else x for x in rule.rhs]
-            new_rules.append(Rule(rule.lhs, new_rhs, weight=rule.weight, alias=rule.alias))
-            new_rules.extend(v for k, v in t_rules.items() if k in rule.rhs)
-        else:
-            new_rules.append(rule)
-    return Grammar(new_rules)
-
-
-def _bin(g):
-    """Applies the BIN rule to 'g' (see top comment)."""
-    new_rules = []
-    for rule in g.rules:
-        if len(rule.rhs) > 2:
-            new_rules += _split(rule)
-        else:
-            new_rules.append(rule)
-    return Grammar(new_rules)
-
-
-def _unit(g):
-    """Applies the UNIT rule to 'g' (see top comment)."""
-    nt_unit_rule = get_any_nt_unit_rule(g)
-    while nt_unit_rule:
-        g = _remove_unit_rule(g, nt_unit_rule)
-        nt_unit_rule = get_any_nt_unit_rule(g)
-    return g
-
-
-def to_cnf(g):
-    """Creates a CNF grammar from a general context-free grammar 'g'."""
-    g = _unit(_bin(_term(g)))
-    return CnfWrapper(g)
-
-
-def unroll_unit_skiprule(lhs, orig_rhs, skipped_rules, children, weight, alias):
-    if not skipped_rules:
-        return RuleNode(Rule(lhs, orig_rhs, weight=weight, alias=alias), children, weight=weight)
-    else:
-        weight = weight - skipped_rules[0].weight
-        return RuleNode(
-            Rule(lhs, [skipped_rules[0].lhs], weight=weight, alias=alias), [
-                unroll_unit_skiprule(skipped_rules[0].lhs, orig_rhs,
-                                skipped_rules[1:], children,
-                                skipped_rules[0].weight, skipped_rules[0].alias)
-            ], weight=weight)
-
-
-def revert_cnf(node):
-    """Reverts a parse tree (RuleNode) to its original non-CNF form (Node)."""
-    if isinstance(node, T):
-        return node
-    # Reverts TERM rule.
-    if node.rule.lhs.name.startswith('__T_'):
-        return node.children[0]
-    else:
-        children = []
-        for child in map(revert_cnf, node.children):
-            # Reverts BIN rule.
-            if isinstance(child, RuleNode) and child.rule.lhs.name.startswith('__SP_'):
-                children += child.children
-            else:
-                children.append(child)
-        # Reverts UNIT rule.
-        if isinstance(node.rule, UnitSkipRule):
-            return unroll_unit_skiprule(node.rule.lhs, node.rule.rhs,
-                                    node.rule.skipped_rules, children,
-                                    node.rule.weight, node.rule.alias)
-        else:
-            return RuleNode(node.rule, children)
diff --git a/src/poetry/core/_vendor/lark/parsers/earley.py b/src/poetry/core/_vendor/lark/parsers/earley.py
deleted file mode 100644
index bc85886..0000000
--- a/src/poetry/core/_vendor/lark/parsers/earley.py
+++ /dev/null
@@ -1,308 +0,0 @@
-"""This module implements an Earley parser.
-
-The core Earley algorithm used here is based on Elizabeth Scott's implementation, here:
-    https://www.sciencedirect.com/science/article/pii/S1571066108001497
-
-That is probably the best reference for understanding the algorithm here.
-
-The Earley parser outputs an SPPF-tree as per that document. The SPPF tree format
-is explained here: https://lark-parser.readthedocs.io/en/latest/_static/sppf/sppf.html
-"""
-
-from typing import TYPE_CHECKING, Callable, Optional, List, Any
-from collections import deque
-
-from ..lexer import Token
-from ..tree import Tree
-from ..exceptions import UnexpectedEOF, UnexpectedToken
-from ..utils import logger, OrderedSet
-from .grammar_analysis import GrammarAnalyzer
-from ..grammar import NonTerminal
-from .earley_common import Item
-from .earley_forest import ForestSumVisitor, SymbolNode, StableSymbolNode, TokenNode, ForestToParseTree
-
-if TYPE_CHECKING:
-    from ..common import LexerConf, ParserConf
-
-class Parser:
-    lexer_conf: 'LexerConf'
-    parser_conf: 'ParserConf'
-    debug: bool
-
-    def __init__(self, lexer_conf: 'LexerConf', parser_conf: 'ParserConf', term_matcher: Callable,
-                 resolve_ambiguity: bool=True, debug: bool=False,
-                 tree_class: Optional[Callable[[str, List], Any]]=Tree, ordered_sets: bool=True):
-        analysis = GrammarAnalyzer(parser_conf)
-        self.lexer_conf = lexer_conf
-        self.parser_conf = parser_conf
-        self.resolve_ambiguity = resolve_ambiguity
-        self.debug = debug
-        self.Tree = tree_class
-        self.Set = OrderedSet if ordered_sets else set
-        self.SymbolNode = StableSymbolNode if ordered_sets else SymbolNode
-
-        self.FIRST = analysis.FIRST
-        self.NULLABLE = analysis.NULLABLE
-        self.callbacks = parser_conf.callbacks
-        # TODO add typing info
-        self.predictions = {}   # type: ignore[var-annotated]
-
-        ## These could be moved to the grammar analyzer. Pre-computing these is *much* faster than
-        #  the slow 'isupper' in is_terminal.
-        self.TERMINALS = { sym for r in parser_conf.rules for sym in r.expansion if sym.is_term }
-        self.NON_TERMINALS = { sym for r in parser_conf.rules for sym in r.expansion if not sym.is_term }
-
-        self.forest_sum_visitor = None
-        for rule in parser_conf.rules:
-            if rule.origin not in self.predictions:
-                self.predictions[rule.origin] = [x.rule for x in analysis.expand_rule(rule.origin)]
-
-            ## Detect if any rules/terminals have priorities set. If the user specified priority = None, then
-            #  the priorities will be stripped from all rules/terminals before they reach us, allowing us to
-            #  skip the extra tree walk. We'll also skip this if the user just didn't specify priorities
-            #  on any rules/terminals.
-            if self.forest_sum_visitor is None and rule.options.priority is not None:
-                self.forest_sum_visitor = ForestSumVisitor
-
-        # Check terminals for priorities
-        # Ignore terminal priorities if the basic lexer is used
-        if self.lexer_conf.lexer_type != 'basic' and self.forest_sum_visitor is None:
-            for term in self.lexer_conf.terminals:
-                if term.priority:
-                    self.forest_sum_visitor = ForestSumVisitor
-                    break
-
-        self.term_matcher = term_matcher
-
-
-    def predict_and_complete(self, i, to_scan, columns, transitives):
-        """The core Earley Predictor and Completer.
-
-        At each stage of the input, we handling any completed items (things
-        that matched on the last cycle) and use those to predict what should
-        come next in the input stream. The completions and any predicted
-        non-terminals are recursively processed until we reach a set of,
-        which can be added to the scan list for the next scanner cycle."""
-        # Held Completions (H in E.Scotts paper).
-        node_cache = {}
-        held_completions = {}
-
-        column = columns[i]
-        # R (items) = Ei (column.items)
-        items = deque(column)
-        while items:
-            item = items.pop()    # remove an element, A say, from R
-
-            ### The Earley completer
-            if item.is_complete:   ### (item.s == string)
-                if item.node is None:
-                    label = (item.s, item.start, i)
-                    item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, self.SymbolNode(*label))
-                    item.node.add_family(item.s, item.rule, item.start, None, None)
-
-                # create_leo_transitives(item.rule.origin, item.start)
-
-                ###R Joop Leo right recursion Completer
-                if item.rule.origin in transitives[item.start]:
-                    transitive = transitives[item.start][item.s]
-                    if transitive.previous in transitives[transitive.column]:
-                        root_transitive = transitives[transitive.column][transitive.previous]
-                    else:
-                        root_transitive = transitive
-
-                    new_item = Item(transitive.rule, transitive.ptr, transitive.start)
-                    label = (root_transitive.s, root_transitive.start, i)
-                    new_item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, self.SymbolNode(*label))
-                    new_item.node.add_path(root_transitive, item.node)
-                    if new_item.expect in self.TERMINALS:
-                        # Add (B :: aC.B, h, y) to Q
-                        to_scan.add(new_item)
-                    elif new_item not in column:
-                        # Add (B :: aC.B, h, y) to Ei and R
-                        column.add(new_item)
-                        items.append(new_item)
-                ###R Regular Earley completer
-                else:
-                    # Empty has 0 length. If we complete an empty symbol in a particular
-                    # parse step, we need to be able to use that same empty symbol to complete
-                    # any predictions that result, that themselves require empty. Avoids
-                    # infinite recursion on empty symbols.
-                    # held_completions is 'H' in E.Scott's paper.
-                    is_empty_item = item.start == i
-                    if is_empty_item:
-                        held_completions[item.rule.origin] = item.node
-
-                    originators = [originator for originator in columns[item.start] if originator.expect is not None and originator.expect == item.s]
-                    for originator in originators:
-                        new_item = originator.advance()
-                        label = (new_item.s, originator.start, i)
-                        new_item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, self.SymbolNode(*label))
-                        new_item.node.add_family(new_item.s, new_item.rule, i, originator.node, item.node)
-                        if new_item.expect in self.TERMINALS:
-                            # Add (B :: aC.B, h, y) to Q
-                            to_scan.add(new_item)
-                        elif new_item not in column:
-                            # Add (B :: aC.B, h, y) to Ei and R
-                            column.add(new_item)
-                            items.append(new_item)
-
-            ### The Earley predictor
-            elif item.expect in self.NON_TERMINALS: ### (item.s == lr0)
-                new_items = []
-                for rule in self.predictions[item.expect]:
-                    new_item = Item(rule, 0, i)
-                    new_items.append(new_item)
-
-                # Process any held completions (H).
-                if item.expect in held_completions:
-                    new_item = item.advance()
-                    label = (new_item.s, item.start, i)
-                    new_item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, self.SymbolNode(*label))
-                    new_item.node.add_family(new_item.s, new_item.rule, new_item.start, item.node, held_completions[item.expect])
-                    new_items.append(new_item)
-
-                for new_item in new_items:
-                    if new_item.expect in self.TERMINALS:
-                        to_scan.add(new_item)
-                    elif new_item not in column:
-                        column.add(new_item)
-                        items.append(new_item)
-
-    def _parse(self, lexer, columns, to_scan, start_symbol=None):
-        def is_quasi_complete(item):
-            if item.is_complete:
-                return True
-
-            quasi = item.advance()
-            while not quasi.is_complete:
-                if quasi.expect not in self.NULLABLE:
-                    return False
-                if quasi.rule.origin == start_symbol and quasi.expect == start_symbol:
-                    return False
-                quasi = quasi.advance()
-            return True
-
-        # def create_leo_transitives(origin, start):
-        #   ...   # removed at commit 4c1cfb2faf24e8f8bff7112627a00b94d261b420
-
-        def scan(i, token, to_scan):
-            """The core Earley Scanner.
-
-            This is a custom implementation of the scanner that uses the
-            Lark lexer to match tokens. The scan list is built by the
-            Earley predictor, based on the previously completed tokens.
-            This ensures that at each phase of the parse we have a custom
-            lexer context, allowing for more complex ambiguities."""
-            next_to_scan = self.Set()
-            next_set = self.Set()
-            columns.append(next_set)
-            transitives.append({})
-            node_cache = {}
-
-            for item in self.Set(to_scan):
-                if match(item.expect, token):
-                    new_item = item.advance()
-                    label = (new_item.s, new_item.start, i)
-                    # 'terminals' may not contain token.type when using %declare
-                    # Additionally, token is not always a Token
-                    # For example, it can be a Tree when using TreeMatcher
-                    term = terminals.get(token.type) if isinstance(token, Token) else None
-                    # Set the priority of the token node to 0 so that the
-                    # terminal priorities do not affect the Tree chosen by
-                    # ForestSumVisitor after the basic lexer has already
-                    # "used up" the terminal priorities
-                    token_node = TokenNode(token, term, priority=0)
-                    new_item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, self.SymbolNode(*label))
-                    new_item.node.add_family(new_item.s, item.rule, new_item.start, item.node, token_node)
-
-                    if new_item.expect in self.TERMINALS:
-                        # add (B ::= Aai+1.B, h, y) to Q'
-                        next_to_scan.add(new_item)
-                    else:
-                        # add (B ::= Aa+1.B, h, y) to Ei+1
-                        next_set.add(new_item)
-
-            if not next_set and not next_to_scan:
-                expect = {i.expect.name for i in to_scan}
-                raise UnexpectedToken(token, expect, considered_rules=set(to_scan), state=frozenset(i.s for i in to_scan))
-
-            return next_to_scan
-
-
-        # Define parser functions
-        match = self.term_matcher
-
-        terminals = self.lexer_conf.terminals_by_name
-
-        # Cache for nodes & tokens created in a particular parse step.
-        transitives = [{}]
-
-        ## The main Earley loop.
-        # Run the Prediction/Completion cycle for any Items in the current Earley set.
-        # Completions will be added to the SPPF tree, and predictions will be recursively
-        # processed down to terminals/empty nodes to be added to the scanner for the next
-        # step.
-        expects = {i.expect for i in to_scan}
-        i = 0
-        for token in lexer.lex(expects):
-            self.predict_and_complete(i, to_scan, columns, transitives)
-
-            to_scan = scan(i, token, to_scan)
-            i += 1
-
-            expects.clear()
-            expects |= {i.expect for i in to_scan}
-
-        self.predict_and_complete(i, to_scan, columns, transitives)
-
-        ## Column is now the final column in the parse.
-        assert i == len(columns)-1
-        return to_scan
-
-    def parse(self, lexer, start):
-        assert start, start
-        start_symbol = NonTerminal(start)
-
-        columns = [self.Set()]
-        to_scan = self.Set()     # The scan buffer. 'Q' in E.Scott's paper.
-
-        ## Predict for the start_symbol.
-        # Add predicted items to the first Earley set (for the predictor) if they
-        # result in a non-terminal, or the scanner if they result in a terminal.
-        for rule in self.predictions[start_symbol]:
-            item = Item(rule, 0, 0)
-            if item.expect in self.TERMINALS:
-                to_scan.add(item)
-            else:
-                columns[0].add(item)
-
-        to_scan = self._parse(lexer, columns, to_scan, start_symbol)
-
-        # If the parse was successful, the start
-        # symbol should have been completed in the last step of the Earley cycle, and will be in
-        # this column. Find the item for the start_symbol, which is the root of the SPPF tree.
-        solutions = [n.node for n in columns[-1] if n.is_complete and n.node is not None and n.s == start_symbol and n.start == 0]
-        if not solutions:
-            expected_terminals = [t.expect.name for t in to_scan]
-            raise UnexpectedEOF(expected_terminals, state=frozenset(i.s for i in to_scan))
-
-        if self.debug:
-            from .earley_forest import ForestToPyDotVisitor
-            try:
-                debug_walker = ForestToPyDotVisitor()
-            except ImportError:
-                logger.warning("Cannot find dependency 'pydot', will not generate sppf debug image")
-            else:
-                debug_walker.visit(solutions[0], "sppf.png")
-
-
-        if len(solutions) > 1:
-            assert False, 'Earley should not generate multiple start symbol items!'
-
-        if self.Tree is not None:
-            # Perform our SPPF -> AST conversion
-            transformer = ForestToParseTree(self.Tree, self.callbacks, self.forest_sum_visitor and self.forest_sum_visitor(), self.resolve_ambiguity)
-            return transformer.transform(solutions[0])
-
-        # return the root of the SPPF
-        return solutions[0]
diff --git a/src/poetry/core/_vendor/lark/parsers/earley_common.py b/src/poetry/core/_vendor/lark/parsers/earley_common.py
deleted file mode 100644
index 46e242b..0000000
--- a/src/poetry/core/_vendor/lark/parsers/earley_common.py
+++ /dev/null
@@ -1,42 +0,0 @@
-"""This module implements useful building blocks for the Earley parser
-"""
-
-
-class Item:
-    "An Earley Item, the atom of the algorithm."
-
-    __slots__ = ('s', 'rule', 'ptr', 'start', 'is_complete', 'expect', 'previous', 'node', '_hash')
-    def __init__(self, rule, ptr, start):
-        self.is_complete = len(rule.expansion) == ptr
-        self.rule = rule    # rule
-        self.ptr = ptr      # ptr
-        self.start = start  # j
-        self.node = None    # w
-        if self.is_complete:
-            self.s = rule.origin
-            self.expect = None
-            self.previous = rule.expansion[ptr - 1] if ptr > 0 and len(rule.expansion) else None
-        else:
-            self.s = (rule, ptr)
-            self.expect = rule.expansion[ptr]
-            self.previous = rule.expansion[ptr - 1] if ptr > 0 and len(rule.expansion) else None
-        self._hash = hash((self.s, self.start))
-
-    def advance(self):
-        return Item(self.rule, self.ptr + 1, self.start)
-
-    def __eq__(self, other):
-        return self is other or (self.s == other.s and self.start == other.start)
-
-    def __hash__(self):
-        return self._hash
-
-    def __repr__(self):
-        before = ( expansion.name for expansion in self.rule.expansion[:self.ptr] )
-        after = ( expansion.name for expansion in self.rule.expansion[self.ptr:] )
-        symbol = "{} ::= {}* {}".format(self.rule.origin.name, ' '.join(before), ' '.join(after))
-        return '%s (%d)' % (symbol, self.start)
-
-
-# class TransitiveItem(Item):
-#   ...   # removed at commit 4c1cfb2faf24e8f8bff7112627a00b94d261b420
diff --git a/src/poetry/core/_vendor/lark/parsers/earley_forest.py b/src/poetry/core/_vendor/lark/parsers/earley_forest.py
deleted file mode 100644
index 301f4d1..0000000
--- a/src/poetry/core/_vendor/lark/parsers/earley_forest.py
+++ /dev/null
@@ -1,810 +0,0 @@
-""""This module implements an SPPF implementation
-
-This is used as the primary output mechanism for the Earley parser
-in order to store complex ambiguities.
-
-Full reference and more details is here:
-https://web.archive.org/web/20190616123959/http://www.bramvandersanden.com/post/2014/06/shared-packed-parse-forest/
-"""
-
-from typing import Type, AbstractSet
-from random import randint
-from collections import deque
-from operator import attrgetter
-from importlib import import_module
-from functools import partial
-
-from ..parse_tree_builder import AmbiguousIntermediateExpander
-from ..visitors import Discard
-from ..utils import logger, OrderedSet
-from ..tree import Tree
-
-class ForestNode:
-    pass
-
-class SymbolNode(ForestNode):
-    """
-    A Symbol Node represents a symbol (or Intermediate LR0).
-
-    Symbol nodes are keyed by the symbol (s). For intermediate nodes
-    s will be an LR0, stored as a tuple of (rule, ptr). For completed symbol
-    nodes, s will be a string representing the non-terminal origin (i.e.
-    the left hand side of the rule).
-
-    The children of a Symbol or Intermediate Node will always be Packed Nodes;
-    with each Packed Node child representing a single derivation of a production.
-
-    Hence a Symbol Node with a single child is unambiguous.
-
-    Parameters:
-        s: A Symbol, or a tuple of (rule, ptr) for an intermediate node.
-        start: The index of the start of the substring matched by this symbol (inclusive).
-        end: The index of the end of the substring matched by this symbol (exclusive).
-
-    Properties:
-        is_intermediate: True if this node is an intermediate node.
-        priority: The priority of the node's symbol.
-    """
-    Set: Type[AbstractSet] = set   # Overridden by StableSymbolNode
-    __slots__ = ('s', 'start', 'end', '_children', 'paths', 'paths_loaded', 'priority', 'is_intermediate', '_hash')
-    def __init__(self, s, start, end):
-        self.s = s
-        self.start = start
-        self.end = end
-        self._children = self.Set()
-        self.paths = self.Set()
-        self.paths_loaded = False
-
-        ### We use inf here as it can be safely negated without resorting to conditionals,
-        #   unlike None or float('NaN'), and sorts appropriately.
-        self.priority = float('-inf')
-        self.is_intermediate = isinstance(s, tuple)
-        self._hash = hash((self.s, self.start, self.end))
-
-    def add_family(self, lr0, rule, start, left, right):
-        self._children.add(PackedNode(self, lr0, rule, start, left, right))
-
-    def add_path(self, transitive, node):
-        self.paths.add((transitive, node))
-
-    def load_paths(self):
-        for transitive, node in self.paths:
-            if transitive.next_titem is not None:
-                vn = type(self)(transitive.next_titem.s, transitive.next_titem.start, self.end)
-                vn.add_path(transitive.next_titem, node)
-                self.add_family(transitive.reduction.rule.origin, transitive.reduction.rule, transitive.reduction.start, transitive.reduction.node, vn)
-            else:
-                self.add_family(transitive.reduction.rule.origin, transitive.reduction.rule, transitive.reduction.start, transitive.reduction.node, node)
-        self.paths_loaded = True
-
-    @property
-    def is_ambiguous(self):
-        """Returns True if this node is ambiguous."""
-        return len(self.children) > 1
-
-    @property
-    def children(self):
-        """Returns a list of this node's children sorted from greatest to
-        least priority."""
-        if not self.paths_loaded:
-            self.load_paths()
-        return sorted(self._children, key=attrgetter('sort_key'))
-
-    def __iter__(self):
-        return iter(self._children)
-
-    def __eq__(self, other):
-        if not isinstance(other, SymbolNode):
-            return False
-        return self is other or (type(self.s) == type(other.s) and self.s == other.s and self.start == other.start and self.end is other.end)
-
-    def __hash__(self):
-        return self._hash
-
-    def __repr__(self):
-        if self.is_intermediate:
-            rule = self.s[0]
-            ptr = self.s[1]
-            before = ( expansion.name for expansion in rule.expansion[:ptr] )
-            after = ( expansion.name for expansion in rule.expansion[ptr:] )
-            symbol = "{} ::= {}* {}".format(rule.origin.name, ' '.join(before), ' '.join(after))
-        else:
-            symbol = self.s.name
-        return "({}, {}, {}, {})".format(symbol, self.start, self.end, self.priority)
-
-class StableSymbolNode(SymbolNode):
-    "A version of SymbolNode that uses OrderedSet for output stability"
-    Set = OrderedSet
-
-class PackedNode(ForestNode):
-    """
-    A Packed Node represents a single derivation in a symbol node.
-
-    Parameters:
-        rule: The rule associated with this node.
-        parent: The parent of this node.
-        left: The left child of this node. ``None`` if one does not exist.
-        right: The right child of this node. ``None`` if one does not exist.
-        priority: The priority of this node.
-    """
-    __slots__ = ('parent', 's', 'rule', 'start', 'left', 'right', 'priority', '_hash')
-    def __init__(self, parent, s, rule, start, left, right):
-        self.parent = parent
-        self.s = s
-        self.start = start
-        self.rule = rule
-        self.left = left
-        self.right = right
-        self.priority = float('-inf')
-        self._hash = hash((self.left, self.right))
-
-    @property
-    def is_empty(self):
-        return self.left is None and self.right is None
-
-    @property
-    def sort_key(self):
-        """
-        Used to sort PackedNode children of SymbolNodes.
-        A SymbolNode has multiple PackedNodes if it matched
-        ambiguously. Hence, we use the sort order to identify
-        the order in which ambiguous children should be considered.
-        """
-        return self.is_empty, -self.priority, self.rule.order
-
-    @property
-    def children(self):
-        """Returns a list of this node's children."""
-        return [x for x in [self.left, self.right] if x is not None]
-
-    def __iter__(self):
-        yield self.left
-        yield self.right
-
-    def __eq__(self, other):
-        if not isinstance(other, PackedNode):
-            return False
-        return self is other or (self.left == other.left and self.right == other.right)
-
-    def __hash__(self):
-        return self._hash
-
-    def __repr__(self):
-        if isinstance(self.s, tuple):
-            rule = self.s[0]
-            ptr = self.s[1]
-            before = ( expansion.name for expansion in rule.expansion[:ptr] )
-            after = ( expansion.name for expansion in rule.expansion[ptr:] )
-            symbol = "{} ::= {}* {}".format(rule.origin.name, ' '.join(before), ' '.join(after))
-        else:
-            symbol = self.s.name
-        return "({}, {}, {}, {})".format(symbol, self.start, self.priority, self.rule.order)
-
-class TokenNode(ForestNode):
-    """
-    A Token Node represents a matched terminal and is always a leaf node.
-
-    Parameters:
-        token: The Token associated with this node.
-        term: The TerminalDef matched by the token.
-        priority: The priority of this node.
-    """
-    __slots__ = ('token', 'term', 'priority', '_hash')
-    def __init__(self, token, term, priority=None):
-        self.token = token
-        self.term = term
-        if priority is not None:
-            self.priority = priority
-        else:
-            self.priority = term.priority if term is not None else 0
-        self._hash = hash(token)
-
-    def __eq__(self, other):
-        if not isinstance(other, TokenNode):
-            return False
-        return self is other or (self.token == other.token)
-
-    def __hash__(self):
-        return self._hash
-
-    def __repr__(self):
-        return repr(self.token)
-
-class ForestVisitor:
-    """
-    An abstract base class for building forest visitors.
-
-    This class performs a controllable depth-first walk of an SPPF.
-    The visitor will not enter cycles and will backtrack if one is encountered.
-    Subclasses are notified of cycles through the ``on_cycle`` method.
-
-    Behavior for visit events is defined by overriding the
-    ``visit*node*`` functions.
-
-    The walk is controlled by the return values of the ``visit*node_in``
-    methods. Returning a node(s) will schedule them to be visited. The visitor
-    will begin to backtrack if no nodes are returned.
-
-    Parameters:
-        single_visit: If ``True``, non-Token nodes will only be visited once.
-    """
-
-    def __init__(self, single_visit=False):
-        self.single_visit = single_visit
-
-    def visit_token_node(self, node):
-        """Called when a ``Token`` is visited. ``Token`` nodes are always leaves."""
-        pass
-
-    def visit_symbol_node_in(self, node):
-        """Called when a symbol node is visited. Nodes that are returned
-        will be scheduled to be visited. If ``visit_intermediate_node_in``
-        is not implemented, this function will be called for intermediate
-        nodes as well."""
-        pass
-
-    def visit_symbol_node_out(self, node):
-        """Called after all nodes returned from a corresponding ``visit_symbol_node_in``
-        call have been visited. If ``visit_intermediate_node_out``
-        is not implemented, this function will be called for intermediate
-        nodes as well."""
-        pass
-
-    def visit_packed_node_in(self, node):
-        """Called when a packed node is visited. Nodes that are returned
-        will be scheduled to be visited. """
-        pass
-
-    def visit_packed_node_out(self, node):
-        """Called after all nodes returned from a corresponding ``visit_packed_node_in``
-        call have been visited."""
-        pass
-
-    def on_cycle(self, node, path):
-        """Called when a cycle is encountered.
-
-        Parameters:
-            node: The node that causes a cycle.
-            path: The list of nodes being visited: nodes that have been
-                entered but not exited. The first element is the root in a forest
-                visit, and the last element is the node visited most recently.
-                ``path`` should be treated as read-only.
-        """
-        pass
-
-    def get_cycle_in_path(self, node, path):
-        """A utility function for use in ``on_cycle`` to obtain a slice of
-        ``path`` that only contains the nodes that make up the cycle."""
-        index = len(path) - 1
-        while id(path[index]) != id(node):
-            index -= 1
-        return path[index:]
-
-    def visit(self, root):
-        # Visiting is a list of IDs of all symbol/intermediate nodes currently in
-        # the stack. It serves two purposes: to detect when we 'recurse' in and out
-        # of a symbol/intermediate so that we can process both up and down. Also,
-        # since the SPPF can have cycles it allows us to detect if we're trying
-        # to recurse into a node that's already on the stack (infinite recursion).
-        visiting = set()
-
-        # set of all nodes that have been visited
-        visited = set()
-
-        # a list of nodes that are currently being visited
-        # used for the `on_cycle` callback
-        path = []
-
-        # We do not use recursion here to walk the Forest due to the limited
-        # stack size in python. Therefore input_stack is essentially our stack.
-        input_stack = deque([root])
-
-        # It is much faster to cache these as locals since they are called
-        # many times in large parses.
-        vpno = getattr(self, 'visit_packed_node_out')
-        vpni = getattr(self, 'visit_packed_node_in')
-        vsno = getattr(self, 'visit_symbol_node_out')
-        vsni = getattr(self, 'visit_symbol_node_in')
-        vino = getattr(self, 'visit_intermediate_node_out', vsno)
-        vini = getattr(self, 'visit_intermediate_node_in', vsni)
-        vtn = getattr(self, 'visit_token_node')
-        oc = getattr(self, 'on_cycle')
-
-        while input_stack:
-            current = next(reversed(input_stack))
-            try:
-                next_node = next(current)
-            except StopIteration:
-                input_stack.pop()
-                continue
-            except TypeError:
-                ### If the current object is not an iterator, pass through to Token/SymbolNode
-                pass
-            else:
-                if next_node is None:
-                    continue
-
-                if id(next_node) in visiting:
-                    oc(next_node, path)
-                    continue
-
-                input_stack.append(next_node)
-                continue
-
-            if isinstance(current, TokenNode):
-                vtn(current.token)
-                input_stack.pop()
-                continue
-
-            current_id = id(current)
-            if current_id in visiting:
-                if isinstance(current, PackedNode):
-                    vpno(current)
-                elif current.is_intermediate:
-                    vino(current)
-                else:
-                    vsno(current)
-                input_stack.pop()
-                path.pop()
-                visiting.remove(current_id)
-                visited.add(current_id)
-            elif self.single_visit and current_id in visited:
-                input_stack.pop()
-            else:
-                visiting.add(current_id)
-                path.append(current)
-                if isinstance(current, PackedNode):
-                    next_node = vpni(current)
-                elif current.is_intermediate:
-                    next_node = vini(current)
-                else:
-                    next_node = vsni(current)
-                if next_node is None:
-                    continue
-
-                if not isinstance(next_node, ForestNode):
-                    next_node = iter(next_node)
-                elif id(next_node) in visiting:
-                    oc(next_node, path)
-                    continue
-
-                input_stack.append(next_node)
-
-class ForestTransformer(ForestVisitor):
-    """The base class for a bottom-up forest transformation. Most users will
-    want to use ``TreeForestTransformer`` instead as it has a friendlier
-    interface and covers most use cases.
-
-    Transformations are applied via inheritance and overriding of the
-    ``transform*node`` methods.
-
-    ``transform_token_node`` receives a ``Token`` as an argument.
-    All other methods receive the node that is being transformed and
-    a list of the results of the transformations of that node's children.
-    The return value of these methods are the resulting transformations.
-
-    If ``Discard`` is raised in a node's transformation, no data from that node
-    will be passed to its parent's transformation.
-    """
-
-    def __init__(self):
-        super(ForestTransformer, self).__init__()
-        # results of transformations
-        self.data = dict()
-        # used to track parent nodes
-        self.node_stack = deque()
-
-    def transform(self, root):
-        """Perform a transformation on an SPPF."""
-        self.node_stack.append('result')
-        self.data['result'] = []
-        self.visit(root)
-        assert len(self.data['result']) <= 1
-        if self.data['result']:
-            return self.data['result'][0]
-
-    def transform_symbol_node(self, node, data):
-        """Transform a symbol node."""
-        return node
-
-    def transform_intermediate_node(self, node, data):
-        """Transform an intermediate node."""
-        return node
-
-    def transform_packed_node(self, node, data):
-        """Transform a packed node."""
-        return node
-
-    def transform_token_node(self, node):
-        """Transform a ``Token``."""
-        return node
-
-    def visit_symbol_node_in(self, node):
-        self.node_stack.append(id(node))
-        self.data[id(node)] = []
-        return node.children
-
-    def visit_packed_node_in(self, node):
-        self.node_stack.append(id(node))
-        self.data[id(node)] = []
-        return node.children
-
-    def visit_token_node(self, node):
-        transformed = self.transform_token_node(node)
-        if transformed is not Discard:
-            self.data[self.node_stack[-1]].append(transformed)
-
-    def _visit_node_out_helper(self, node, method):
-        self.node_stack.pop()
-        transformed = method(node, self.data[id(node)])
-        if transformed is not Discard:
-            self.data[self.node_stack[-1]].append(transformed)
-        del self.data[id(node)]
-
-    def visit_symbol_node_out(self, node):
-        self._visit_node_out_helper(node, self.transform_symbol_node)
-
-    def visit_intermediate_node_out(self, node):
-        self._visit_node_out_helper(node, self.transform_intermediate_node)
-
-    def visit_packed_node_out(self, node):
-        self._visit_node_out_helper(node, self.transform_packed_node)
-
-
-class ForestSumVisitor(ForestVisitor):
-    """
-    A visitor for prioritizing ambiguous parts of the Forest.
-
-    This visitor is used when support for explicit priorities on
-    rules is requested (whether normal, or invert). It walks the
-    forest (or subsets thereof) and cascades properties upwards
-    from the leaves.
-
-    It would be ideal to do this during parsing, however this would
-    require processing each Earley item multiple times. That's
-    a big performance drawback; so running a forest walk is the
-    lesser of two evils: there can be significantly more Earley
-    items created during parsing than there are SPPF nodes in the
-    final tree.
-    """
-    def __init__(self):
-        super(ForestSumVisitor, self).__init__(single_visit=True)
-
-    def visit_packed_node_in(self, node):
-        yield node.left
-        yield node.right
-
-    def visit_symbol_node_in(self, node):
-        return iter(node.children)
-
-    def visit_packed_node_out(self, node):
-        priority = node.rule.options.priority if not node.parent.is_intermediate and node.rule.options.priority else 0
-        priority += getattr(node.right, 'priority', 0)
-        priority += getattr(node.left, 'priority', 0)
-        node.priority = priority
-
-    def visit_symbol_node_out(self, node):
-        node.priority = max(child.priority for child in node.children)
-
-class PackedData():
-    """Used in transformationss of packed nodes to distinguish the data
-    that comes from the left child and the right child.
-    """
-
-    class _NoData():
-        pass
-
-    NO_DATA = _NoData()
-
-    def __init__(self, node, data):
-        self.left = self.NO_DATA
-        self.right = self.NO_DATA
-        if data:
-            if node.left is not None:
-                self.left = data[0]
-                if len(data) > 1:
-                    self.right = data[1]
-            else:
-                self.right = data[0]
-
-class ForestToParseTree(ForestTransformer):
-    """Used by the earley parser when ambiguity equals 'resolve' or
-    'explicit'. Transforms an SPPF into an (ambiguous) parse tree.
-
-    Parameters:
-        tree_class: The tree class to use for construction
-        callbacks: A dictionary of rules to functions that output a tree
-        prioritizer: A ``ForestVisitor`` that manipulates the priorities of ForestNodes
-        resolve_ambiguity: If True, ambiguities will be resolved based on
-                        priorities. Otherwise, `_ambig` nodes will be in the resulting tree.
-        use_cache: If True, the results of packed node transformations will be cached.
-    """
-
-    def __init__(self, tree_class=Tree, callbacks=dict(), prioritizer=ForestSumVisitor(), resolve_ambiguity=True, use_cache=True):
-        super(ForestToParseTree, self).__init__()
-        self.tree_class = tree_class
-        self.callbacks = callbacks
-        self.prioritizer = prioritizer
-        self.resolve_ambiguity = resolve_ambiguity
-        self._use_cache = use_cache
-        self._cache = {}
-        self._on_cycle_retreat = False
-        self._cycle_node = None
-        self._successful_visits = set()
-
-    def visit(self, root):
-        if self.prioritizer:
-            self.prioritizer.visit(root)
-        super(ForestToParseTree, self).visit(root)
-        self._cache = {}
-
-    def on_cycle(self, node, path):
-        logger.debug("Cycle encountered in the SPPF at node: %s. "
-                "As infinite ambiguities cannot be represented in a tree, "
-                "this family of derivations will be discarded.", node)
-        self._cycle_node = node
-        self._on_cycle_retreat = True
-
-    def _check_cycle(self, node):
-        if self._on_cycle_retreat:
-            if id(node) == id(self._cycle_node) or id(node) in self._successful_visits:
-                self._cycle_node = None
-                self._on_cycle_retreat = False
-            else:
-                return Discard
-
-    def _collapse_ambig(self, children):
-        new_children = []
-        for child in children:
-            if hasattr(child, 'data') and child.data == '_ambig':
-                new_children += child.children
-            else:
-                new_children.append(child)
-        return new_children
-
-    def _call_rule_func(self, node, data):
-        # called when transforming children of symbol nodes
-        # data is a list of trees or tokens that correspond to the
-        # symbol's rule expansion
-        return self.callbacks[node.rule](data)
-
-    def _call_ambig_func(self, node, data):
-        # called when transforming a symbol node
-        # data is a list of trees where each tree's data is
-        # equal to the name of the symbol or one of its aliases.
-        if len(data) > 1:
-            return self.tree_class('_ambig', data)
-        elif data:
-            return data[0]
-        return Discard
-
-    def transform_symbol_node(self, node, data):
-        if id(node) not in self._successful_visits:
-            return Discard
-        r = self._check_cycle(node)
-        if r is Discard:
-            return r
-        self._successful_visits.remove(id(node))
-        data = self._collapse_ambig(data)
-        return self._call_ambig_func(node, data)
-
-    def transform_intermediate_node(self, node, data):
-        if id(node) not in self._successful_visits:
-            return Discard
-        r = self._check_cycle(node)
-        if r is Discard:
-            return r
-        self._successful_visits.remove(id(node))
-        if len(data) > 1:
-            children = [self.tree_class('_inter', c) for c in data]
-            return self.tree_class('_iambig', children)
-        return data[0]
-
-    def transform_packed_node(self, node, data):
-        r = self._check_cycle(node)
-        if r is Discard:
-            return r
-        if self.resolve_ambiguity and id(node.parent) in self._successful_visits:
-            return Discard
-        if self._use_cache and id(node) in self._cache:
-            return self._cache[id(node)]
-        children = []
-        assert len(data) <= 2
-        data = PackedData(node, data)
-        if data.left is not PackedData.NO_DATA:
-            if node.left.is_intermediate and isinstance(data.left, list):
-                children += data.left
-            else:
-                children.append(data.left)
-        if data.right is not PackedData.NO_DATA:
-            children.append(data.right)
-        if node.parent.is_intermediate:
-            return self._cache.setdefault(id(node), children)
-        return self._cache.setdefault(id(node), self._call_rule_func(node, children))
-
-    def visit_symbol_node_in(self, node):
-        super(ForestToParseTree, self).visit_symbol_node_in(node)
-        if self._on_cycle_retreat:
-            return
-        return node.children
-
-    def visit_packed_node_in(self, node):
-        self._on_cycle_retreat = False
-        to_visit = super(ForestToParseTree, self).visit_packed_node_in(node)
-        if not self.resolve_ambiguity or id(node.parent) not in self._successful_visits:
-            if not self._use_cache or id(node) not in self._cache:
-                return to_visit
-
-    def visit_packed_node_out(self, node):
-        super(ForestToParseTree, self).visit_packed_node_out(node)
-        if not self._on_cycle_retreat:
-            self._successful_visits.add(id(node.parent))
-
-def handles_ambiguity(func):
-    """Decorator for methods of subclasses of ``TreeForestTransformer``.
-    Denotes that the method should receive a list of transformed derivations."""
-    func.handles_ambiguity = True
-    return func
-
-class TreeForestTransformer(ForestToParseTree):
-    """A ``ForestTransformer`` with a tree ``Transformer``-like interface.
-    By default, it will construct a tree.
-
-    Methods provided via inheritance are called based on the rule/symbol
-    names of nodes in the forest.
-
-    Methods that act on rules will receive a list of the results of the
-    transformations of the rule's children. By default, trees and tokens.
-
-    Methods that act on tokens will receive a token.
-
-    Alternatively, methods that act on rules may be annotated with
-    ``handles_ambiguity``. In this case, the function will receive a list
-    of all the transformations of all the derivations of the rule.
-    By default, a list of trees where each tree.data is equal to the
-    rule name or one of its aliases.
-
-    Non-tree transformations are made possible by override of
-    ``__default__``, ``__default_token__``, and ``__default_ambig__``.
-
-    Note:
-        Tree shaping features such as inlined rules and token filtering are
-        not built into the transformation. Positions are also not propagated.
-
-    Parameters:
-        tree_class: The tree class to use for construction
-        prioritizer: A ``ForestVisitor`` that manipulates the priorities of nodes in the SPPF.
-        resolve_ambiguity: If True, ambiguities will be resolved based on priorities.
-        use_cache (bool): If True, caches the results of some transformations,
-                          potentially improving performance when ``resolve_ambiguity==False``.
-                          Only use if you know what you are doing: i.e. All transformation
-                          functions are pure and referentially transparent.
-    """
-
-    def __init__(self, tree_class=Tree, prioritizer=ForestSumVisitor(), resolve_ambiguity=True, use_cache=False):
-        super(TreeForestTransformer, self).__init__(tree_class, dict(), prioritizer, resolve_ambiguity, use_cache)
-
-    def __default__(self, name, data):
-        """Default operation on tree (for override).
-
-        Returns a tree with name with data as children.
-        """
-        return self.tree_class(name, data)
-
-    def __default_ambig__(self, name, data):
-        """Default operation on ambiguous rule (for override).
-
-        Wraps data in an '_ambig_' node if it contains more than
-        one element.
-        """
-        if len(data) > 1:
-            return self.tree_class('_ambig', data)
-        elif data:
-            return data[0]
-        return Discard
-
-    def __default_token__(self, node):
-        """Default operation on ``Token`` (for override).
-
-        Returns ``node``.
-        """
-        return node
-
-    def transform_token_node(self, node):
-        return getattr(self, node.type, self.__default_token__)(node)
-
-    def _call_rule_func(self, node, data):
-        name = node.rule.alias or node.rule.options.template_source or node.rule.origin.name
-        user_func = getattr(self, name, self.__default__)
-        if user_func == self.__default__ or hasattr(user_func, 'handles_ambiguity'):
-            user_func = partial(self.__default__, name)
-        if not self.resolve_ambiguity:
-            wrapper = partial(AmbiguousIntermediateExpander, self.tree_class)
-            user_func = wrapper(user_func)
-        return user_func(data)
-
-    def _call_ambig_func(self, node, data):
-        name = node.s.name
-        user_func = getattr(self, name, self.__default_ambig__)
-        if user_func == self.__default_ambig__ or not hasattr(user_func, 'handles_ambiguity'):
-            user_func = partial(self.__default_ambig__, name)
-        return user_func(data)
-
-class ForestToPyDotVisitor(ForestVisitor):
-    """
-    A Forest visitor which writes the SPPF to a PNG.
-
-    The SPPF can get really large, really quickly because
-    of the amount of meta-data it stores, so this is probably
-    only useful for trivial trees and learning how the SPPF
-    is structured.
-    """
-    def __init__(self, rankdir="TB"):
-        super(ForestToPyDotVisitor, self).__init__(single_visit=True)
-        self.pydot = import_module('pydot')
-        self.graph = self.pydot.Dot(graph_type='digraph', rankdir=rankdir)
-
-    def visit(self, root, filename):
-        super(ForestToPyDotVisitor, self).visit(root)
-        try:
-            self.graph.write_png(filename)
-        except FileNotFoundError as e:
-            logger.error("Could not write png: ", e)
-
-    def visit_token_node(self, node):
-        graph_node_id = str(id(node))
-        graph_node_label = "\"{}\"".format(node.value.replace('"', '\\"'))
-        graph_node_color = 0x808080
-        graph_node_style = "\"filled,rounded\""
-        graph_node_shape = "diamond"
-        graph_node = self.pydot.Node(graph_node_id, style=graph_node_style, fillcolor="#{:06x}".format(graph_node_color), shape=graph_node_shape, label=graph_node_label)
-        self.graph.add_node(graph_node)
-
-    def visit_packed_node_in(self, node):
-        graph_node_id = str(id(node))
-        graph_node_label = repr(node)
-        graph_node_color = 0x808080
-        graph_node_style = "filled"
-        graph_node_shape = "diamond"
-        graph_node = self.pydot.Node(graph_node_id, style=graph_node_style, fillcolor="#{:06x}".format(graph_node_color), shape=graph_node_shape, label=graph_node_label)
-        self.graph.add_node(graph_node)
-        yield node.left
-        yield node.right
-
-    def visit_packed_node_out(self, node):
-        graph_node_id = str(id(node))
-        graph_node = self.graph.get_node(graph_node_id)[0]
-        for child in [node.left, node.right]:
-            if child is not None:
-                child_graph_node_id = str(id(child.token if isinstance(child, TokenNode) else child))
-                child_graph_node = self.graph.get_node(child_graph_node_id)[0]
-                self.graph.add_edge(self.pydot.Edge(graph_node, child_graph_node))
-            else:
-                #### Try and be above the Python object ID range; probably impl. specific, but maybe this is okay.
-                child_graph_node_id = str(randint(100000000000000000000000000000,123456789012345678901234567890))
-                child_graph_node_style = "invis"
-                child_graph_node = self.pydot.Node(child_graph_node_id, style=child_graph_node_style, label="None")
-                child_edge_style = "invis"
-                self.graph.add_node(child_graph_node)
-                self.graph.add_edge(self.pydot.Edge(graph_node, child_graph_node, style=child_edge_style))
-
-    def visit_symbol_node_in(self, node):
-        graph_node_id = str(id(node))
-        graph_node_label = repr(node)
-        graph_node_color = 0x808080
-        graph_node_style = "\"filled\""
-        if node.is_intermediate:
-            graph_node_shape = "ellipse"
-        else:
-            graph_node_shape = "rectangle"
-        graph_node = self.pydot.Node(graph_node_id, style=graph_node_style, fillcolor="#{:06x}".format(graph_node_color), shape=graph_node_shape, label=graph_node_label)
-        self.graph.add_node(graph_node)
-        return iter(node.children)
-
-    def visit_symbol_node_out(self, node):
-        graph_node_id = str(id(node))
-        graph_node = self.graph.get_node(graph_node_id)[0]
-        for child in node.children:
-            child_graph_node_id = str(id(child))
-            child_graph_node = self.graph.get_node(child_graph_node_id)[0]
-            self.graph.add_edge(self.pydot.Edge(graph_node, child_graph_node))
diff --git a/src/poetry/core/_vendor/lark/parsers/grammar_analysis.py b/src/poetry/core/_vendor/lark/parsers/grammar_analysis.py
deleted file mode 100644
index b52e50d..0000000
--- a/src/poetry/core/_vendor/lark/parsers/grammar_analysis.py
+++ /dev/null
@@ -1,203 +0,0 @@
-"Provides for superficial grammar analysis."
-
-from collections import Counter, defaultdict
-from typing import List, Dict, Iterator, FrozenSet, Set
-
-from ..utils import bfs, fzset, classify
-from ..exceptions import GrammarError
-from ..grammar import Rule, Terminal, NonTerminal, Symbol
-from ..common import ParserConf
-
-
-class RulePtr:
-    __slots__ = ('rule', 'index')
-    rule: Rule
-    index: int
-
-    def __init__(self, rule: Rule, index: int):
-        assert isinstance(rule, Rule)
-        assert index <= len(rule.expansion)
-        self.rule = rule
-        self.index = index
-
-    def __repr__(self):
-        before = [x.name for x in self.rule.expansion[:self.index]]
-        after = [x.name for x in self.rule.expansion[self.index:]]
-        return '<%s : %s * %s>' % (self.rule.origin.name, ' '.join(before), ' '.join(after))
-
-    @property
-    def next(self) -> Symbol:
-        return self.rule.expansion[self.index]
-
-    def advance(self, sym: Symbol) -> 'RulePtr':
-        assert self.next == sym
-        return RulePtr(self.rule, self.index+1)
-
-    @property
-    def is_satisfied(self) -> bool:
-        return self.index == len(self.rule.expansion)
-
-    def __eq__(self, other) -> bool:
-        if not isinstance(other, RulePtr):
-            return NotImplemented
-        return self.rule == other.rule and self.index == other.index
-
-    def __hash__(self) -> int:
-        return hash((self.rule, self.index))
-
-
-State = FrozenSet[RulePtr]
-
-# state generation ensures no duplicate LR0ItemSets
-class LR0ItemSet:
-    __slots__ = ('kernel', 'closure', 'transitions', 'lookaheads')
-
-    kernel: State
-    closure: State
-    transitions: Dict[Symbol, 'LR0ItemSet']
-    lookaheads: Dict[Symbol, Set[Rule]]
-
-    def __init__(self, kernel, closure):
-        self.kernel = fzset(kernel)
-        self.closure = fzset(closure)
-        self.transitions = {}
-        self.lookaheads = defaultdict(set)
-
-    def __repr__(self):
-        return '{%s | %s}' % (', '.join([repr(r) for r in self.kernel]), ', '.join([repr(r) for r in self.closure]))
-
-
-def update_set(set1, set2):
-    if not set2 or set1 > set2:
-        return False
-
-    copy = set(set1)
-    set1 |= set2
-    return set1 != copy
-
-def calculate_sets(rules):
-    """Calculate FOLLOW sets.
-
-    Adapted from: http://lara.epfl.ch/w/cc09:algorithm_for_first_and_follow_sets"""
-    symbols = {sym for rule in rules for sym in rule.expansion} | {rule.origin for rule in rules}
-
-    # foreach grammar rule X ::= Y(1) ... Y(k)
-    # if k=0 or {Y(1),...,Y(k)} subset of NULLABLE then
-    #   NULLABLE = NULLABLE union {X}
-    # for i = 1 to k
-    #   if i=1 or {Y(1),...,Y(i-1)} subset of NULLABLE then
-    #     FIRST(X) = FIRST(X) union FIRST(Y(i))
-    #   for j = i+1 to k
-    #     if i=k or {Y(i+1),...Y(k)} subset of NULLABLE then
-    #       FOLLOW(Y(i)) = FOLLOW(Y(i)) union FOLLOW(X)
-    #     if i+1=j or {Y(i+1),...,Y(j-1)} subset of NULLABLE then
-    #       FOLLOW(Y(i)) = FOLLOW(Y(i)) union FIRST(Y(j))
-    # until none of NULLABLE,FIRST,FOLLOW changed in last iteration
-
-    NULLABLE = set()
-    FIRST = {}
-    FOLLOW = {}
-    for sym in symbols:
-        FIRST[sym]={sym} if sym.is_term else set()
-        FOLLOW[sym]=set()
-
-    # Calculate NULLABLE and FIRST
-    changed = True
-    while changed:
-        changed = False
-
-        for rule in rules:
-            if set(rule.expansion) <= NULLABLE:
-                if update_set(NULLABLE, {rule.origin}):
-                    changed = True
-
-            for i, sym in enumerate(rule.expansion):
-                if set(rule.expansion[:i]) <= NULLABLE:
-                    if update_set(FIRST[rule.origin], FIRST[sym]):
-                        changed = True
-                else:
-                    break
-
-    # Calculate FOLLOW
-    changed = True
-    while changed:
-        changed = False
-
-        for rule in rules:
-            for i, sym in enumerate(rule.expansion):
-                if i==len(rule.expansion)-1 or set(rule.expansion[i+1:]) <= NULLABLE:
-                    if update_set(FOLLOW[sym], FOLLOW[rule.origin]):
-                        changed = True
-
-                for j in range(i+1, len(rule.expansion)):
-                    if set(rule.expansion[i+1:j]) <= NULLABLE:
-                        if update_set(FOLLOW[sym], FIRST[rule.expansion[j]]):
-                            changed = True
-
-    return FIRST, FOLLOW, NULLABLE
-
-
-class GrammarAnalyzer:
-    def __init__(self, parser_conf: ParserConf, debug: bool=False, strict: bool=False):
-        self.debug = debug
-        self.strict = strict
-
-        root_rules = {start: Rule(NonTerminal('$root_' + start), [NonTerminal(start), Terminal('$END')])
-                      for start in parser_conf.start}
-
-        rules = parser_conf.rules + list(root_rules.values())
-        self.rules_by_origin: Dict[NonTerminal, List[Rule]] = classify(rules, lambda r: r.origin)
-
-        if len(rules) != len(set(rules)):
-            duplicates = [item for item, count in Counter(rules).items() if count > 1]
-            raise GrammarError("Rules defined twice: %s" % ', '.join(str(i) for i in duplicates))
-
-        for r in rules:
-            for sym in r.expansion:
-                if not (sym.is_term or sym in self.rules_by_origin):
-                    raise GrammarError("Using an undefined rule: %s" % sym)
-
-        self.start_states = {start: self.expand_rule(root_rule.origin)
-                             for start, root_rule in root_rules.items()}
-
-        self.end_states = {start: fzset({RulePtr(root_rule, len(root_rule.expansion))})
-                           for start, root_rule in root_rules.items()}
-
-        lr0_root_rules = {start: Rule(NonTerminal('$root_' + start), [NonTerminal(start)])
-                for start in parser_conf.start}
-
-        lr0_rules = parser_conf.rules + list(lr0_root_rules.values())
-        assert(len(lr0_rules) == len(set(lr0_rules)))
-
-        self.lr0_rules_by_origin = classify(lr0_rules, lambda r: r.origin)
-
-        # cache RulePtr(r, 0) in r (no duplicate RulePtr objects)
-        self.lr0_start_states = {start: LR0ItemSet([RulePtr(root_rule, 0)], self.expand_rule(root_rule.origin, self.lr0_rules_by_origin))
-                for start, root_rule in lr0_root_rules.items()}
-
-        self.FIRST, self.FOLLOW, self.NULLABLE = calculate_sets(rules)
-
-    def expand_rule(self, source_rule: NonTerminal, rules_by_origin=None) -> State:
-        "Returns all init_ptrs accessible by rule (recursive)"
-
-        if rules_by_origin is None:
-            rules_by_origin = self.rules_by_origin
-
-        init_ptrs = set()
-        def _expand_rule(rule: NonTerminal) -> Iterator[NonTerminal]:
-            assert not rule.is_term, rule
-
-            for r in rules_by_origin[rule]:
-                init_ptr = RulePtr(r, 0)
-                init_ptrs.add(init_ptr)
-
-                if r.expansion: # if not empty rule
-                    new_r = init_ptr.next
-                    if not new_r.is_term:
-                        assert isinstance(new_r, NonTerminal)
-                        yield new_r
-
-        for _ in bfs([source_rule], _expand_rule):
-            pass
-
-        return fzset(init_ptrs)
diff --git a/src/poetry/core/_vendor/lark/parsers/lalr_analysis.py b/src/poetry/core/_vendor/lark/parsers/lalr_analysis.py
deleted file mode 100644
index b7b3fdf..0000000
--- a/src/poetry/core/_vendor/lark/parsers/lalr_analysis.py
+++ /dev/null
@@ -1,332 +0,0 @@
-"""This module builds a LALR(1) transition-table for lalr_parser.py
-
-For now, shift/reduce conflicts are automatically resolved as shifts.
-"""
-
-# Author: Erez Shinan (2017)
-# Email : erezshin@gmail.com
-
-from typing import Dict, Set, Iterator, Tuple, List, TypeVar, Generic
-from collections import defaultdict
-
-from ..utils import classify, classify_bool, bfs, fzset, Enumerator, logger
-from ..exceptions import GrammarError
-
-from .grammar_analysis import GrammarAnalyzer, Terminal, LR0ItemSet, RulePtr, State
-from ..grammar import Rule, Symbol
-from ..common import ParserConf
-
-###{standalone
-
-class Action:
-    def __init__(self, name):
-        self.name = name
-    def __str__(self):
-        return self.name
-    def __repr__(self):
-        return str(self)
-
-Shift = Action('Shift')
-Reduce = Action('Reduce')
-
-StateT = TypeVar("StateT")
-
-class ParseTableBase(Generic[StateT]):
-    states: Dict[StateT, Dict[str, Tuple]]
-    start_states: Dict[str, StateT]
-    end_states: Dict[str, StateT]
-
-    def __init__(self, states, start_states, end_states):
-        self.states = states
-        self.start_states = start_states
-        self.end_states = end_states
-
-    def serialize(self, memo):
-        tokens = Enumerator()
-
-        states = {
-            state: {tokens.get(token): ((1, arg.serialize(memo)) if action is Reduce else (0, arg))
-                    for token, (action, arg) in actions.items()}
-            for state, actions in self.states.items()
-        }
-
-        return {
-            'tokens': tokens.reversed(),
-            'states': states,
-            'start_states': self.start_states,
-            'end_states': self.end_states,
-        }
-
-    @classmethod
-    def deserialize(cls, data, memo):
-        tokens = data['tokens']
-        states = {
-            state: {tokens[token]: ((Reduce, Rule.deserialize(arg, memo)) if action==1 else (Shift, arg))
-                    for token, (action, arg) in actions.items()}
-            for state, actions in data['states'].items()
-        }
-        return cls(states, data['start_states'], data['end_states'])
-
-class ParseTable(ParseTableBase['State']):
-    """Parse-table whose key is State, i.e. set[RulePtr]
-
-    Slower than IntParseTable, but useful for debugging
-    """
-    pass
-
-
-class IntParseTable(ParseTableBase[int]):
-    """Parse-table whose key is int. Best for performance."""
-
-    @classmethod
-    def from_ParseTable(cls, parse_table: ParseTable):
-        enum = list(parse_table.states)
-        state_to_idx: Dict['State', int] = {s:i for i,s in enumerate(enum)}
-        int_states = {}
-
-        for s, la in parse_table.states.items():
-            la = {k:(v[0], state_to_idx[v[1]]) if v[0] is Shift else v
-                  for k,v in la.items()}
-            int_states[ state_to_idx[s] ] = la
-
-
-        start_states = {start:state_to_idx[s] for start, s in parse_table.start_states.items()}
-        end_states = {start:state_to_idx[s] for start, s in parse_table.end_states.items()}
-        return cls(int_states, start_states, end_states)
-
-###}
-
-
-# digraph and traverse, see The Theory and Practice of Compiler Writing
-
-# computes F(x) = G(x) union (union { G(y) | x R y })
-# X: nodes
-# R: relation (function mapping node -> list of nodes that satisfy the relation)
-# G: set valued function
-def digraph(X, R, G):
-    F = {}
-    S = []
-    N = dict.fromkeys(X, 0)
-    for x in X:
-        # this is always true for the first iteration, but N[x] may be updated in traverse below
-        if N[x] == 0:
-            traverse(x, S, N, X, R, G, F)
-    return F
-
-# x: single node
-# S: stack
-# N: weights
-# X: nodes
-# R: relation (see above)
-# G: set valued function
-# F: set valued function we are computing (map of input -> output)
-def traverse(x, S, N, X, R, G, F):
-    S.append(x)
-    d = len(S)
-    N[x] = d
-    F[x] = G[x]
-    for y in R[x]:
-        if N[y] == 0:
-            traverse(y, S, N, X, R, G, F)
-        n_x = N[x]
-        assert(n_x > 0)
-        n_y = N[y]
-        assert(n_y != 0)
-        if (n_y > 0) and (n_y < n_x):
-            N[x] = n_y
-        F[x].update(F[y])
-    if N[x] == d:
-        f_x = F[x]
-        while True:
-            z = S.pop()
-            N[z] = -1
-            F[z] = f_x
-            if z == x:
-                break
-
-
-class LALR_Analyzer(GrammarAnalyzer):
-    lr0_itemsets: Set[LR0ItemSet]
-    nonterminal_transitions: List[Tuple[LR0ItemSet, Symbol]]
-    lookback: Dict[Tuple[LR0ItemSet, Symbol], Set[Tuple[LR0ItemSet, Rule]]]
-    includes: Dict[Tuple[LR0ItemSet, Symbol], Set[Tuple[LR0ItemSet, Symbol]]]
-    reads: Dict[Tuple[LR0ItemSet, Symbol], Set[Tuple[LR0ItemSet, Symbol]]]
-    directly_reads: Dict[Tuple[LR0ItemSet, Symbol], Set[Symbol]]
-
-
-    def __init__(self, parser_conf: ParserConf, debug: bool=False, strict: bool=False):
-        GrammarAnalyzer.__init__(self, parser_conf, debug, strict)
-        self.nonterminal_transitions = []
-        self.directly_reads = defaultdict(set)
-        self.reads = defaultdict(set)
-        self.includes = defaultdict(set)
-        self.lookback = defaultdict(set)
-
-
-    def compute_lr0_states(self) -> None:
-        self.lr0_itemsets = set()
-        # map of kernels to LR0ItemSets
-        cache: Dict['State', LR0ItemSet] = {}
-
-        def step(state: LR0ItemSet) -> Iterator[LR0ItemSet]:
-            _, unsat = classify_bool(state.closure, lambda rp: rp.is_satisfied)
-
-            d = classify(unsat, lambda rp: rp.next)
-            for sym, rps in d.items():
-                kernel = fzset({rp.advance(sym) for rp in rps})
-                new_state = cache.get(kernel, None)
-                if new_state is None:
-                    closure = set(kernel)
-                    for rp in kernel:
-                        if not rp.is_satisfied and not rp.next.is_term:
-                            closure |= self.expand_rule(rp.next, self.lr0_rules_by_origin)
-                    new_state = LR0ItemSet(kernel, closure)
-                    cache[kernel] = new_state
-
-                state.transitions[sym] = new_state
-                yield new_state
-
-            self.lr0_itemsets.add(state)
-
-        for _ in bfs(self.lr0_start_states.values(), step):
-            pass
-
-    def compute_reads_relations(self):
-        # handle start state
-        for root in self.lr0_start_states.values():
-            assert(len(root.kernel) == 1)
-            for rp in root.kernel:
-                assert(rp.index == 0)
-                self.directly_reads[(root, rp.next)] = set([ Terminal('$END') ])
-
-        for state in self.lr0_itemsets:
-            seen = set()
-            for rp in state.closure:
-                if rp.is_satisfied:
-                    continue
-                s = rp.next
-                # if s is a not a nonterminal
-                if s not in self.lr0_rules_by_origin:
-                    continue
-                if s in seen:
-                    continue
-                seen.add(s)
-                nt = (state, s)
-                self.nonterminal_transitions.append(nt)
-                dr = self.directly_reads[nt]
-                r = self.reads[nt]
-                next_state = state.transitions[s]
-                for rp2 in next_state.closure:
-                    if rp2.is_satisfied:
-                        continue
-                    s2 = rp2.next
-                    # if s2 is a terminal
-                    if s2 not in self.lr0_rules_by_origin:
-                        dr.add(s2)
-                    if s2 in self.NULLABLE:
-                        r.add((next_state, s2))
-
-    def compute_includes_lookback(self):
-        for nt in self.nonterminal_transitions:
-            state, nonterminal = nt
-            includes = []
-            lookback = self.lookback[nt]
-            for rp in state.closure:
-                if rp.rule.origin != nonterminal:
-                    continue
-                # traverse the states for rp(.rule)
-                state2 = state
-                for i in range(rp.index, len(rp.rule.expansion)):
-                    s = rp.rule.expansion[i]
-                    nt2 = (state2, s)
-                    state2 = state2.transitions[s]
-                    if nt2 not in self.reads:
-                        continue
-                    for j in range(i + 1, len(rp.rule.expansion)):
-                        if rp.rule.expansion[j] not in self.NULLABLE:
-                            break
-                    else:
-                        includes.append(nt2)
-                # state2 is at the final state for rp.rule
-                if rp.index == 0:
-                    for rp2 in state2.closure:
-                        if (rp2.rule == rp.rule) and rp2.is_satisfied:
-                            lookback.add((state2, rp2.rule))
-            for nt2 in includes:
-                self.includes[nt2].add(nt)
-
-    def compute_lookaheads(self):
-        read_sets = digraph(self.nonterminal_transitions, self.reads, self.directly_reads)
-        follow_sets = digraph(self.nonterminal_transitions, self.includes, read_sets)
-
-        for nt, lookbacks in self.lookback.items():
-            for state, rule in lookbacks:
-                for s in follow_sets[nt]:
-                    state.lookaheads[s].add(rule)
-
-    def compute_lalr1_states(self) -> None:
-        m: Dict[LR0ItemSet, Dict[str, Tuple]] = {}
-        reduce_reduce = []
-        for itemset in self.lr0_itemsets:
-            actions: Dict[Symbol, Tuple] = {la: (Shift, next_state.closure)
-                                                      for la, next_state in itemset.transitions.items()}
-            for la, rules in itemset.lookaheads.items():
-                if len(rules) > 1:
-                    # Try to resolve conflict based on priority
-                    p = [(r.options.priority or 0, r) for r in rules]
-                    p.sort(key=lambda r: r[0], reverse=True)
-                    best, second_best = p[:2]
-                    if best[0] > second_best[0]:
-                        rules = {best[1]}
-                    else:
-                        reduce_reduce.append((itemset, la, rules))
-                        continue
-
-                rule ,= rules
-                if la in actions:
-                    if self.strict:
-                        raise GrammarError(f"Shift/Reduce conflict for terminal {la.name}. [strict-mode]\n ")
-                    elif self.debug:
-                        logger.warning('Shift/Reduce conflict for terminal %s: (resolving as shift)', la.name)
-                        logger.warning(' * %s', rule)
-                    else:
-                        logger.debug('Shift/Reduce conflict for terminal %s: (resolving as shift)', la.name)
-                        logger.debug(' * %s', rule)
-                else:
-                    actions[la] = (Reduce, rule)
-            m[itemset] = { k.name: v for k, v in actions.items() }
-
-        if reduce_reduce:
-            msgs = []
-            for itemset, la, rules in reduce_reduce:
-                msg = 'Reduce/Reduce collision in %s between the following rules: %s' % (la, ''.join([ '\n\t- ' + str(r) for r in rules ]))
-                if self.debug:
-                    msg += '\n    collision occurred in state: {%s\n    }' % ''.join(['\n\t' + str(x) for x in itemset.closure])
-                msgs.append(msg)
-            raise GrammarError('\n\n'.join(msgs))
-
-        states = { k.closure: v for k, v in m.items() }
-
-        # compute end states
-        end_states: Dict[str, 'State'] = {}
-        for state in states:
-            for rp in state:
-                for start in self.lr0_start_states:
-                    if rp.rule.origin.name == ('$root_' + start) and rp.is_satisfied:
-                        assert start not in end_states
-                        end_states[start] = state
-
-        start_states = { start: state.closure for start, state in self.lr0_start_states.items() }
-        _parse_table = ParseTable(states, start_states, end_states)
-
-        if self.debug:
-            self.parse_table = _parse_table
-        else:
-            self.parse_table = IntParseTable.from_ParseTable(_parse_table)
-
-    def compute_lalr(self):
-        self.compute_lr0_states()
-        self.compute_reads_relations()
-        self.compute_includes_lookback()
-        self.compute_lookaheads()
-        self.compute_lalr1_states()
diff --git a/src/poetry/core/_vendor/lark/parsers/lalr_interactive_parser.py b/src/poetry/core/_vendor/lark/parsers/lalr_interactive_parser.py
deleted file mode 100644
index d5a2152..0000000
--- a/src/poetry/core/_vendor/lark/parsers/lalr_interactive_parser.py
+++ /dev/null
@@ -1,157 +0,0 @@
-# This module provides a LALR interactive parser, which is used for debugging and error handling
-
-from typing import Iterator, List
-from copy import copy
-import warnings
-
-from lark.exceptions import UnexpectedToken
-from lark.lexer import Token, LexerThread
-
-###{standalone
-
-class InteractiveParser:
-    """InteractiveParser gives you advanced control over parsing and error handling when parsing with LALR.
-
-    For a simpler interface, see the ``on_error`` argument to ``Lark.parse()``.
-    """
-    def __init__(self, parser, parser_state, lexer_thread: LexerThread):
-        self.parser = parser
-        self.parser_state = parser_state
-        self.lexer_thread = lexer_thread
-        self.result = None
-
-    @property
-    def lexer_state(self) -> LexerThread:
-        warnings.warn("lexer_state will be removed in subsequent releases. Use lexer_thread instead.", DeprecationWarning)
-        return self.lexer_thread
-
-    def feed_token(self, token: Token):
-        """Feed the parser with a token, and advance it to the next state, as if it received it from the lexer.
-
-        Note that ``token`` has to be an instance of ``Token``.
-        """
-        return self.parser_state.feed_token(token, token.type == '$END')
-
-    def iter_parse(self) -> Iterator[Token]:
-        """Step through the different stages of the parse, by reading tokens from the lexer
-        and feeding them to the parser, one per iteration.
-
-        Returns an iterator of the tokens it encounters.
-
-        When the parse is over, the resulting tree can be found in ``InteractiveParser.result``.
-        """
-        for token in self.lexer_thread.lex(self.parser_state):
-            yield token
-            self.result = self.feed_token(token)
-
-    def exhaust_lexer(self) -> List[Token]:
-        """Try to feed the rest of the lexer state into the interactive parser.
-
-        Note that this modifies the instance in place and does not feed an '$END' Token
-        """
-        return list(self.iter_parse())
-
-
-    def feed_eof(self, last_token=None):
-        """Feed a '$END' Token. Borrows from 'last_token' if given."""
-        eof = Token.new_borrow_pos('$END', '', last_token) if last_token is not None else self.lexer_thread._Token('$END', '', 0, 1, 1)
-        return self.feed_token(eof)
-
-
-    def __copy__(self):
-        """Create a new interactive parser with a separate state.
-
-        Calls to feed_token() won't affect the old instance, and vice-versa.
-        """
-        return type(self)(
-            self.parser,
-            copy(self.parser_state),
-            copy(self.lexer_thread),
-        )
-
-    def copy(self):
-        return copy(self)
-
-    def __eq__(self, other):
-        if not isinstance(other, InteractiveParser):
-            return False
-
-        return self.parser_state == other.parser_state and self.lexer_thread == other.lexer_thread
-
-    def as_immutable(self):
-        """Convert to an ``ImmutableInteractiveParser``."""
-        p = copy(self)
-        return ImmutableInteractiveParser(p.parser, p.parser_state, p.lexer_thread)
-
-    def pretty(self):
-        """Print the output of ``choices()`` in a way that's easier to read."""
-        out = ["Parser choices:"]
-        for k, v in self.choices().items():
-            out.append('\t- %s -> %r' % (k, v))
-        out.append('stack size: %s' % len(self.parser_state.state_stack))
-        return '\n'.join(out)
-
-    def choices(self):
-        """Returns a dictionary of token types, matched to their action in the parser.
-
-        Only returns token types that are accepted by the current state.
-
-        Updated by ``feed_token()``.
-        """
-        return self.parser_state.parse_conf.parse_table.states[self.parser_state.position]
-
-    def accepts(self):
-        """Returns the set of possible tokens that will advance the parser into a new valid state."""
-        accepts = set()
-        conf_no_callbacks = copy(self.parser_state.parse_conf)
-        # We don't want to call callbacks here since those might have arbitrary side effects
-        # and are unnecessarily slow.
-        conf_no_callbacks.callbacks = {}
-        for t in self.choices():
-            if t.isupper(): # is terminal?
-                new_cursor = copy(self)
-                new_cursor.parser_state.parse_conf = conf_no_callbacks
-                try:
-                    new_cursor.feed_token(self.lexer_thread._Token(t, ''))
-                except UnexpectedToken:
-                    pass
-                else:
-                    accepts.add(t)
-        return accepts
-
-    def resume_parse(self):
-        """Resume automated parsing from the current state.
-        """
-        return self.parser.parse_from_state(self.parser_state, last_token=self.lexer_thread.state.last_token)
-
-
-
-class ImmutableInteractiveParser(InteractiveParser):
-    """Same as ``InteractiveParser``, but operations create a new instance instead
-    of changing it in-place.
-    """
-
-    result = None
-
-    def __hash__(self):
-        return hash((self.parser_state, self.lexer_thread))
-
-    def feed_token(self, token):
-        c = copy(self)
-        c.result = InteractiveParser.feed_token(c, token)
-        return c
-
-    def exhaust_lexer(self):
-        """Try to feed the rest of the lexer state into the parser.
-
-        Note that this returns a new ImmutableInteractiveParser and does not feed an '$END' Token"""
-        cursor = self.as_mutable()
-        cursor.exhaust_lexer()
-        return cursor.as_immutable()
-
-    def as_mutable(self):
-        """Convert to an ``InteractiveParser``."""
-        p = copy(self)
-        return InteractiveParser(p.parser, p.parser_state, p.lexer_thread)
-
-###}
diff --git a/src/poetry/core/_vendor/lark/parsers/lalr_parser.py b/src/poetry/core/_vendor/lark/parsers/lalr_parser.py
deleted file mode 100644
index 6ae2a04..0000000
--- a/src/poetry/core/_vendor/lark/parsers/lalr_parser.py
+++ /dev/null
@@ -1,122 +0,0 @@
-"""This module implements a LALR(1) Parser
-"""
-# Author: Erez Shinan (2017)
-# Email : erezshin@gmail.com
-from typing import Dict, Any, Optional
-from ..lexer import Token, LexerThread
-from ..utils import Serialize
-from ..common import ParserConf, ParserCallbacks
-
-from .lalr_analysis import LALR_Analyzer, IntParseTable, ParseTableBase
-from .lalr_interactive_parser import InteractiveParser
-from lark.exceptions import UnexpectedCharacters, UnexpectedInput, UnexpectedToken
-from .lalr_parser_state import ParserState, ParseConf
-
-###{standalone
-
-class LALR_Parser(Serialize):
-    def __init__(self, parser_conf: ParserConf, debug: bool=False, strict: bool=False):
-        analysis = LALR_Analyzer(parser_conf, debug=debug, strict=strict)
-        analysis.compute_lalr()
-        callbacks = parser_conf.callbacks
-
-        self._parse_table = analysis.parse_table
-        self.parser_conf = parser_conf
-        self.parser = _Parser(analysis.parse_table, callbacks, debug)
-
-    @classmethod
-    def deserialize(cls, data, memo, callbacks, debug=False):
-        inst = cls.__new__(cls)
-        inst._parse_table = IntParseTable.deserialize(data, memo)
-        inst.parser = _Parser(inst._parse_table, callbacks, debug)
-        return inst
-
-    def serialize(self, memo: Any = None) -> Dict[str, Any]:
-        return self._parse_table.serialize(memo)
-
-    def parse_interactive(self, lexer: LexerThread, start: str):
-        return self.parser.parse(lexer, start, start_interactive=True)
-
-    def parse(self, lexer, start, on_error=None):
-        try:
-            return self.parser.parse(lexer, start)
-        except UnexpectedInput as e:
-            if on_error is None:
-                raise
-
-            while True:
-                if isinstance(e, UnexpectedCharacters):
-                    s = e.interactive_parser.lexer_thread.state
-                    p = s.line_ctr.char_pos
-
-                if not on_error(e):
-                    raise e
-
-                if isinstance(e, UnexpectedCharacters):
-                    # If user didn't change the character position, then we should
-                    if p == s.line_ctr.char_pos:
-                        s.line_ctr.feed(s.text[p:p+1])
-
-                try:
-                    return e.interactive_parser.resume_parse()
-                except UnexpectedToken as e2:
-                    if (isinstance(e, UnexpectedToken)
-                        and e.token.type == e2.token.type == '$END'
-                        and e.interactive_parser == e2.interactive_parser):
-                        # Prevent infinite loop
-                        raise e2
-                    e = e2
-                except UnexpectedCharacters as e2:
-                    e = e2
-
-
-class _Parser:
-    parse_table: ParseTableBase
-    callbacks: ParserCallbacks
-    debug: bool
-
-    def __init__(self, parse_table: ParseTableBase, callbacks: ParserCallbacks, debug: bool=False):
-        self.parse_table = parse_table
-        self.callbacks = callbacks
-        self.debug = debug
-
-    def parse(self, lexer: LexerThread, start: str, value_stack=None, state_stack=None, start_interactive=False):
-        parse_conf = ParseConf(self.parse_table, self.callbacks, start)
-        parser_state = ParserState(parse_conf, lexer, state_stack, value_stack)
-        if start_interactive:
-            return InteractiveParser(self, parser_state, parser_state.lexer)
-        return self.parse_from_state(parser_state)
-
-
-    def parse_from_state(self, state: ParserState, last_token: Optional[Token]=None):
-        """Run the main LALR parser loop
-
-        Parameters:
-            state - the initial state. Changed in-place.
-            last_token - Used only for line information in case of an empty lexer.
-        """
-        try:
-            token = last_token
-            for token in state.lexer.lex(state):
-                assert token is not None
-                state.feed_token(token)
-
-            end_token = Token.new_borrow_pos('$END', '', token) if token else Token('$END', '', 0, 1, 1)
-            return state.feed_token(end_token, True)
-        except UnexpectedInput as e:
-            try:
-                e.interactive_parser = InteractiveParser(self, state, state.lexer)
-            except NameError:
-                pass
-            raise e
-        except Exception as e:
-            if self.debug:
-                print("")
-                print("STATE STACK DUMP")
-                print("----------------")
-                for i, s in enumerate(state.state_stack):
-                    print('%d)' % i , s)
-                print("")
-
-            raise
-###}
diff --git a/src/poetry/core/_vendor/lark/parsers/lalr_parser_state.py b/src/poetry/core/_vendor/lark/parsers/lalr_parser_state.py
deleted file mode 100644
index 3505697..0000000
--- a/src/poetry/core/_vendor/lark/parsers/lalr_parser_state.py
+++ /dev/null
@@ -1,110 +0,0 @@
-from copy import deepcopy, copy
-from typing import Dict, Any, Generic, List
-from ..lexer import Token, LexerThread
-from ..common import ParserCallbacks
-
-from .lalr_analysis import Shift, ParseTableBase, StateT
-from lark.exceptions import UnexpectedToken
-
-###{standalone
-
-class ParseConf(Generic[StateT]):
-    __slots__ = 'parse_table', 'callbacks', 'start', 'start_state', 'end_state', 'states'
-
-    parse_table: ParseTableBase[StateT]
-    callbacks: ParserCallbacks
-    start: str
-
-    start_state: StateT
-    end_state: StateT
-    states: Dict[StateT, Dict[str, tuple]]
-
-    def __init__(self, parse_table: ParseTableBase[StateT], callbacks: ParserCallbacks, start: str):
-        self.parse_table = parse_table
-
-        self.start_state = self.parse_table.start_states[start]
-        self.end_state = self.parse_table.end_states[start]
-        self.states = self.parse_table.states
-
-        self.callbacks = callbacks
-        self.start = start
-
-class ParserState(Generic[StateT]):
-    __slots__ = 'parse_conf', 'lexer', 'state_stack', 'value_stack'
-
-    parse_conf: ParseConf[StateT]
-    lexer: LexerThread
-    state_stack: List[StateT]
-    value_stack: list
-
-    def __init__(self, parse_conf: ParseConf[StateT], lexer: LexerThread, state_stack=None, value_stack=None):
-        self.parse_conf = parse_conf
-        self.lexer = lexer
-        self.state_stack = state_stack or [self.parse_conf.start_state]
-        self.value_stack = value_stack or []
-
-    @property
-    def position(self) -> StateT:
-        return self.state_stack[-1]
-
-    # Necessary for match_examples() to work
-    def __eq__(self, other) -> bool:
-        if not isinstance(other, ParserState):
-            return NotImplemented
-        return len(self.state_stack) == len(other.state_stack) and self.position == other.position
-
-    def __copy__(self):
-        return type(self)(
-            self.parse_conf,
-            self.lexer, # XXX copy
-            copy(self.state_stack),
-            deepcopy(self.value_stack),
-        )
-
-    def copy(self) -> 'ParserState[StateT]':
-        return copy(self)
-
-    def feed_token(self, token: Token, is_end=False) -> Any:
-        state_stack = self.state_stack
-        value_stack = self.value_stack
-        states = self.parse_conf.states
-        end_state = self.parse_conf.end_state
-        callbacks = self.parse_conf.callbacks
-
-        while True:
-            state = state_stack[-1]
-            try:
-                action, arg = states[state][token.type]
-            except KeyError:
-                expected = {s for s in states[state].keys() if s.isupper()}
-                raise UnexpectedToken(token, expected, state=self, interactive_parser=None)
-
-            assert arg != end_state
-
-            if action is Shift:
-                # shift once and return
-                assert not is_end
-                state_stack.append(arg)
-                value_stack.append(token if token.type not in callbacks else callbacks[token.type](token))
-                return
-            else:
-                # reduce+shift as many times as necessary
-                rule = arg
-                size = len(rule.expansion)
-                if size:
-                    s = value_stack[-size:]
-                    del state_stack[-size:]
-                    del value_stack[-size:]
-                else:
-                    s = []
-
-                value = callbacks[rule](s) if callbacks else s
-
-                _action, new_state = states[state_stack[-1]][rule.origin.name]
-                assert _action is Shift
-                state_stack.append(new_state)
-                value_stack.append(value)
-
-                if is_end and state_stack[-1] == end_state:
-                    return value_stack[-1]
-###}
diff --git a/src/poetry/core/_vendor/lark/parsers/xearley.py b/src/poetry/core/_vendor/lark/parsers/xearley.py
deleted file mode 100644
index d9748df..0000000
--- a/src/poetry/core/_vendor/lark/parsers/xearley.py
+++ /dev/null
@@ -1,165 +0,0 @@
-"""This module implements an Earley parser with a dynamic lexer
-
-The core Earley algorithm used here is based on Elizabeth Scott's implementation, here:
-    https://www.sciencedirect.com/science/article/pii/S1571066108001497
-
-That is probably the best reference for understanding the algorithm here.
-
-The Earley parser outputs an SPPF-tree as per that document. The SPPF tree format
-is better documented here:
-    http://www.bramvandersanden.com/post/2014/06/shared-packed-parse-forest/
-
-Instead of running a lexer beforehand, or using a costy char-by-char method, this parser
-uses regular expressions by necessity, achieving high-performance while maintaining all of
-Earley's power in parsing any CFG.
-"""
-
-from typing import TYPE_CHECKING, Callable, Optional, List, Any
-from collections import defaultdict
-
-from ..tree import Tree
-from ..exceptions import UnexpectedCharacters
-from ..lexer import Token
-from ..grammar import Terminal
-from .earley import Parser as BaseParser
-from .earley_forest import TokenNode
-
-if TYPE_CHECKING:
-    from ..common import LexerConf, ParserConf
-
-class Parser(BaseParser):
-    def __init__(self, lexer_conf: 'LexerConf', parser_conf: 'ParserConf', term_matcher: Callable,
-                 resolve_ambiguity: bool=True, complete_lex: bool=False, debug: bool=False,
-                 tree_class: Optional[Callable[[str, List], Any]]=Tree, ordered_sets: bool=True):
-        BaseParser.__init__(self, lexer_conf, parser_conf, term_matcher, resolve_ambiguity,
-                            debug, tree_class, ordered_sets)
-        self.ignore = [Terminal(t) for t in lexer_conf.ignore]
-        self.complete_lex = complete_lex
-
-    def _parse(self, stream, columns, to_scan, start_symbol=None):
-
-        def scan(i, to_scan):
-            """The core Earley Scanner.
-
-            This is a custom implementation of the scanner that uses the
-            Lark lexer to match tokens. The scan list is built by the
-            Earley predictor, based on the previously completed tokens.
-            This ensures that at each phase of the parse we have a custom
-            lexer context, allowing for more complex ambiguities."""
-
-            node_cache = {}
-
-            # 1) Loop the expectations and ask the lexer to match.
-            # Since regexp is forward looking on the input stream, and we only
-            # want to process tokens when we hit the point in the stream at which
-            # they complete, we push all tokens into a buffer (delayed_matches), to
-            # be held possibly for a later parse step when we reach the point in the
-            # input stream at which they complete.
-            for item in self.Set(to_scan):
-                m = match(item.expect, stream, i)
-                if m:
-                    t = Token(item.expect.name, m.group(0), i, text_line, text_column)
-                    delayed_matches[m.end()].append( (item, i, t) )
-
-                    if self.complete_lex:
-                        s = m.group(0)
-                        for j in range(1, len(s)):
-                            m = match(item.expect, s[:-j])
-                            if m:
-                                t = Token(item.expect.name, m.group(0), i, text_line, text_column)
-                                delayed_matches[i+m.end()].append( (item, i, t) )
-
-                    # XXX The following 3 lines were commented out for causing a bug. See issue #768
-                    # # Remove any items that successfully matched in this pass from the to_scan buffer.
-                    # # This ensures we don't carry over tokens that already matched, if we're ignoring below.
-                    # to_scan.remove(item)
-
-            # 3) Process any ignores. This is typically used for e.g. whitespace.
-            # We carry over any unmatched items from the to_scan buffer to be matched again after
-            # the ignore. This should allow us to use ignored symbols in non-terminals to implement
-            # e.g. mandatory spacing.
-            for x in self.ignore:
-                m = match(x, stream, i)
-                if m:
-                    # Carry over any items still in the scan buffer, to past the end of the ignored items.
-                    delayed_matches[m.end()].extend([(item, i, None) for item in to_scan ])
-
-                    # If we're ignoring up to the end of the file, # carry over the start symbol if it already completed.
-                    delayed_matches[m.end()].extend([(item, i, None) for item in columns[i] if item.is_complete and item.s == start_symbol])
-
-            next_to_scan = self.Set()
-            next_set = self.Set()
-            columns.append(next_set)
-            transitives.append({})
-
-            ## 4) Process Tokens from delayed_matches.
-            # This is the core of the Earley scanner. Create an SPPF node for each Token,
-            # and create the symbol node in the SPPF tree. Advance the item that completed,
-            # and add the resulting new item to either the Earley set (for processing by the
-            # completer/predictor) or the to_scan buffer for the next parse step.
-            for item, start, token in delayed_matches[i+1]:
-                if token is not None:
-                    token.end_line = text_line
-                    token.end_column = text_column + 1
-                    token.end_pos = i + 1
-
-                    new_item = item.advance()
-                    label = (new_item.s, new_item.start, i)
-                    token_node = TokenNode(token, terminals[token.type])
-                    new_item.node = node_cache[label] if label in node_cache else node_cache.setdefault(label, self.SymbolNode(*label))
-                    new_item.node.add_family(new_item.s, item.rule, new_item.start, item.node, token_node)
-                else:
-                    new_item = item
-
-                if new_item.expect in self.TERMINALS:
-                    # add (B ::= Aai+1.B, h, y) to Q'
-                    next_to_scan.add(new_item)
-                else:
-                    # add (B ::= Aa+1.B, h, y) to Ei+1
-                    next_set.add(new_item)
-
-            del delayed_matches[i+1]    # No longer needed, so unburden memory
-
-            if not next_set and not delayed_matches and not next_to_scan:
-                considered_rules = list(sorted(to_scan, key=lambda key: key.rule.origin.name))
-                raise UnexpectedCharacters(stream, i, text_line, text_column, {item.expect.name for item in to_scan},
-                                           set(to_scan), state=frozenset(i.s for i in to_scan),
-                                           considered_rules=considered_rules
-                                           )
-
-            return next_to_scan
-
-
-        delayed_matches = defaultdict(list)
-        match = self.term_matcher
-        terminals = self.lexer_conf.terminals_by_name
-
-        # Cache for nodes & tokens created in a particular parse step.
-        transitives = [{}]
-
-        text_line = 1
-        text_column = 1
-
-        ## The main Earley loop.
-        # Run the Prediction/Completion cycle for any Items in the current Earley set.
-        # Completions will be added to the SPPF tree, and predictions will be recursively
-        # processed down to terminals/empty nodes to be added to the scanner for the next
-        # step.
-        i = 0
-        for token in stream:
-            self.predict_and_complete(i, to_scan, columns, transitives)
-
-            to_scan = scan(i, to_scan)
-
-            if token == '\n':
-                text_line += 1
-                text_column = 1
-            else:
-                text_column += 1
-            i += 1
-
-        self.predict_and_complete(i, to_scan, columns, transitives)
-
-        ## Column is now the final column in the parse.
-        assert i == len(columns)-1
-        return to_scan
diff --git a/src/poetry/core/_vendor/lark/py.typed b/src/poetry/core/_vendor/lark/py.typed
deleted file mode 100644
index e69de29..0000000
diff --git a/src/poetry/core/_vendor/lark/reconstruct.py b/src/poetry/core/_vendor/lark/reconstruct.py
deleted file mode 100644
index 2d8423a..0000000
--- a/src/poetry/core/_vendor/lark/reconstruct.py
+++ /dev/null
@@ -1,107 +0,0 @@
-"""This is an experimental tool for reconstructing text from a shaped tree, based on a Lark grammar.
-"""
-
-from typing import Dict, Callable, Iterable, Optional
-
-from .lark import Lark
-from .tree import Tree, ParseTree
-from .visitors import Transformer_InPlace
-from .lexer import Token, PatternStr, TerminalDef
-from .grammar import Terminal, NonTerminal, Symbol
-
-from .tree_matcher import TreeMatcher, is_discarded_terminal
-from .utils import is_id_continue
-
-def is_iter_empty(i):
-    try:
-        _ = next(i)
-        return False
-    except StopIteration:
-        return True
-
-
-class WriteTokensTransformer(Transformer_InPlace):
-    "Inserts discarded tokens into their correct place, according to the rules of grammar"
-
-    tokens: Dict[str, TerminalDef]
-    term_subs: Dict[str, Callable[[Symbol], str]]
-
-    def __init__(self, tokens: Dict[str, TerminalDef], term_subs: Dict[str, Callable[[Symbol], str]]) -> None:
-        self.tokens = tokens
-        self.term_subs = term_subs
-
-    def __default__(self, data, children, meta):
-        if not getattr(meta, 'match_tree', False):
-            return Tree(data, children)
-
-        iter_args = iter(children)
-        to_write = []
-        for sym in meta.orig_expansion:
-            if is_discarded_terminal(sym):
-                try:
-                    v = self.term_subs[sym.name](sym)
-                except KeyError:
-                    t = self.tokens[sym.name]
-                    if not isinstance(t.pattern, PatternStr):
-                        raise NotImplementedError("Reconstructing regexps not supported yet: %s" % t)
-
-                    v = t.pattern.value
-                to_write.append(v)
-            else:
-                x = next(iter_args)
-                if isinstance(x, list):
-                    to_write += x
-                else:
-                    if isinstance(x, Token):
-                        assert Terminal(x.type) == sym, x
-                    else:
-                        assert NonTerminal(x.data) == sym, (sym, x)
-                    to_write.append(x)
-
-        assert is_iter_empty(iter_args)
-        return to_write
-
-
-class Reconstructor(TreeMatcher):
-    """
-    A Reconstructor that will, given a full parse Tree, generate source code.
-
-    Note:
-        The reconstructor cannot generate values from regexps. If you need to produce discarded
-        regexes, such as newlines, use `term_subs` and provide default values for them.
-
-    Parameters:
-        parser: a Lark instance
-        term_subs: a dictionary of [Terminal name as str] to [output text as str]
-    """
-
-    write_tokens: WriteTokensTransformer
-
-    def __init__(self, parser: Lark, term_subs: Optional[Dict[str, Callable[[Symbol], str]]]=None) -> None:
-        TreeMatcher.__init__(self, parser)
-
-        self.write_tokens = WriteTokensTransformer({t.name:t for t in self.tokens}, term_subs or {})
-
-    def _reconstruct(self, tree):
-        unreduced_tree = self.match_tree(tree, tree.data)
-
-        res = self.write_tokens.transform(unreduced_tree)
-        for item in res:
-            if isinstance(item, Tree):
-                # TODO use orig_expansion.rulename to support templates
-                yield from self._reconstruct(item)
-            else:
-                yield item
-
-    def reconstruct(self, tree: ParseTree, postproc: Optional[Callable[[Iterable[str]], Iterable[str]]]=None, insert_spaces: bool=True) -> str:
-        x = self._reconstruct(tree)
-        if postproc:
-            x = postproc(x)
-        y = []
-        prev_item = ''
-        for item in x:
-            if insert_spaces and prev_item and item and is_id_continue(prev_item[-1]) and is_id_continue(item[0]):
-                y.append(' ')
-            y.append(item)
-            prev_item = item
-        return ''.join(y)
diff --git a/src/poetry/core/_vendor/lark/tools/__init__.py b/src/poetry/core/_vendor/lark/tools/__init__.py
deleted file mode 100644
index c6995c6..0000000
--- a/src/poetry/core/_vendor/lark/tools/__init__.py
+++ /dev/null
@@ -1,71 +0,0 @@
-import sys
-from argparse import ArgumentParser, FileType
-from textwrap import indent
-from logging import DEBUG, INFO, WARN, ERROR
-from typing import Optional
-import warnings
-
-from lark import Lark, logger
-try:
-    from interegular import logger as interegular_logger
-    has_interegular = True
-except ImportError:
-    has_interegular = False
-
-lalr_argparser = ArgumentParser(add_help=False, epilog='Look at the Lark documentation for more info on the options')
-
-flags = [
-    ('d', 'debug'),
-    'keep_all_tokens',
-    'regex',
-    'propagate_positions',
-    'maybe_placeholders',
-    'use_bytes'
-]
-
-options = ['start', 'lexer']
-
-lalr_argparser.add_argument('-v', '--verbose', action='count', default=0, help="Increase Logger output level, up to three times")
-lalr_argparser.add_argument('-s', '--start', action='append', default=[])
-lalr_argparser.add_argument('-l', '--lexer', default='contextual', choices=('basic', 'contextual'))
-encoding: Optional[str] = 'utf-8' if sys.version_info > (3, 4) else None
-lalr_argparser.add_argument('-o', '--out', type=FileType('w', encoding=encoding), default=sys.stdout, help='the output file (default=stdout)')
-lalr_argparser.add_argument('grammar_file', type=FileType('r', encoding=encoding), help='A valid .lark file')
-
-for flag in flags:
-    if isinstance(flag, tuple):
-        options.append(flag[1])
-        lalr_argparser.add_argument('-' + flag[0], '--' + flag[1], action='store_true')
-    elif isinstance(flag, str):
-        options.append(flag)
-        lalr_argparser.add_argument('--' + flag, action='store_true')
-    else:
-        raise NotImplementedError("flags must only contain strings or tuples of strings")
-
-
-def build_lalr(namespace):
-    logger.setLevel((ERROR, WARN, INFO, DEBUG)[min(namespace.verbose, 3)])
-    if has_interegular:
-        interegular_logger.setLevel(logger.getEffectiveLevel())
-    if len(namespace.start) == 0:
-        namespace.start.append('start')
-    kwargs = {n: getattr(namespace, n) for n in options}
-    return Lark(namespace.grammar_file, parser='lalr', **kwargs), namespace.out
-
-
-def showwarning_as_comment(message, category, filename, lineno, file=None, line=None):
-    # Based on warnings._showwarnmsg_impl
-    text = warnings.formatwarning(message, category, filename, lineno, line)
-    text = indent(text, '# ')
-    if file is None:
-        file = sys.stderr
-        if file is None:
-            return
-    try:
-        file.write(text)
-    except OSError:
-        pass
-
-
-def make_warnings_comments():
-    warnings.showwarning = showwarning_as_comment
diff --git a/src/poetry/core/_vendor/lark/tools/nearley.py b/src/poetry/core/_vendor/lark/tools/nearley.py
deleted file mode 100644
index 1fc27d5..0000000
--- a/src/poetry/core/_vendor/lark/tools/nearley.py
+++ /dev/null
@@ -1,202 +0,0 @@
-"Converts Nearley grammars to Lark"
-
-import os.path
-import sys
-import codecs
-import argparse
-
-
-from lark import Lark, Transformer, v_args
-
-nearley_grammar = r"""
-    start: (ruledef|directive)+
-
-    directive: "@" NAME (STRING|NAME)
-             | "@" JS  -> js_code
-    ruledef: NAME "->" expansions
-           | NAME REGEXP "->" expansions -> macro
-    expansions: expansion ("|" expansion)*
-
-    expansion: expr+ js
-
-    ?expr: item (":" /[+*?]/)?
-
-    ?item: rule|string|regexp|null
-         | "(" expansions ")"
-
-    rule: NAME
-    string: STRING
-    regexp: REGEXP
-    null: "null"
-    JS: /{%.*?%}/s
-    js: JS?
-
-    NAME: /[a-zA-Z_$]\w*/
-    COMMENT: /#[^\n]*/
-    REGEXP: /\[.*?\]/
-
-    STRING: _STRING "i"?
-
-    %import common.ESCAPED_STRING -> _STRING
-    %import common.WS
-    %ignore WS
-    %ignore COMMENT
-
-    """
-
-nearley_grammar_parser = Lark(nearley_grammar, parser='earley', lexer='basic')
-
-def _get_rulename(name):
-    name = {'_': '_ws_maybe', '__': '_ws'}.get(name, name)
-    return 'n_' + name.replace('$', '__DOLLAR__').lower()
-
-@v_args(inline=True)
-class NearleyToLark(Transformer):
-    def __init__(self):
-        self._count = 0
-        self.extra_rules = {}
-        self.extra_rules_rev = {}
-        self.alias_js_code = {}
-
-    def _new_function(self, code):
-        name = 'alias_%d' % self._count
-        self._count += 1
-
-        self.alias_js_code[name] = code
-        return name
-
-    def _extra_rule(self, rule):
-        if rule in self.extra_rules_rev:
-            return self.extra_rules_rev[rule]
-
-        name = 'xrule_%d' % len(self.extra_rules)
-        assert name not in self.extra_rules
-        self.extra_rules[name] = rule
-        self.extra_rules_rev[rule] = name
-        return name
-
-    def rule(self, name):
-        return _get_rulename(name)
-
-    def ruledef(self, name, exps):
-        return '!%s: %s' % (_get_rulename(name), exps)
-
-    def expr(self, item, op):
-        rule = '(%s)%s' % (item, op)
-        return self._extra_rule(rule)
-
-    def regexp(self, r):
-        return '/%s/' % r
-
-    def null(self):
-        return ''
-
-    def string(self, s):
-        return self._extra_rule(s)
-
-    def expansion(self, *x):
-        x, js = x[:-1], x[-1]
-        if js.children:
-            js_code ,= js.children
-            js_code = js_code[2:-2]
-            alias = '-> ' + self._new_function(js_code)
-        else:
-            alias = ''
-        return ' '.join(x) + alias
-
-    def expansions(self, *x):
-        return '%s' % ('\n    |'.join(x))
-
-    def start(self, *rules):
-        return '\n'.join(filter(None, rules))
-
-def _nearley_to_lark(g, builtin_path, n2l, js_code, folder_path, includes):
-    rule_defs = []
-
-    tree = nearley_grammar_parser.parse(g)
-    for statement in tree.children:
-        if statement.data == 'directive':
-            directive, arg = statement.children
-            if directive in ('builtin', 'include'):
-                folder = builtin_path if directive == 'builtin' else folder_path
-                path = os.path.join(folder, arg[1:-1])
-                if path not in includes:
-                    includes.add(path)
-                    with codecs.open(path, encoding='utf8') as f:
-                        text = f.read()
-                    rule_defs += _nearley_to_lark(text, builtin_path, n2l, js_code, os.path.abspath(os.path.dirname(path)), includes)
-            else:
-                assert False, directive
-        elif statement.data == 'js_code':
-            code ,= statement.children
-            code = code[2:-2]
-            js_code.append(code)
-        elif statement.data == 'macro':
-            pass    # TODO Add support for macros!
-        elif statement.data == 'ruledef':
-            rule_defs.append(n2l.transform(statement))
-        else:
-            raise Exception("Unknown statement: %s" % statement)
-
-    return rule_defs
-
-
-def create_code_for_nearley_grammar(g, start, builtin_path, folder_path, es6=False):
-    import js2py
-
-    emit_code = []
-    def emit(x=None):
-        if x:
-            emit_code.append(x)
-        emit_code.append('\n')
-
-    js_code = ['function id(x) {return x[0];}']
-    n2l = NearleyToLark()
-    rule_defs = _nearley_to_lark(g, builtin_path, n2l, js_code, folder_path, set())
-    lark_g = '\n'.join(rule_defs)
-    lark_g += '\n'+'\n'.join('!%s: %s' % item for item in n2l.extra_rules.items())
-
-    emit('from lark import Lark, Transformer')
-    emit()
-    emit('grammar = ' + repr(lark_g))
-    emit()
-
-    for alias, code in n2l.alias_js_code.items():
-        js_code.append('%s = (%s);' % (alias, code))
-
-    if es6:
-        emit(js2py.translate_js6('\n'.join(js_code)))
-    else:
-        emit(js2py.translate_js('\n'.join(js_code)))
-    emit('class TransformNearley(Transformer):')
-    for alias in n2l.alias_js_code:
-        emit("    %s = var.get('%s').to_python()" % (alias, alias))
-    emit("    __default__ = lambda self, n, c, m: c if c else None")
-
-    emit()
-    emit('parser = Lark(grammar, start="n_%s", maybe_placeholders=False)' % start)
-    emit('def parse(text):')
-    emit('    return TransformNearley().transform(parser.parse(text))')
-
-    return ''.join(emit_code)
-
-def main(fn, start, nearley_lib, es6=False):
-    with codecs.open(fn, encoding='utf8') as f:
-        grammar = f.read()
-    return create_code_for_nearley_grammar(grammar, start, os.path.join(nearley_lib, 'builtin'), os.path.abspath(os.path.dirname(fn)), es6=es6)
-
-def get_arg_parser():
-    parser = argparse.ArgumentParser(description='Reads a Nearley grammar (with js functions), and outputs an equivalent lark parser.')
-    parser.add_argument('nearley_grammar', help='Path to the file containing the nearley grammar')
-    parser.add_argument('start_rule', help='Rule within the nearley grammar to make the base rule')
-    parser.add_argument('nearley_lib', help='Path to root directory of nearley codebase (used for including builtins)')
-    parser.add_argument('--es6', help='Enable experimental ES6 support', action='store_true')
-    return parser
-
-if __name__ == '__main__':
-    parser = get_arg_parser()
-    if len(sys.argv) == 1:
-        parser.print_help(sys.stderr)
-        sys.exit(1)
-    args = parser.parse_args()
-    print(main(fn=args.nearley_grammar, start=args.start_rule, nearley_lib=args.nearley_lib, es6=args.es6))
diff --git a/src/poetry/core/_vendor/lark/tools/serialize.py b/src/poetry/core/_vendor/lark/tools/serialize.py
deleted file mode 100644
index eb28824..0000000
--- a/src/poetry/core/_vendor/lark/tools/serialize.py
+++ /dev/null
@@ -1,32 +0,0 @@
-import sys
-import json
-
-from lark.grammar import Rule
-from lark.lexer import TerminalDef
-from lark.tools import lalr_argparser, build_lalr
-
-import argparse
-
-argparser = argparse.ArgumentParser(prog='python -m lark.tools.serialize', parents=[lalr_argparser],
-                                    description="Lark Serialization Tool - Stores Lark's internal state & LALR analysis as a JSON file",
-                                    epilog='Look at the Lark documentation for more info on the options')
-
-
-def serialize(lark_inst, outfile):
-    data, memo = lark_inst.memo_serialize([TerminalDef, Rule])
-    outfile.write('{\n')
-    outfile.write('  "data": %s,\n' % json.dumps(data))
-    outfile.write('  "memo": %s\n' % json.dumps(memo))
-    outfile.write('}\n')
-
-
-def main():
-    if len(sys.argv)==1:
-        argparser.print_help(sys.stderr)
-        sys.exit(1)
-    ns = argparser.parse_args()
-    serialize(*build_lalr(ns))
-
-
-if __name__ == '__main__':
-    main()
diff --git a/src/poetry/core/_vendor/lark/tools/standalone.py b/src/poetry/core/_vendor/lark/tools/standalone.py
deleted file mode 100644
index 9940ccb..0000000
--- a/src/poetry/core/_vendor/lark/tools/standalone.py
+++ /dev/null
@@ -1,196 +0,0 @@
-###{standalone
-#
-#
-#   Lark Stand-alone Generator Tool
-# ----------------------------------
-# Generates a stand-alone LALR(1) parser
-#
-# Git:    https://github.com/erezsh/lark
-# Author: Erez Shinan (erezshin@gmail.com)
-#
-#
-#    >>> LICENSE
-#
-#    This tool and its generated code use a separate license from Lark,
-#    and are subject to the terms of the Mozilla Public License, v. 2.0.
-#    If a copy of the MPL was not distributed with this
-#    file, You can obtain one at https://mozilla.org/MPL/2.0/.
-#
-#    If you wish to purchase a commercial license for this tool and its
-#    generated code, you may contact me via email or otherwise.
-#
-#    If MPL2 is incompatible with your free or open-source project,
-#    contact me and we'll work it out.
-#
-#
-
-from copy import deepcopy
-from abc import ABC, abstractmethod
-from types import ModuleType
-from typing import (
-    TypeVar, Generic, Type, Tuple, List, Dict, Iterator, Collection, Callable, Optional, FrozenSet, Any,
-    Union, Iterable, IO, TYPE_CHECKING, overload, Sequence,
-    Pattern as REPattern, ClassVar, Set, Mapping
-)
-###}
-
-import sys
-import token, tokenize
-import os
-from os import path
-from collections import defaultdict
-from functools import partial
-from argparse import ArgumentParser
-
-import lark
-from lark.tools import lalr_argparser, build_lalr, make_warnings_comments
-
-
-from lark.grammar import Rule
-from lark.lexer import TerminalDef
-
-_dir = path.dirname(__file__)
-_larkdir = path.join(_dir, path.pardir)
-
-
-EXTRACT_STANDALONE_FILES = [
-    'tools/standalone.py',
-    'exceptions.py',
-    'utils.py',
-    'tree.py',
-    'visitors.py',
-    'grammar.py',
-    'lexer.py',
-    'common.py',
-    'parse_tree_builder.py',
-    'parsers/lalr_analysis.py',
-    'parsers/lalr_parser_state.py',
-    'parsers/lalr_parser.py',
-    'parsers/lalr_interactive_parser.py',
-    'parser_frontends.py',
-    'lark.py',
-    'indenter.py',
-]
-
-def extract_sections(lines):
-    section = None
-    text = []
-    sections = defaultdict(list)
-    for line in lines:
-        if line.startswith('###'):
-            if line[3] == '{':
-                section = line[4:].strip()
-            elif line[3] == '}':
-                sections[section] += text
-                section = None
-                text = []
-            else:
-                raise ValueError(line)
-        elif section:
-            text.append(line)
-
-    return {name: ''.join(text) for name, text in sections.items()}
-
-
-def strip_docstrings(line_gen):
-    """ Strip comments and docstrings from a file.
-    Based on code from: https://stackoverflow.com/questions/1769332/script-to-remove-python-comments-docstrings
-    """
-    res = []
-
-    prev_toktype = token.INDENT
-    last_lineno = -1
-    last_col = 0
-
-    tokgen = tokenize.generate_tokens(line_gen)
-    for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokgen:
-        if slineno > last_lineno:
-            last_col = 0
-        if scol > last_col:
-            res.append(" " * (scol - last_col))
-        if toktype == token.STRING and prev_toktype == token.INDENT:
-            # Docstring
-            res.append("#--")
-        elif toktype == tokenize.COMMENT:
-            # Comment
-            res.append("##\n")
-        else:
-            res.append(ttext)
-        prev_toktype = toktype
-        last_col = ecol
-        last_lineno = elineno
-
-    return ''.join(res)
-
-
-def gen_standalone(lark_inst, output=None, out=sys.stdout, compress=False):
-    if output is None:
-        output = partial(print, file=out)
-
-    import pickle, zlib, base64
-    def compressed_output(obj):
-        s = pickle.dumps(obj, pickle.HIGHEST_PROTOCOL)
-        c = zlib.compress(s)
-        output(repr(base64.b64encode(c)))
-
-    def output_decompress(name):
-        output('%(name)s = pickle.loads(zlib.decompress(base64.b64decode(%(name)s)))' % locals())
-
-    output('# The file was automatically generated by Lark v%s' % lark.__version__)
-    output('__version__ = "%s"' % lark.__version__)
-    output()
-
-    for i, pyfile in enumerate(EXTRACT_STANDALONE_FILES):
-        with open(os.path.join(_larkdir, pyfile)) as f:
-            code = extract_sections(f)['standalone']
-            if i:   # if not this file
-                code = strip_docstrings(partial(next, iter(code.splitlines(True))))
-            output(code)
-
-    data, m = lark_inst.memo_serialize([TerminalDef, Rule])
-    output('import pickle, zlib, base64')
-    if compress:
-        output('DATA = (')
-        compressed_output(data)
-        output(')')
-        output_decompress('DATA')
-        output('MEMO = (')
-        compressed_output(m)
-        output(')')
-        output_decompress('MEMO')
-    else:
-        output('DATA = (')
-        output(data)
-        output(')')
-        output('MEMO = (')
-        output(m)
-        output(')')
-
-
-    output('Shift = 0')
-    output('Reduce = 1')
-    output("def Lark_StandAlone(**kwargs):")
-    output("  return Lark._load_from_dict(DATA, MEMO, **kwargs)")
-
-
-
-
-def main():
-    make_warnings_comments()
-    parser = ArgumentParser(prog="prog='python -m lark.tools.standalone'", description="Lark Stand-alone Generator Tool",
-                            parents=[lalr_argparser], epilog='Look at the Lark documentation for more info on the options')
-    parser.add_argument('-c', '--compress', action='store_true', default=0, help="Enable compression")
-    if len(sys.argv) == 1:
-        parser.print_help(sys.stderr)
-        sys.exit(1)
-    ns = parser.parse_args()
-
-    lark_inst, out = build_lalr(ns)
-    gen_standalone(lark_inst, out=out, compress=ns.compress)
-
-    ns.out.close()
-    ns.grammar_file.close()
-
-
-if __name__ == '__main__':
-    main()
diff --git a/src/poetry/core/_vendor/lark/tree.py b/src/poetry/core/_vendor/lark/tree.py
deleted file mode 100644
index 438837e..0000000
--- a/src/poetry/core/_vendor/lark/tree.py
+++ /dev/null
@@ -1,272 +0,0 @@
-import sys
-from copy import deepcopy
-
-from typing import List, Callable, Iterator, Union, Optional, Generic, TypeVar, TYPE_CHECKING
-
-if TYPE_CHECKING:
-    from .lexer import TerminalDef, Token
-    try:
-        import rich
-    except ImportError:
-        pass
-    if sys.version_info >= (3, 8):
-        from typing import Literal
-    else:
-        from typing_extensions import Literal
-
-###{standalone
-from collections import OrderedDict
-
-class Meta:
-
-    empty: bool
-    line: int
-    column: int
-    start_pos: int
-    end_line: int
-    end_column: int
-    end_pos: int
-    orig_expansion: 'List[TerminalDef]'
-    match_tree: bool
-
-    def __init__(self):
-        self.empty = True
-
-
-_Leaf_T = TypeVar("_Leaf_T")
-Branch = Union[_Leaf_T, 'Tree[_Leaf_T]']
-
-
-class Tree(Generic[_Leaf_T]):
-    """The main tree class.
-
-    Creates a new tree, and stores "data" and "children" in attributes of the same name.
-    Trees can be hashed and compared.
-
-    Parameters:
-        data: The name of the rule or alias
-        children: List of matched sub-rules and terminals
-        meta: Line & Column numbers (if ``propagate_positions`` is enabled).
-            meta attributes: (line, column, end_line, end_column, start_pos, end_pos,
-                              container_line, container_column, container_end_line, container_end_column)
-            container_* attributes consider all symbols, including those that have been inlined in the tree.
-            For example, in the rule 'a: _A B _C', the regular attributes will mark the start and end of B,
-            but the container_* attributes will also include _A and _C in the range. However, rules that
-            contain 'a' will consider it in full, including _A and _C for all attributes.
-    """
-
-    data: str
-    children: 'List[Branch[_Leaf_T]]'
-
-    def __init__(self, data: str, children: 'List[Branch[_Leaf_T]]', meta: Optional[Meta]=None) -> None:
-        self.data = data
-        self.children = children
-        self._meta = meta
-
-    @property
-    def meta(self) -> Meta:
-        if self._meta is None:
-            self._meta = Meta()
-        return self._meta
-
-    def __repr__(self):
-        return 'Tree(%r, %r)' % (self.data, self.children)
-
-    def _pretty_label(self):
-        return self.data
-
-    def _pretty(self, level, indent_str):
-        yield f'{indent_str*level}{self._pretty_label()}'
-        if len(self.children) == 1 and not isinstance(self.children[0], Tree):
-            yield f'\t{self.children[0]}\n'
-        else:
-            yield '\n'
-            for n in self.children:
-                if isinstance(n, Tree):
-                    yield from n._pretty(level+1, indent_str)
-                else:
-                    yield f'{indent_str*(level+1)}{n}\n'
-
-    def pretty(self, indent_str: str='  ') -> str:
-        """Returns an indented string representation of the tree.
-
-        Great for debugging.
-        """
-        return ''.join(self._pretty(0, indent_str))
-
-    def __rich__(self, parent:Optional['rich.tree.Tree']=None) -> 'rich.tree.Tree':
-        """Returns a tree widget for the 'rich' library.
-
-        Example:
-            ::
-                from rich import print
-                from lark import Tree
-
-                tree = Tree('root', ['node1', 'node2'])
-                print(tree)
-        """
-        return self._rich(parent)
-
-    def _rich(self, parent):
-        if parent:
-            tree = parent.add(f'[bold]{self.data}[/bold]')
-        else:
-            import rich.tree
-            tree = rich.tree.Tree(self.data)
-
-        for c in self.children:
-            if isinstance(c, Tree):
-                c._rich(tree)
-            else:
-                tree.add(f'[green]{c}[/green]')
-
-        return tree
-
-    def __eq__(self, other):
-        try:
-            return self.data == other.data and self.children == other.children
-        except AttributeError:
-            return False
-
-    def __ne__(self, other):
-        return not (self == other)
-
-    def __hash__(self) -> int:
-        return hash((self.data, tuple(self.children)))
-
-    def iter_subtrees(self) -> 'Iterator[Tree[_Leaf_T]]':
-        """Depth-first iteration.
-
-        Iterates over all the subtrees, never returning to the same node twice (Lark's parse-tree is actually a DAG).
-        """
-        queue = [self]
-        subtrees = OrderedDict()
-        for subtree in queue:
-            subtrees[id(subtree)] = subtree
-            # Reason for type ignore https://github.com/python/mypy/issues/10999
-            queue += [c for c in reversed(subtree.children)  # type: ignore[misc]
-                      if isinstance(c, Tree) and id(c) not in subtrees]
-
-        del queue
-        return reversed(list(subtrees.values()))
-
-    def iter_subtrees_topdown(self):
-        """Breadth-first iteration.
-
-        Iterates over all the subtrees, return nodes in order like pretty() does.
-        """
-        stack = [self]
-        stack_append = stack.append
-        stack_pop = stack.pop
-        while stack:
-            node = stack_pop()
-            if not isinstance(node, Tree):
-                continue
-            yield node
-            for child in reversed(node.children):
-                stack_append(child)
-
-    def find_pred(self, pred: 'Callable[[Tree[_Leaf_T]], bool]') -> 'Iterator[Tree[_Leaf_T]]':
-        """Returns all nodes of the tree that evaluate pred(node) as true."""
-        return filter(pred, self.iter_subtrees())
-
-    def find_data(self, data: str) -> 'Iterator[Tree[_Leaf_T]]':
-        """Returns all nodes of the tree whose data equals the given data."""
-        return self.find_pred(lambda t: t.data == data)
-
-###}
-
-    def expand_kids_by_data(self, *data_values):
-        """Expand (inline) children with any of the given data values. Returns True if anything changed"""
-        changed = False
-        for i in range(len(self.children)-1, -1, -1):
-            child = self.children[i]
-            if isinstance(child, Tree) and child.data in data_values:
-                self.children[i:i+1] = child.children
-                changed = True
-        return changed
-
-
-    def scan_values(self, pred: 'Callable[[Branch[_Leaf_T]], bool]') -> Iterator[_Leaf_T]:
-        """Return all values in the tree that evaluate pred(value) as true.
-
-        This can be used to find all the tokens in the tree.
-
-        Example:
-            >>> all_tokens = tree.scan_values(lambda v: isinstance(v, Token))
-        """
-        for c in self.children:
-            if isinstance(c, Tree):
-                for t in c.scan_values(pred):
-                    yield t
-            else:
-                if pred(c):
-                    yield c
-
-    def __deepcopy__(self, memo):
-        return type(self)(self.data, deepcopy(self.children, memo), meta=self._meta)
-
-    def copy(self) -> 'Tree[_Leaf_T]':
-        return type(self)(self.data, self.children)
-
-    def set(self, data: str, children: 'List[Branch[_Leaf_T]]') -> None:
-        self.data = data
-        self.children = children
-
-
-ParseTree = Tree['Token']
-
-
-class SlottedTree(Tree):
-    __slots__ = 'data', 'children', 'rule', '_meta'
-
-
-def pydot__tree_to_png(tree: Tree, filename: str, rankdir: 'Literal["TB", "LR", "BT", "RL"]'="LR", **kwargs) -> None:
-    graph = pydot__tree_to_graph(tree, rankdir, **kwargs)
-    graph.write_png(filename)
-
-
-def pydot__tree_to_dot(tree: Tree, filename, rankdir="LR", **kwargs):
-    graph = pydot__tree_to_graph(tree, rankdir, **kwargs)
-    graph.write(filename)
-
-
-def pydot__tree_to_graph(tree: Tree, rankdir="LR", **kwargs):
-    """Creates a colorful image that represents the tree (data+children, without meta)
-
-    Possible values for `rankdir` are "TB", "LR", "BT", "RL", corresponding to
-    directed graphs drawn from top to bottom, from left to right, from bottom to
-    top, and from right to left, respectively.
-
-    `kwargs` can be any graph attribute (e. g. `dpi=200`). For a list of
-    possible attributes, see https://www.graphviz.org/doc/info/attrs.html.
-    """
-
-    import pydot  # type: ignore[import]
-    graph = pydot.Dot(graph_type='digraph', rankdir=rankdir, **kwargs)
-
-    i = [0]
-
-    def new_leaf(leaf):
-        node = pydot.Node(i[0], label=repr(leaf))
-        i[0] += 1
-        graph.add_node(node)
-        return node
-
-    def _to_pydot(subtree):
-        color = hash(subtree.data) & 0xffffff
-        color |= 0x808080
-
-        subnodes = [_to_pydot(child) if isinstance(child, Tree) else new_leaf(child)
-                    for child in subtree.children]
-        node = pydot.Node(i[0], style="filled", fillcolor="#%x" % color, label=subtree.data)
-        i[0] += 1
-        graph.add_node(node)
-
-        for subnode in subnodes:
-            graph.add_edge(pydot.Edge(node, subnode))
-
-        return node
-
-    _to_pydot(tree)
-    return graph
diff --git a/src/poetry/core/_vendor/lark/tree_matcher.py b/src/poetry/core/_vendor/lark/tree_matcher.py
deleted file mode 100644
index 0f42652..0000000
--- a/src/poetry/core/_vendor/lark/tree_matcher.py
+++ /dev/null
@@ -1,186 +0,0 @@
-"""Tree matcher based on Lark grammar"""
-
-import re
-from collections import defaultdict
-
-from . import Tree, Token
-from .common import ParserConf
-from .parsers import earley
-from .grammar import Rule, Terminal, NonTerminal
-
-
-def is_discarded_terminal(t):
-    return t.is_term and t.filter_out
-
-
-class _MakeTreeMatch:
-    def __init__(self, name, expansion):
-        self.name = name
-        self.expansion = expansion
-
-    def __call__(self, args):
-        t = Tree(self.name, args)
-        t.meta.match_tree = True
-        t.meta.orig_expansion = self.expansion
-        return t
-
-
-def _best_from_group(seq, group_key, cmp_key):
-    d = {}
-    for item in seq:
-        key = group_key(item)
-        if key in d:
-            v1 = cmp_key(item)
-            v2 = cmp_key(d[key])
-            if v2 > v1:
-                d[key] = item
-        else:
-            d[key] = item
-    return list(d.values())
-
-
-def _best_rules_from_group(rules):
-    rules = _best_from_group(rules, lambda r: r, lambda r: -len(r.expansion))
-    rules.sort(key=lambda r: len(r.expansion))
-    return rules
-
-
-def _match(term, token):
-    if isinstance(token, Tree):
-        name, _args = parse_rulename(term.name)
-        return token.data == name
-    elif isinstance(token, Token):
-        return term == Terminal(token.type)
-    assert False, (term, token)
-
-
-def make_recons_rule(origin, expansion, old_expansion):
-    return Rule(origin, expansion, alias=_MakeTreeMatch(origin.name, old_expansion))
-
-
-def make_recons_rule_to_term(origin, term):
-    return make_recons_rule(origin, [Terminal(term.name)], [term])
-
-
-def parse_rulename(s):
-    "Parse rule names that may contain a template syntax (like rule{a, b, ...})"
-    name, args_str = re.match(r'(\w+)(?:{(.+)})?', s).groups()
-    args = args_str and [a.strip() for a in args_str.split(',')]
-    return name, args
-
-
-
-class ChildrenLexer:
-    def __init__(self, children):
-        self.children = children
-
-    def lex(self, parser_state):
-        return self.children
-
-class TreeMatcher:
-    """Match the elements of a tree node, based on an ontology
-    provided by a Lark grammar.
-
-    Supports templates and inlined rules (`rule{a, b,..}` and `_rule`)
-
-    Initialize with an instance of Lark.
-    """
-
-    def __init__(self, parser):
-        # XXX TODO calling compile twice returns different results!
-        assert not parser.options.maybe_placeholders
-        # XXX TODO: we just ignore the potential existence of a postlexer
-        self.tokens, rules, _extra = parser.grammar.compile(parser.options.start, set())
-
-        self.rules_for_root = defaultdict(list)
-
-        self.rules = list(self._build_recons_rules(rules))
-        self.rules.reverse()
-
-        # Choose the best rule from each group of {rule => [rule.alias]}, since we only really need one derivation.
-        self.rules = _best_rules_from_group(self.rules)
-
-        self.parser = parser
-        self._parser_cache = {}
-
-    def _build_recons_rules(self, rules):
-        "Convert tree-parsing/construction rules to tree-matching rules"
-        expand1s = {r.origin for r in rules if r.options.expand1}
-
-        aliases = defaultdict(list)
-        for r in rules:
-            if r.alias:
-                aliases[r.origin].append(r.alias)
-
-        rule_names = {r.origin for r in rules}
-        nonterminals = {sym for sym in rule_names
-                        if sym.name.startswith('_') or sym in expand1s or sym in aliases}
-
-        seen = set()
-        for r in rules:
-            recons_exp = [sym if sym in nonterminals else Terminal(sym.name)
-                          for sym in r.expansion if not is_discarded_terminal(sym)]
-
-            # Skip self-recursive constructs
-            if recons_exp == [r.origin] and r.alias is None:
-                continue
-
-            sym = NonTerminal(r.alias) if r.alias else r.origin
-            rule = make_recons_rule(sym, recons_exp, r.expansion)
-
-            if sym in expand1s and len(recons_exp) != 1:
-                self.rules_for_root[sym.name].append(rule)
-
-                if sym.name not in seen:
-                    yield make_recons_rule_to_term(sym, sym)
-                    seen.add(sym.name)
-            else:
-                if sym.name.startswith('_') or sym in expand1s:
-                    yield rule
-                else:
-                    self.rules_for_root[sym.name].append(rule)
-
-        for origin, rule_aliases in aliases.items():
-            for alias in rule_aliases:
-                yield make_recons_rule_to_term(origin, NonTerminal(alias))
-            yield make_recons_rule_to_term(origin, origin)
-
-    def match_tree(self, tree, rulename):
-        """Match the elements of `tree` to the symbols of rule `rulename`.
-
-        Parameters:
-            tree (Tree): the tree node to match
-            rulename (str): The expected full rule name (including template args)
-
-        Returns:
-            Tree: an unreduced tree that matches `rulename`
-
-        Raises:
-            UnexpectedToken: If no match was found.
-
-        Note:
-            It's the callers' responsibility match the tree recursively.
-        """
-        if rulename:
-            # validate
-            name, _args = parse_rulename(rulename)
-            assert tree.data == name
-        else:
-            rulename = tree.data
-
-        # TODO: ambiguity?
-        try:
-            parser = self._parser_cache[rulename]
-        except KeyError:
-            rules = self.rules + _best_rules_from_group(self.rules_for_root[rulename])
-
-            # TODO pass callbacks through dict, instead of alias?
-            callbacks = {rule: rule.alias for rule in rules}
-            conf = ParserConf(rules, callbacks, [rulename])
-            parser = earley.Parser(self.parser.lexer_conf, conf, _match, resolve_ambiguity=True)
-            self._parser_cache[rulename] = parser
-
-        # find a full derivation
-        unreduced_tree = parser.parse(ChildrenLexer(tree.children), rulename)
-        assert unreduced_tree.data == rulename
-        return unreduced_tree
diff --git a/src/poetry/core/_vendor/lark/tree_templates.py b/src/poetry/core/_vendor/lark/tree_templates.py
deleted file mode 100644
index 6ec7323..0000000
--- a/src/poetry/core/_vendor/lark/tree_templates.py
+++ /dev/null
@@ -1,180 +0,0 @@
-"""This module defines utilities for matching and translation tree templates.
-
-A tree templates is a tree that contains nodes that are template variables.
-
-"""
-
-from typing import Union, Optional, Mapping, Dict, Tuple, Iterator
-
-from lark import Tree, Transformer
-from lark.exceptions import MissingVariableError
-
-Branch = Union[Tree[str], str]
-TreeOrCode = Union[Tree[str], str]
-MatchResult = Dict[str, Tree]
-_TEMPLATE_MARKER = '$'
-
-
-class TemplateConf:
-    """Template Configuration
-
-    Allows customization for different uses of Template
-
-    parse() must return a Tree instance.
-    """
-
-    def __init__(self, parse=None):
-        self._parse = parse
-
-    def test_var(self, var: Union[Tree[str], str]) -> Optional[str]:
-        """Given a tree node, if it is a template variable return its name. Otherwise, return None.
-
-        This method may be overridden for customization
-
-        Parameters:
-            var: Tree | str - The tree node to test
-
-        """
-        if isinstance(var, str):
-            return _get_template_name(var)
-
-        if (
-            isinstance(var, Tree)
-            and var.data == "var"
-            and len(var.children) > 0
-            and isinstance(var.children[0], str)
-        ):
-            return _get_template_name(var.children[0])
-
-        return None
-
-    def _get_tree(self, template: TreeOrCode) -> Tree[str]:
-        if isinstance(template, str):
-            assert self._parse
-            template = self._parse(template)
-
-        if not isinstance(template, Tree):
-            raise TypeError("template parser must return a Tree instance")
-
-        return template
-
-    def __call__(self, template: Tree[str]) -> 'Template':
-        return Template(template, conf=self)
-
-    def _match_tree_template(self, template: TreeOrCode, tree: Branch) -> Optional[MatchResult]:
-        """Returns dict of {var: match} if found a match, else None
-        """
-        template_var = self.test_var(template)
-        if template_var:
-            if not isinstance(tree, Tree):
-                raise TypeError(f"Template variables can only match Tree instances. Not {tree!r}")
-            return {template_var: tree}
-
-        if isinstance(template, str):
-            if template == tree:
-                return {}
-            return None
-
-        assert isinstance(template, Tree) and isinstance(tree, Tree), f"template={template} tree={tree}"
-
-        if template.data == tree.data and len(template.children) == len(tree.children):
-            res = {}
-            for t1, t2 in zip(template.children, tree.children):
-                matches = self._match_tree_template(t1, t2)
-                if matches is None:
-                    return None
-
-                res.update(matches)
-
-            return res
-
-        return None
-
-
-class _ReplaceVars(Transformer[str, Tree[str]]):
-    def __init__(self, conf: TemplateConf, vars: Mapping[str, Tree[str]]) -> None:
-        super().__init__()
-        self._conf = conf
-        self._vars = vars
-
-    def __default__(self, data, children, meta) -> Tree[str]:
-        tree = super().__default__(data, children, meta)
-
-        var = self._conf.test_var(tree)
-        if var:
-            try:
-                return self._vars[var]
-            except KeyError:
-                raise MissingVariableError(f"No mapping for template variable ({var})")
-        return tree
-
-
-class Template:
-    """Represents a tree template, tied to a specific configuration
-
-    A tree template is a tree that contains nodes that are template variables.
-    Those variables will match any tree.
-    (future versions may support annotations on the variables, to allow more complex templates)
-    """
-
-    def __init__(self, tree: Tree[str], conf: TemplateConf = TemplateConf()):
-        self.conf = conf
-        self.tree = conf._get_tree(tree)
-
-    def match(self, tree: TreeOrCode) -> Optional[MatchResult]:
-        """Match a tree template to a tree.
-
-        A tree template without variables will only match ``tree`` if it is equal to the template.
-
-        Parameters:
-            tree (Tree): The tree to match to the template
-
-        Returns:
-            Optional[Dict[str, Tree]]: If match is found, returns a dictionary mapping
-                template variable names to their matching tree nodes.
-                If no match was found, returns None.
-        """
-        tree = self.conf._get_tree(tree)
-        return self.conf._match_tree_template(self.tree, tree)
-
-    def search(self, tree: TreeOrCode) -> Iterator[Tuple[Tree[str], MatchResult]]:
-        """Search for all occurrences of the tree template inside ``tree``.
-        """
-        tree = self.conf._get_tree(tree)
-        for subtree in tree.iter_subtrees():
-            res = self.match(subtree)
-            if res:
-                yield subtree, res
-
-    def apply_vars(self, vars: Mapping[str, Tree[str]]) -> Tree[str]:
-        """Apply vars to the template tree
-        """
-        return _ReplaceVars(self.conf, vars).transform(self.tree)
-
-
-def translate(t1: Template, t2: Template, tree: TreeOrCode):
-    """Search tree and translate each occurrence of t1 into t2.
-    """
-    tree = t1.conf._get_tree(tree)      # ensure it's a tree, parse if necessary and possible
-    for subtree, vars in t1.search(tree):
-        res = t2.apply_vars(vars)
-        subtree.set(res.data, res.children)
-    return tree
-
-
-class TemplateTranslator:
-    """Utility class for translating a collection of patterns
-    """
-
-    def __init__(self, translations: Mapping[Template, Template]):
-        assert all(isinstance(k, Template) and isinstance(v, Template) for k, v in translations.items())
-        self.translations = translations
-
-    def translate(self, tree: Tree[str]):
-        for k, v in self.translations.items():
-            tree = translate(k, v, tree)
-        return tree
-
-
-def _get_template_name(value: str) -> Optional[str]:
-    return value.lstrip(_TEMPLATE_MARKER) if value.startswith(_TEMPLATE_MARKER) else None
diff --git a/src/poetry/core/_vendor/lark/utils.py b/src/poetry/core/_vendor/lark/utils.py
deleted file mode 100644
index cc6183b..0000000
--- a/src/poetry/core/_vendor/lark/utils.py
+++ /dev/null
@@ -1,358 +0,0 @@
-import unicodedata
-import os
-from itertools import product
-from collections import deque
-from typing import Callable, Iterator, List, Optional, Tuple, Type, TypeVar, Union, Dict, Any, Sequence, Iterable, AbstractSet
-
-###{standalone
-import sys, re
-import logging
-
-logger: logging.Logger = logging.getLogger("lark")
-logger.addHandler(logging.StreamHandler())
-# Set to highest level, since we have some warnings amongst the code
-# By default, we should not output any log messages
-logger.setLevel(logging.CRITICAL)
-
-
-NO_VALUE = object()
-
-T = TypeVar("T")
-
-
-def classify(seq: Iterable, key: Optional[Callable] = None, value: Optional[Callable] = None) -> Dict:
-    d: Dict[Any, Any] = {}
-    for item in seq:
-        k = key(item) if (key is not None) else item
-        v = value(item) if (value is not None) else item
-        try:
-            d[k].append(v)
-        except KeyError:
-            d[k] = [v]
-    return d
-
-
-def _deserialize(data: Any, namespace: Dict[str, Any], memo: Dict) -> Any:
-    if isinstance(data, dict):
-        if '__type__' in data:  # Object
-            class_ = namespace[data['__type__']]
-            return class_.deserialize(data, memo)
-        elif '@' in data:
-            return memo[data['@']]
-        return {key:_deserialize(value, namespace, memo) for key, value in data.items()}
-    elif isinstance(data, list):
-        return [_deserialize(value, namespace, memo) for value in data]
-    return data
-
-
-_T = TypeVar("_T", bound="Serialize")
-
-class Serialize:
-    """Safe-ish serialization interface that doesn't rely on Pickle
-
-    Attributes:
-        __serialize_fields__ (List[str]): Fields (aka attributes) to serialize.
-        __serialize_namespace__ (list): List of classes that deserialization is allowed to instantiate.
-                                        Should include all field types that aren't builtin types.
-    """
-
-    def memo_serialize(self, types_to_memoize: List) -> Any:
-        memo = SerializeMemoizer(types_to_memoize)
-        return self.serialize(memo), memo.serialize()
-
-    def serialize(self, memo = None) -> Dict[str, Any]:
-        if memo and memo.in_types(self):
-            return {'@': memo.memoized.get(self)}
-
-        fields = getattr(self, '__serialize_fields__')
-        res = {f: _serialize(getattr(self, f), memo) for f in fields}
-        res['__type__'] = type(self).__name__
-        if hasattr(self, '_serialize'):
-            self._serialize(res, memo)  # type: ignore[attr-defined]
-        return res
-
-    @classmethod
-    def deserialize(cls: Type[_T], data: Dict[str, Any], memo: Dict[int, Any]) -> _T:
-        namespace = getattr(cls, '__serialize_namespace__', [])
-        namespace = {c.__name__:c for c in namespace}
-
-        fields = getattr(cls, '__serialize_fields__')
-
-        if '@' in data:
-            return memo[data['@']]
-
-        inst = cls.__new__(cls)
-        for f in fields:
-            try:
-                setattr(inst, f, _deserialize(data[f], namespace, memo))
-            except KeyError as e:
-                raise KeyError("Cannot find key for class", cls, e)
-
-        if hasattr(inst, '_deserialize'):
-            inst._deserialize()  # type: ignore[attr-defined]
-
-        return inst
-
-
-class SerializeMemoizer(Serialize):
-    "A version of serialize that memoizes objects to reduce space"
-
-    __serialize_fields__ = 'memoized',
-
-    def __init__(self, types_to_memoize: List) -> None:
-        self.types_to_memoize = tuple(types_to_memoize)
-        self.memoized = Enumerator()
-
-    def in_types(self, value: Serialize) -> bool:
-        return isinstance(value, self.types_to_memoize)
-
-    def serialize(self) -> Dict[int, Any]:  # type: ignore[override]
-        return _serialize(self.memoized.reversed(), None)
-
-    @classmethod
-    def deserialize(cls, data: Dict[int, Any], namespace: Dict[str, Any], memo: Dict[Any, Any]) -> Dict[int, Any]:  # type: ignore[override]
-        return _deserialize(data, namespace, memo)
-
-
-try:
-    import regex
-    _has_regex = True
-except ImportError:
-    _has_regex = False
-
-if sys.version_info >= (3, 11):
-    import re._parser as sre_parse
-    import re._constants as sre_constants
-else:
-    import sre_parse
-    import sre_constants
-
-categ_pattern = re.compile(r'\\p{[A-Za-z_]+}')
-
-def get_regexp_width(expr: str) -> Union[Tuple[int, int], List[int]]:
-    if _has_regex:
-        # Since `sre_parse` cannot deal with Unicode categories of the form `\p{Mn}`, we replace these with
-        # a simple letter, which makes no difference as we are only trying to get the possible lengths of the regex
-        # match here below.
-        regexp_final = re.sub(categ_pattern, 'A', expr)
-    else:
-        if re.search(categ_pattern, expr):
-            raise ImportError('`regex` module must be installed in order to use Unicode categories.', expr)
-        regexp_final = expr
-    try:
-        # Fixed in next version (past 0.960) of typeshed
-        return [int(x) for x in sre_parse.parse(regexp_final).getwidth()]   # type: ignore[attr-defined]
-    except sre_constants.error:
-        if not _has_regex:
-            raise ValueError(expr)
-        else:
-            # sre_parse does not support the new features in regex. To not completely fail in that case,
-            # we manually test for the most important info (whether the empty string is matched)
-            c = regex.compile(regexp_final)
-            if c.match('') is None:
-                # MAXREPEAT is a none pickable subclass of int, therefore needs to be converted to enable caching
-                return 1, int(sre_constants.MAXREPEAT)
-            else:
-                return 0, int(sre_constants.MAXREPEAT)
-
-###}
-
-
-_ID_START =    'Lu', 'Ll', 'Lt', 'Lm', 'Lo', 'Mn', 'Mc', 'Pc'
-_ID_CONTINUE = _ID_START + ('Nd', 'Nl',)
-
-def _test_unicode_category(s: str, categories: Sequence[str]) -> bool:
-    if len(s) != 1:
-        return all(_test_unicode_category(char, categories) for char in s)
-    return s == '_' or unicodedata.category(s) in categories
-
-def is_id_continue(s: str) -> bool:
-    """
-    Checks if all characters in `s` are alphanumeric characters (Unicode standard, so diacritics, indian vowels, non-latin
-    numbers, etc. all pass). Synonymous with a Python `ID_CONTINUE` identifier. See PEP 3131 for details.
-    """
-    return _test_unicode_category(s, _ID_CONTINUE)
-
-def is_id_start(s: str) -> bool:
-    """
-    Checks if all characters in `s` are alphabetic characters (Unicode standard, so diacritics, indian vowels, non-latin
-    numbers, etc. all pass). Synonymous with a Python `ID_START` identifier. See PEP 3131 for details.
-    """
-    return _test_unicode_category(s, _ID_START)
-
-
-def dedup_list(l: Sequence[T]) -> List[T]:
-    """Given a list (l) will removing duplicates from the list,
-       preserving the original order of the list. Assumes that
-       the list entries are hashable."""
-    dedup = set()
-    # This returns None, but that's expected
-    return [x for x in l if not (x in dedup or dedup.add(x))]  # type: ignore[func-returns-value]
-    # 2x faster (ordered in PyPy and CPython 3.6+, guaranteed to be ordered in Python 3.7+)
-    # return list(dict.fromkeys(l))
-
-
-class Enumerator(Serialize):
-    def __init__(self) -> None:
-        self.enums: Dict[Any, int] = {}
-
-    def get(self, item) -> int:
-        if item not in self.enums:
-            self.enums[item] = len(self.enums)
-        return self.enums[item]
-
-    def __len__(self):
-        return len(self.enums)
-
-    def reversed(self) -> Dict[int, Any]:
-        r = {v: k for k, v in self.enums.items()}
-        assert len(r) == len(self.enums)
-        return r
-
-
-
-def combine_alternatives(lists):
-    """
-    Accepts a list of alternatives, and enumerates all their possible concatenations.
-
-    Examples:
-        >>> combine_alternatives([range(2), [4,5]])
-        [[0, 4], [0, 5], [1, 4], [1, 5]]
-
-        >>> combine_alternatives(["abc", "xy", '$'])
-        [['a', 'x', '$'], ['a', 'y', '$'], ['b', 'x', '$'], ['b', 'y', '$'], ['c', 'x', '$'], ['c', 'y', '$']]
-
-        >>> combine_alternatives([])
-        [[]]
-    """
-    if not lists:
-        return [[]]
-    assert all(l for l in lists), lists
-    return list(product(*lists))
-
-try:
-    # atomicwrites doesn't have type bindings
-    import atomicwrites     # type: ignore[import]
-    _has_atomicwrites = True
-except ImportError:
-    _has_atomicwrites = False
-
-class FS:
-    exists = staticmethod(os.path.exists)
-
-    @staticmethod
-    def open(name, mode="r", **kwargs):
-        if _has_atomicwrites and "w" in mode:
-            return atomicwrites.atomic_write(name, mode=mode, overwrite=True, **kwargs)
-        else:
-            return open(name, mode, **kwargs)
-
-
-
-def isascii(s: str) -> bool:
-    """ str.isascii only exists in python3.7+ """
-    if sys.version_info >= (3, 7):
-        return s.isascii()
-    else:
-        try:
-            s.encode('ascii')
-            return True
-        except (UnicodeDecodeError, UnicodeEncodeError):
-            return False
-
-
-class fzset(frozenset):
-    def __repr__(self):
-        return '{%s}' % ', '.join(map(repr, self))
-
-
-def classify_bool(seq: Iterable, pred: Callable) -> Any:
-    false_elems = []
-    true_elems = [elem for elem in seq if pred(elem) or false_elems.append(elem)]  # type: ignore[func-returns-value]
-    return true_elems, false_elems
-
-
-def bfs(initial: Iterable, expand: Callable) -> Iterator:
-    open_q = deque(list(initial))
-    visited = set(open_q)
-    while open_q:
-        node = open_q.popleft()
-        yield node
-        for next_node in expand(node):
-            if next_node not in visited:
-                visited.add(next_node)
-                open_q.append(next_node)
-
-def bfs_all_unique(initial, expand):
-    "bfs, but doesn't keep track of visited (aka seen), because there can be no repetitions"
-    open_q = deque(list(initial))
-    while open_q:
-        node = open_q.popleft()
-        yield node
-        open_q += expand(node)
-
-
-def _serialize(value: Any, memo: Optional[SerializeMemoizer]) -> Any:
-    if isinstance(value, Serialize):
-        return value.serialize(memo)
-    elif isinstance(value, list):
-        return [_serialize(elem, memo) for elem in value]
-    elif isinstance(value, frozenset):
-        return list(value)  # TODO reversible?
-    elif isinstance(value, dict):
-        return {key:_serialize(elem, memo) for key, elem in value.items()}
-    # assert value is None or isinstance(value, (int, float, str, tuple)), value
-    return value
-
-
-
-
-def small_factors(n: int, max_factor: int) -> List[Tuple[int, int]]:
-    """
-    Splits n up into smaller factors and summands <= max_factor.
-    Returns a list of [(a, b), ...]
-    so that the following code returns n:
-
-    n = 1
-    for a, b in values:
-        n = n * a + b
-
-    Currently, we also keep a + b <= max_factor, but that might change
-    """
-    assert n >= 0
-    assert max_factor > 2
-    if n <= max_factor:
-        return [(n, 0)]
-
-    for a in range(max_factor, 1, -1):
-        r, b = divmod(n, a)
-        if a + b <= max_factor:
-            return small_factors(r, max_factor) + [(a, b)]
-    assert False, "Failed to factorize %s" % n
-
-
-class OrderedSet(AbstractSet[T]):
-    """A minimal OrderedSet implementation, using a dictionary.
-
-    (relies on the dictionary being ordered)
-    """
-    def __init__(self, items: Iterable[T] =()):
-        self.d = dict.fromkeys(items)
-
-    def __contains__(self, item: Any) -> bool:
-        return item in self.d
-
-    def add(self, item: T):
-        self.d[item] = None
-
-    def __iter__(self) -> Iterator[T]:
-        return iter(self.d)
-
-    def remove(self, item: T):
-        del self.d[item]
-
-    def __bool__(self):
-        return bool(self.d)
-
-    def __len__(self) -> int:
-        return len(self.d)
diff --git a/src/poetry/core/_vendor/lark/visitors.py b/src/poetry/core/_vendor/lark/visitors.py
deleted file mode 100644
index ae9d128..0000000
--- a/src/poetry/core/_vendor/lark/visitors.py
+++ /dev/null
@@ -1,593 +0,0 @@
-from typing import TypeVar, Tuple, List, Callable, Generic, Type, Union, Optional, Any, cast
-from abc import ABC
-
-from .utils import combine_alternatives
-from .tree import Tree, Branch
-from .exceptions import VisitError, GrammarError
-from .lexer import Token
-
-###{standalone
-from functools import wraps, update_wrapper
-from inspect import getmembers, getmro
-
-_Return_T = TypeVar('_Return_T')
-_Return_V = TypeVar('_Return_V')
-_Leaf_T = TypeVar('_Leaf_T')
-_Leaf_U = TypeVar('_Leaf_U')
-_R = TypeVar('_R')
-_FUNC = Callable[..., _Return_T]
-_DECORATED = Union[_FUNC, type]
-
-class _DiscardType:
-    """When the Discard value is returned from a transformer callback,
-    that node is discarded and won't appear in the parent.
-
-    Note:
-        This feature is disabled when the transformer is provided to Lark
-        using the ``transformer`` keyword (aka Tree-less LALR mode).
-
-    Example:
-        ::
-
-            class T(Transformer):
-                def ignore_tree(self, children):
-                    return Discard
-
-                def IGNORE_TOKEN(self, token):
-                    return Discard
-    """
-
-    def __repr__(self):
-        return "lark.visitors.Discard"
-
-Discard = _DiscardType()
-
-# Transformers
-
-class _Decoratable:
-    "Provides support for decorating methods with @v_args"
-
-    @classmethod
-    def _apply_v_args(cls, visit_wrapper):
-        mro = getmro(cls)
-        assert mro[0] is cls
-        libmembers = {name for _cls in mro[1:] for name, _ in getmembers(_cls)}
-        for name, value in getmembers(cls):
-
-            # Make sure the function isn't inherited (unless it's overwritten)
-            if name.startswith('_') or (name in libmembers and name not in cls.__dict__):
-                continue
-            if not callable(value):
-                continue
-
-            # Skip if v_args already applied (at the function level)
-            if isinstance(cls.__dict__[name], _VArgsWrapper):
-                continue
-
-            setattr(cls, name, _VArgsWrapper(cls.__dict__[name], visit_wrapper))
-        return cls
-
-    def __class_getitem__(cls, _):
-        return cls
-
-
-class Transformer(_Decoratable, ABC, Generic[_Leaf_T, _Return_T]):
-    """Transformers work bottom-up (or depth-first), starting with visiting the leaves and working
-    their way up until ending at the root of the tree.
-
-    For each node visited, the transformer will call the appropriate method (callbacks), according to the
-    node's ``data``, and use the returned value to replace the node, thereby creating a new tree structure.
-
-    Transformers can be used to implement map & reduce patterns. Because nodes are reduced from leaf to root,
-    at any point the callbacks may assume the children have already been transformed (if applicable).
-
-    If the transformer cannot find a method with the right name, it will instead call ``__default__``, which by
-    default creates a copy of the node.
-
-    To discard a node, return Discard (``lark.visitors.Discard``).
-
-    ``Transformer`` can do anything ``Visitor`` can do, but because it reconstructs the tree,
-    it is slightly less efficient.
-
-    A transformer without methods essentially performs a non-memoized partial deepcopy.
-
-    All these classes implement the transformer interface:
-
-    - ``Transformer`` - Recursively transforms the tree. This is the one you probably want.
-    - ``Transformer_InPlace`` - Non-recursive. Changes the tree in-place instead of returning new instances
-    - ``Transformer_InPlaceRecursive`` - Recursive. Changes the tree in-place instead of returning new instances
-
-    Parameters:
-        visit_tokens (bool, optional): Should the transformer visit tokens in addition to rules.
-                                       Setting this to ``False`` is slightly faster. Defaults to ``True``.
-                                       (For processing ignored tokens, use the ``lexer_callbacks`` options)
-
-    """
-    __visit_tokens__ = True   # For backwards compatibility
-
-    def __init__(self,  visit_tokens: bool=True) -> None:
-        self.__visit_tokens__ = visit_tokens
-
-    def _call_userfunc(self, tree, new_children=None):
-        # Assumes tree is already transformed
-        children = new_children if new_children is not None else tree.children
-        try:
-            f = getattr(self, tree.data)
-        except AttributeError:
-            return self.__default__(tree.data, children, tree.meta)
-        else:
-            try:
-                wrapper = getattr(f, 'visit_wrapper', None)
-                if wrapper is not None:
-                    return f.visit_wrapper(f, tree.data, children, tree.meta)
-                else:
-                    return f(children)
-            except GrammarError:
-                raise
-            except Exception as e:
-                raise VisitError(tree.data, tree, e)
-
-    def _call_userfunc_token(self, token):
-        try:
-            f = getattr(self, token.type)
-        except AttributeError:
-            return self.__default_token__(token)
-        else:
-            try:
-                return f(token)
-            except GrammarError:
-                raise
-            except Exception as e:
-                raise VisitError(token.type, token, e)
-
-    def _transform_children(self, children):
-        for c in children:
-            if isinstance(c, Tree):
-                res = self._transform_tree(c)
-            elif self.__visit_tokens__ and isinstance(c, Token):
-                res = self._call_userfunc_token(c)
-            else:
-                res = c
-
-            if res is not Discard:
-                yield res
-
-    def _transform_tree(self, tree):
-        children = list(self._transform_children(tree.children))
-        return self._call_userfunc(tree, children)
-
-    def transform(self, tree: Tree[_Leaf_T]) -> _Return_T:
-        "Transform the given tree, and return the final result"
-        return self._transform_tree(tree)
-
-    def __mul__(
-            self: 'Transformer[_Leaf_T, Tree[_Leaf_U]]',
-            other: 'Union[Transformer[_Leaf_U, _Return_V], TransformerChain[_Leaf_U, _Return_V,]]'
-    ) -> 'TransformerChain[_Leaf_T, _Return_V]':
-        """Chain two transformers together, returning a new transformer.
-        """
-        return TransformerChain(self, other)
-
-    def __default__(self, data, children, meta):
-        """Default function that is called if there is no attribute matching ``data``
-
-        Can be overridden. Defaults to creating a new copy of the tree node (i.e. ``return Tree(data, children, meta)``)
-        """
-        return Tree(data, children, meta)
-
-    def __default_token__(self, token):
-        """Default function that is called if there is no attribute matching ``token.type``
-
-        Can be overridden. Defaults to returning the token as-is.
-        """
-        return token
-
-
-def merge_transformers(base_transformer=None, **transformers_to_merge):
-    """Merge a collection of transformers into the base_transformer, each into its own 'namespace'.
-
-    When called, it will collect the methods from each transformer, and assign them to base_transformer,
-    with their name prefixed with the given keyword, as ``prefix__methodname``.
-
-    This function is especially useful for processing grammars that import other grammars,
-    thereby creating some of their rules in a 'namespace'. (i.e with a consistent name prefix).
-    In this case, the key for the transformer should match the name of the imported grammar.
-
-    Parameters:
-        base_transformer (Transformer, optional): The transformer that all other transformers will be added to.
-        **transformers_to_merge: Keyword arguments, in the form of ``name_prefix = transformer``.
-
-    Raises:
-        AttributeError: In case of a name collision in the merged methods
-
-    Example:
-        ::
-
-            class TBase(Transformer):
-                def start(self, children):
-                    return children[0] + 'bar'
-
-            class TImportedGrammar(Transformer):
-                def foo(self, children):
-                    return "foo"
-
-            composed_transformer = merge_transformers(TBase(), imported=TImportedGrammar())
-
-            t = Tree('start', [ Tree('imported__foo', []) ])
-
-            assert composed_transformer.transform(t) == 'foobar'
-
-    """
-    if base_transformer is None:
-        base_transformer = Transformer()
-    for prefix, transformer in transformers_to_merge.items():
-        for method_name in dir(transformer):
-            method = getattr(transformer, method_name)
-            if not callable(method):
-                continue
-            if method_name.startswith("_") or method_name == "transform":
-                continue
-            prefixed_method = prefix + "__" + method_name
-            if hasattr(base_transformer, prefixed_method):
-                raise AttributeError("Cannot merge: method '%s' appears more than once" % prefixed_method)
-
-            setattr(base_transformer, prefixed_method, method)
-
-    return base_transformer
-
-
-class InlineTransformer(Transformer):   # XXX Deprecated
-    def _call_userfunc(self, tree, new_children=None):
-        # Assumes tree is already transformed
-        children = new_children if new_children is not None else tree.children
-        try:
-            f = getattr(self, tree.data)
-        except AttributeError:
-            return self.__default__(tree.data, children, tree.meta)
-        else:
-            return f(*children)
-
-
-class TransformerChain(Generic[_Leaf_T, _Return_T]):
-
-    transformers: 'Tuple[Union[Transformer, TransformerChain], ...]'
-
-    def __init__(self, *transformers: 'Union[Transformer, TransformerChain]') -> None:
-        self.transformers = transformers
-
-    def transform(self, tree: Tree[_Leaf_T]) -> _Return_T:
-        for t in self.transformers:
-            tree = t.transform(tree)
-        return cast(_Return_T, tree)
-
-    def __mul__(
-            self: 'TransformerChain[_Leaf_T, Tree[_Leaf_U]]',
-            other: 'Union[Transformer[_Leaf_U, _Return_V], TransformerChain[_Leaf_U, _Return_V]]'
-    ) -> 'TransformerChain[_Leaf_T, _Return_V]':
-        return TransformerChain(*self.transformers + (other,))
-
-
-class Transformer_InPlace(Transformer[_Leaf_T, _Return_T]):
-    """Same as Transformer, but non-recursive, and changes the tree in-place instead of returning new instances
-
-    Useful for huge trees. Conservative in memory.
-    """
-    def _transform_tree(self, tree):           # Cancel recursion
-        return self._call_userfunc(tree)
-
-    def transform(self, tree: Tree[_Leaf_T]) -> _Return_T:
-        for subtree in tree.iter_subtrees():
-            subtree.children = list(self._transform_children(subtree.children))
-
-        return self._transform_tree(tree)
-
-
-class Transformer_NonRecursive(Transformer[_Leaf_T, _Return_T]):
-    """Same as Transformer but non-recursive.
-
-    Like Transformer, it doesn't change the original tree.
-
-    Useful for huge trees.
-    """
-
-    def transform(self, tree: Tree[_Leaf_T]) -> _Return_T:
-        # Tree to postfix
-        rev_postfix = []
-        q: List[Branch[_Leaf_T]] = [tree]
-        while q:
-            t = q.pop()
-            rev_postfix.append(t)
-            if isinstance(t, Tree):
-                q += t.children
-
-        # Postfix to tree
-        stack: List = []
-        for x in reversed(rev_postfix):
-            if isinstance(x, Tree):
-                size = len(x.children)
-                if size:
-                    args = stack[-size:]
-                    del stack[-size:]
-                else:
-                    args = []
-
-                res = self._call_userfunc(x, args)
-                if res is not Discard:
-                    stack.append(res)
-
-            elif self.__visit_tokens__ and isinstance(x, Token):
-                res = self._call_userfunc_token(x)
-                if res is not Discard:
-                    stack.append(res)
-            else:
-                stack.append(x)
-
-        result, = stack  # We should have only one tree remaining
-        # There are no guarantees on the type of the value produced by calling a user func for a
-        # child will produce. This means type system can't statically know that the final result is
-        # _Return_T. As a result a cast is required.
-        return cast(_Return_T, result)
-
-
-class Transformer_InPlaceRecursive(Transformer):
-    "Same as Transformer, recursive, but changes the tree in-place instead of returning new instances"
-    def _transform_tree(self, tree):
-        tree.children = list(self._transform_children(tree.children))
-        return self._call_userfunc(tree)
-
-
-# Visitors
-
-class VisitorBase:
-    def _call_userfunc(self, tree):
-        return getattr(self, tree.data, self.__default__)(tree)
-
-    def __default__(self, tree):
-        """Default function that is called if there is no attribute matching ``tree.data``
-
-        Can be overridden. Defaults to doing nothing.
-        """
-        return tree
-
-    def __class_getitem__(cls, _):
-        return cls
-
-
-class Visitor(VisitorBase, ABC, Generic[_Leaf_T]):
-    """Tree visitor, non-recursive (can handle huge trees).
-
-    Visiting a node calls its methods (provided by the user via inheritance) according to ``tree.data``
-    """
-
-    def visit(self, tree: Tree[_Leaf_T]) -> Tree[_Leaf_T]:
-        "Visits the tree, starting with the leaves and finally the root (bottom-up)"
-        for subtree in tree.iter_subtrees():
-            self._call_userfunc(subtree)
-        return tree
-
-    def visit_topdown(self, tree: Tree[_Leaf_T]) -> Tree[_Leaf_T]:
-        "Visit the tree, starting at the root, and ending at the leaves (top-down)"
-        for subtree in tree.iter_subtrees_topdown():
-            self._call_userfunc(subtree)
-        return tree
-
-
-class Visitor_Recursive(VisitorBase, Generic[_Leaf_T]):
-    """Bottom-up visitor, recursive.
-
-    Visiting a node calls its methods (provided by the user via inheritance) according to ``tree.data``
-
-    Slightly faster than the non-recursive version.
-    """
-
-    def visit(self, tree: Tree[_Leaf_T]) -> Tree[_Leaf_T]:
-        "Visits the tree, starting with the leaves and finally the root (bottom-up)"
-        for child in tree.children:
-            if isinstance(child, Tree):
-                self.visit(child)
-
-        self._call_userfunc(tree)
-        return tree
-
-    def visit_topdown(self,tree: Tree[_Leaf_T]) -> Tree[_Leaf_T]:
-        "Visit the tree, starting at the root, and ending at the leaves (top-down)"
-        self._call_userfunc(tree)
-
-        for child in tree.children:
-            if isinstance(child, Tree):
-                self.visit_topdown(child)
-
-        return tree
-
-
-class Interpreter(_Decoratable, ABC, Generic[_Leaf_T, _Return_T]):
-    """Interpreter walks the tree starting at the root.
-
-    Visits the tree, starting with the root and finally the leaves (top-down)
-
-    For each tree node, it calls its methods (provided by user via inheritance) according to ``tree.data``.
-
-    Unlike ``Transformer`` and ``Visitor``, the Interpreter doesn't automatically visit its sub-branches.
-    The user has to explicitly call ``visit``, ``visit_children``, or use the ``@visit_children_decor``.
-    This allows the user to implement branching and loops.
-    """
-
-    def visit(self, tree: Tree[_Leaf_T]) -> _Return_T:
-        # There are no guarantees on the type of the value produced by calling a user func for a
-        # child will produce. So only annotate the public method and use an internal method when
-        # visiting child trees.
-        return self._visit_tree(tree)
-
-    def _visit_tree(self, tree: Tree[_Leaf_T]):
-        f = getattr(self, tree.data)
-        wrapper = getattr(f, 'visit_wrapper', None)
-        if wrapper is not None:
-            return f.visit_wrapper(f, tree.data, tree.children, tree.meta)
-        else:
-            return f(tree)
-
-    def visit_children(self, tree: Tree[_Leaf_T]) -> List:
-        return [self._visit_tree(child) if isinstance(child, Tree) else child
-                for child in tree.children]
-
-    def __getattr__(self, name):
-        return self.__default__
-
-    def __default__(self, tree):
-        return self.visit_children(tree)
-
-
-_InterMethod = Callable[[Type[Interpreter], _Return_T], _R]
-
-def visit_children_decor(func: _InterMethod) -> _InterMethod:
-    "See Interpreter"
-    @wraps(func)
-    def inner(cls, tree):
-        values = cls.visit_children(tree)
-        return func(cls, values)
-    return inner
-
-# Decorators
-
-def _apply_v_args(obj, visit_wrapper):
-    try:
-        _apply = obj._apply_v_args
-    except AttributeError:
-        return _VArgsWrapper(obj, visit_wrapper)
-    else:
-        return _apply(visit_wrapper)
-
-
-class _VArgsWrapper:
-    """
-    A wrapper around a Callable. It delegates `__call__` to the Callable.
-    If the Callable has a `__get__`, that is also delegate and the resulting function is wrapped.
-    Otherwise, we use the original function mirroring the behaviour without a __get__.
-    We also have the visit_wrapper attribute to be used by Transformers.
-    """
-    base_func: Callable
-
-    def __init__(self, func: Callable, visit_wrapper: Callable[[Callable, str, list, Any], Any]):
-        if isinstance(func, _VArgsWrapper):
-            func = func.base_func
-        # https://github.com/python/mypy/issues/708
-        self.base_func = func  # type: ignore[assignment]
-        self.visit_wrapper = visit_wrapper
-        update_wrapper(self, func)
-
-    def __call__(self, *args, **kwargs):
-        return self.base_func(*args, **kwargs)
-
-    def __get__(self, instance, owner=None):
-        try:
-            # Use the __get__ attribute of the type instead of the instance
-            # to fully mirror the behavior of getattr
-            g = type(self.base_func).__get__
-        except AttributeError:
-            return self
-        else:
-            return _VArgsWrapper(g(self.base_func, instance, owner), self.visit_wrapper)
-
-    def __set_name__(self, owner, name):
-        try:
-            f = type(self.base_func).__set_name__
-        except AttributeError:
-            return
-        else:
-            f(self.base_func, owner, name)
-
-
-def _vargs_inline(f, _data, children, _meta):
-    return f(*children)
-def _vargs_meta_inline(f, _data, children, meta):
-    return f(meta, *children)
-def _vargs_meta(f, _data, children, meta):
-    return f(meta, children)
-def _vargs_tree(f, data, children, meta):
-    return f(Tree(data, children, meta))
-
-
-def v_args(inline: bool = False, meta: bool = False, tree: bool = False, wrapper: Optional[Callable] = None) -> Callable[[_DECORATED], _DECORATED]:
-    """A convenience decorator factory for modifying the behavior of user-supplied visitor methods.
-
-    By default, callback methods of transformers/visitors accept one argument - a list of the node's children.
-
-    ``v_args`` can modify this behavior. When used on a transformer/visitor class definition,
-    it applies to all the callback methods inside it.
-
-    ``v_args`` can be applied to a single method, or to an entire class. When applied to both,
-    the options given to the method take precedence.
-
-    Parameters:
-        inline (bool, optional): Children are provided as ``*args`` instead of a list argument (not recommended for very long lists).
-        meta (bool, optional): Provides two arguments: ``meta`` and ``children`` (instead of just the latter)
-        tree (bool, optional): Provides the entire tree as the argument, instead of the children.
-        wrapper (function, optional): Provide a function to decorate all methods.
-
-    Example:
-        ::
-
-            @v_args(inline=True)
-            class SolveArith(Transformer):
-                def add(self, left, right):
-                    return left + right
-
-                @v_args(meta=True)
-                def mul(self, meta, children):
-                    logger.info(f'mul at line {meta.line}')
-                    left, right = children
-                    return left * right
-
-
-            class ReverseNotation(Transformer_InPlace):
-                @v_args(tree=True)
-                def tree_node(self, tree):
-                    tree.children = tree.children[::-1]
-    """
-    if tree and (meta or inline):
-        raise ValueError("Visitor functions cannot combine 'tree' with 'meta' or 'inline'.")
-
-    func = None
-    if meta:
-        if inline:
-            func = _vargs_meta_inline
-        else:
-            func = _vargs_meta
-    elif inline:
-        func = _vargs_inline
-    elif tree:
-        func = _vargs_tree
-
-    if wrapper is not None:
-        if func is not None:
-            raise ValueError("Cannot use 'wrapper' along with 'tree', 'meta' or 'inline'.")
-        func = wrapper
-
-    def _visitor_args_dec(obj):
-        return _apply_v_args(obj, func)
-    return _visitor_args_dec
-
-
-###}
-
-
-# --- Visitor Utilities ---
-
-class CollapseAmbiguities(Transformer):
-    """
-    Transforms a tree that contains any number of _ambig nodes into a list of trees,
-    each one containing an unambiguous tree.
-
-    The length of the resulting list is the product of the length of all _ambig nodes.
-
-    Warning: This may quickly explode for highly ambiguous trees.
-
-    """
-    def _ambig(self, options):
-        return sum(options, [])
-
-    def __default__(self, data, children_lists, meta):
-        return [Tree(data, children, meta) for children in combine_alternatives(children_lists)]
-
-    def __default_token__(self, t):
-        return [t]
diff --git a/src/poetry/core/_vendor/packaging/LICENSE b/src/poetry/core/_vendor/packaging/LICENSE
deleted file mode 100644
index 6f62d44..0000000
--- a/src/poetry/core/_vendor/packaging/LICENSE
+++ /dev/null
@@ -1,3 +0,0 @@
-This software is made available under the terms of *either* of the licenses
-found in LICENSE.APACHE or LICENSE.BSD. Contributions to this software is made
-under the terms of *both* these licenses.
diff --git a/src/poetry/core/_vendor/packaging/LICENSE.APACHE b/src/poetry/core/_vendor/packaging/LICENSE.APACHE
deleted file mode 100644
index f433b1a..0000000
--- a/src/poetry/core/_vendor/packaging/LICENSE.APACHE
+++ /dev/null
@@ -1,177 +0,0 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
diff --git a/src/poetry/core/_vendor/packaging/LICENSE.BSD b/src/poetry/core/_vendor/packaging/LICENSE.BSD
deleted file mode 100644
index 42ce7b7..0000000
--- a/src/poetry/core/_vendor/packaging/LICENSE.BSD
+++ /dev/null
@@ -1,23 +0,0 @@
-Copyright (c) Donald Stufft and individual contributors.
-All rights reserved.
-
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are met:
-
-    1. Redistributions of source code must retain the above copyright notice,
-       this list of conditions and the following disclaimer.
-
-    2. Redistributions in binary form must reproduce the above copyright
-       notice, this list of conditions and the following disclaimer in the
-       documentation and/or other materials provided with the distribution.
-
-THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
-ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
-WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
-DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
-DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
-SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
-OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
diff --git a/src/poetry/core/_vendor/packaging/__init__.py b/src/poetry/core/_vendor/packaging/__init__.py
deleted file mode 100644
index 22809cf..0000000
--- a/src/poetry/core/_vendor/packaging/__init__.py
+++ /dev/null
@@ -1,15 +0,0 @@
-# This file is dual licensed under the terms of the Apache License, Version
-# 2.0, and the BSD License. See the LICENSE file in the root of this repository
-# for complete details.
-
-__title__ = "packaging"
-__summary__ = "Core utilities for Python packages"
-__uri__ = "https://github.com/pypa/packaging"
-
-__version__ = "23.2"
-
-__author__ = "Donald Stufft and individual contributors"
-__email__ = "donald@stufft.io"
-
-__license__ = "BSD-2-Clause or Apache-2.0"
-__copyright__ = "2014 %s" % __author__
diff --git a/src/poetry/core/_vendor/packaging/_elffile.py b/src/poetry/core/_vendor/packaging/_elffile.py
deleted file mode 100644
index 6fb19b3..0000000
--- a/src/poetry/core/_vendor/packaging/_elffile.py
+++ /dev/null
@@ -1,108 +0,0 @@
-"""
-ELF file parser.
-
-This provides a class ``ELFFile`` that parses an ELF executable in a similar
-interface to ``ZipFile``. Only the read interface is implemented.
-
-Based on: https://gist.github.com/lyssdod/f51579ae8d93c8657a5564aefc2ffbca
-ELF header: https://refspecs.linuxfoundation.org/elf/gabi4+/ch4.eheader.html
-"""
-
-import enum
-import os
-import struct
-from typing import IO, Optional, Tuple
-
-
-class ELFInvalid(ValueError):
-    pass
-
-
-class EIClass(enum.IntEnum):
-    C32 = 1
-    C64 = 2
-
-
-class EIData(enum.IntEnum):
-    Lsb = 1
-    Msb = 2
-
-
-class EMachine(enum.IntEnum):
-    I386 = 3
-    S390 = 22
-    Arm = 40
-    X8664 = 62
-    AArc64 = 183
-
-
-class ELFFile:
-    """
-    Representation of an ELF executable.
-    """
-
-    def __init__(self, f: IO[bytes]) -> None:
-        self._f = f
-
-        try:
-            ident = self._read("16B")
-        except struct.error:
-            raise ELFInvalid("unable to parse identification")
-        magic = bytes(ident[:4])
-        if magic != b"\x7fELF":
-            raise ELFInvalid(f"invalid magic: {magic!r}")
-
-        self.capacity = ident[4]  # Format for program header (bitness).
-        self.encoding = ident[5]  # Data structure encoding (endianness).
-
-        try:
-            # e_fmt: Format for program header.
-            # p_fmt: Format for section header.
-            # p_idx: Indexes to find p_type, p_offset, and p_filesz.
-            e_fmt, self._p_fmt, self._p_idx = {
-                (1, 1): ("<HHIIIIIHHH", "<IIIIIIII", (0, 1, 4)),  # 32-bit LSB.
-                (1, 2): (">HHIIIIIHHH", ">IIIIIIII", (0, 1, 4)),  # 32-bit MSB.
-                (2, 1): ("<HHIQQQIHHH", "<IIQQQQQQ", (0, 2, 5)),  # 64-bit LSB.
-                (2, 2): (">HHIQQQIHHH", ">IIQQQQQQ", (0, 2, 5)),  # 64-bit MSB.
-            }[(self.capacity, self.encoding)]
-        except KeyError:
-            raise ELFInvalid(
-                f"unrecognized capacity ({self.capacity}) or "
-                f"encoding ({self.encoding})"
-            )
-
-        try:
-            (
-                _,
-                self.machine,  # Architecture type.
-                _,
-                _,
-                self._e_phoff,  # Offset of program header.
-                _,
-                self.flags,  # Processor-specific flags.
-                _,
-                self._e_phentsize,  # Size of section.
-                self._e_phnum,  # Number of sections.
-            ) = self._read(e_fmt)
-        except struct.error as e:
-            raise ELFInvalid("unable to parse machine and section information") from e
-
-    def _read(self, fmt: str) -> Tuple[int, ...]:
-        return struct.unpack(fmt, self._f.read(struct.calcsize(fmt)))
-
-    @property
-    def interpreter(self) -> Optional[str]:
-        """
-        The path recorded in the ``PT_INTERP`` section header.
-        """
-        for index in range(self._e_phnum):
-            self._f.seek(self._e_phoff + self._e_phentsize * index)
-            try:
-                data = self._read(self._p_fmt)
-            except struct.error:
-                continue
-            if data[self._p_idx[0]] != 3:  # Not PT_INTERP.
-                continue
-            self._f.seek(data[self._p_idx[1]])
-            return os.fsdecode(self._f.read(data[self._p_idx[2]])).strip("\0")
-        return None
diff --git a/src/poetry/core/_vendor/packaging/_manylinux.py b/src/poetry/core/_vendor/packaging/_manylinux.py
deleted file mode 100644
index 3705d50..0000000
--- a/src/poetry/core/_vendor/packaging/_manylinux.py
+++ /dev/null
@@ -1,252 +0,0 @@
-import collections
-import contextlib
-import functools
-import os
-import re
-import sys
-import warnings
-from typing import Dict, Generator, Iterator, NamedTuple, Optional, Sequence, Tuple
-
-from ._elffile import EIClass, EIData, ELFFile, EMachine
-
-EF_ARM_ABIMASK = 0xFF000000
-EF_ARM_ABI_VER5 = 0x05000000
-EF_ARM_ABI_FLOAT_HARD = 0x00000400
-
-
-# `os.PathLike` not a generic type until Python 3.9, so sticking with `str`
-# as the type for `path` until then.
-@contextlib.contextmanager
-def _parse_elf(path: str) -> Generator[Optional[ELFFile], None, None]:
-    try:
-        with open(path, "rb") as f:
-            yield ELFFile(f)
-    except (OSError, TypeError, ValueError):
-        yield None
-
-
-def _is_linux_armhf(executable: str) -> bool:
-    # hard-float ABI can be detected from the ELF header of the running
-    # process
-    # https://static.docs.arm.com/ihi0044/g/aaelf32.pdf
-    with _parse_elf(executable) as f:
-        return (
-            f is not None
-            and f.capacity == EIClass.C32
-            and f.encoding == EIData.Lsb
-            and f.machine == EMachine.Arm
-            and f.flags & EF_ARM_ABIMASK == EF_ARM_ABI_VER5
-            and f.flags & EF_ARM_ABI_FLOAT_HARD == EF_ARM_ABI_FLOAT_HARD
-        )
-
-
-def _is_linux_i686(executable: str) -> bool:
-    with _parse_elf(executable) as f:
-        return (
-            f is not None
-            and f.capacity == EIClass.C32
-            and f.encoding == EIData.Lsb
-            and f.machine == EMachine.I386
-        )
-
-
-def _have_compatible_abi(executable: str, archs: Sequence[str]) -> bool:
-    if "armv7l" in archs:
-        return _is_linux_armhf(executable)
-    if "i686" in archs:
-        return _is_linux_i686(executable)
-    allowed_archs = {"x86_64", "aarch64", "ppc64", "ppc64le", "s390x", "loongarch64"}
-    return any(arch in allowed_archs for arch in archs)
-
-
-# If glibc ever changes its major version, we need to know what the last
-# minor version was, so we can build the complete list of all versions.
-# For now, guess what the highest minor version might be, assume it will
-# be 50 for testing. Once this actually happens, update the dictionary
-# with the actual value.
-_LAST_GLIBC_MINOR: Dict[int, int] = collections.defaultdict(lambda: 50)
-
-
-class _GLibCVersion(NamedTuple):
-    major: int
-    minor: int
-
-
-def _glibc_version_string_confstr() -> Optional[str]:
-    """
-    Primary implementation of glibc_version_string using os.confstr.
-    """
-    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely
-    # to be broken or missing. This strategy is used in the standard library
-    # platform module.
-    # https://github.com/python/cpython/blob/fcf1d003bf4f0100c/Lib/platform.py#L175-L183
-    try:
-        # Should be a string like "glibc 2.17".
-        version_string: str = getattr(os, "confstr")("CS_GNU_LIBC_VERSION")
-        assert version_string is not None
-        _, version = version_string.rsplit()
-    except (AssertionError, AttributeError, OSError, ValueError):
-        # os.confstr() or CS_GNU_LIBC_VERSION not available (or a bad value)...
-        return None
-    return version
-
-
-def _glibc_version_string_ctypes() -> Optional[str]:
-    """
-    Fallback implementation of glibc_version_string using ctypes.
-    """
-    try:
-        import ctypes
-    except ImportError:
-        return None
-
-    # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen
-    # manpage says, "If filename is NULL, then the returned handle is for the
-    # main program". This way we can let the linker do the work to figure out
-    # which libc our process is actually using.
-    #
-    # We must also handle the special case where the executable is not a
-    # dynamically linked executable. This can occur when using musl libc,
-    # for example. In this situation, dlopen() will error, leading to an
-    # OSError. Interestingly, at least in the case of musl, there is no
-    # errno set on the OSError. The single string argument used to construct
-    # OSError comes from libc itself and is therefore not portable to
-    # hard code here. In any case, failure to call dlopen() means we
-    # can proceed, so we bail on our attempt.
-    try:
-        process_namespace = ctypes.CDLL(None)
-    except OSError:
-        return None
-
-    try:
-        gnu_get_libc_version = process_namespace.gnu_get_libc_version
-    except AttributeError:
-        # Symbol doesn't exist -> therefore, we are not linked to
-        # glibc.
-        return None
-
-    # Call gnu_get_libc_version, which returns a string like "2.5"
-    gnu_get_libc_version.restype = ctypes.c_char_p
-    version_str: str = gnu_get_libc_version()
-    # py2 / py3 compatibility:
-    if not isinstance(version_str, str):
-        version_str = version_str.decode("ascii")
-
-    return version_str
-
-
-def _glibc_version_string() -> Optional[str]:
-    """Returns glibc version string, or None if not using glibc."""
-    return _glibc_version_string_confstr() or _glibc_version_string_ctypes()
-
-
-def _parse_glibc_version(version_str: str) -> Tuple[int, int]:
-    """Parse glibc version.
-
-    We use a regexp instead of str.split because we want to discard any
-    random junk that might come after the minor version -- this might happen
-    in patched/forked versions of glibc (e.g. Linaro's version of glibc
-    uses version strings like "2.20-2014.11"). See gh-3588.
-    """
-    m = re.match(r"(?P<major>[0-9]+)\.(?P<minor>[0-9]+)", version_str)
-    if not m:
-        warnings.warn(
-            f"Expected glibc version with 2 components major.minor,"
-            f" got: {version_str}",
-            RuntimeWarning,
-        )
-        return -1, -1
-    return int(m.group("major")), int(m.group("minor"))
-
-
-@functools.lru_cache()
-def _get_glibc_version() -> Tuple[int, int]:
-    version_str = _glibc_version_string()
-    if version_str is None:
-        return (-1, -1)
-    return _parse_glibc_version(version_str)
-
-
-# From PEP 513, PEP 600
-def _is_compatible(arch: str, version: _GLibCVersion) -> bool:
-    sys_glibc = _get_glibc_version()
-    if sys_glibc < version:
-        return False
-    # Check for presence of _manylinux module.
-    try:
-        import _manylinux  # noqa
-    except ImportError:
-        return True
-    if hasattr(_manylinux, "manylinux_compatible"):
-        result = _manylinux.manylinux_compatible(version[0], version[1], arch)
-        if result is not None:
-            return bool(result)
-        return True
-    if version == _GLibCVersion(2, 5):
-        if hasattr(_manylinux, "manylinux1_compatible"):
-            return bool(_manylinux.manylinux1_compatible)
-    if version == _GLibCVersion(2, 12):
-        if hasattr(_manylinux, "manylinux2010_compatible"):
-            return bool(_manylinux.manylinux2010_compatible)
-    if version == _GLibCVersion(2, 17):
-        if hasattr(_manylinux, "manylinux2014_compatible"):
-            return bool(_manylinux.manylinux2014_compatible)
-    return True
-
-
-_LEGACY_MANYLINUX_MAP = {
-    # CentOS 7 w/ glibc 2.17 (PEP 599)
-    (2, 17): "manylinux2014",
-    # CentOS 6 w/ glibc 2.12 (PEP 571)
-    (2, 12): "manylinux2010",
-    # CentOS 5 w/ glibc 2.5 (PEP 513)
-    (2, 5): "manylinux1",
-}
-
-
-def platform_tags(archs: Sequence[str]) -> Iterator[str]:
-    """Generate manylinux tags compatible to the current platform.
-
-    :param archs: Sequence of compatible architectures.
-        The first one shall be the closest to the actual architecture and be the part of
-        platform tag after the ``linux_`` prefix, e.g. ``x86_64``.
-        The ``linux_`` prefix is assumed as a prerequisite for the current platform to
-        be manylinux-compatible.
-
-    :returns: An iterator of compatible manylinux tags.
-    """
-    if not _have_compatible_abi(sys.executable, archs):
-        return
-    # Oldest glibc to be supported regardless of architecture is (2, 17).
-    too_old_glibc2 = _GLibCVersion(2, 16)
-    if set(archs) & {"x86_64", "i686"}:
-        # On x86/i686 also oldest glibc to be supported is (2, 5).
-        too_old_glibc2 = _GLibCVersion(2, 4)
-    current_glibc = _GLibCVersion(*_get_glibc_version())
-    glibc_max_list = [current_glibc]
-    # We can assume compatibility across glibc major versions.
-    # https://sourceware.org/bugzilla/show_bug.cgi?id=24636
-    #
-    # Build a list of maximum glibc versions so that we can
-    # output the canonical list of all glibc from current_glibc
-    # down to too_old_glibc2, including all intermediary versions.
-    for glibc_major in range(current_glibc.major - 1, 1, -1):
-        glibc_minor = _LAST_GLIBC_MINOR[glibc_major]
-        glibc_max_list.append(_GLibCVersion(glibc_major, glibc_minor))
-    for arch in archs:
-        for glibc_max in glibc_max_list:
-            if glibc_max.major == too_old_glibc2.major:
-                min_minor = too_old_glibc2.minor
-            else:
-                # For other glibc major versions oldest supported is (x, 0).
-                min_minor = -1
-            for glibc_minor in range(glibc_max.minor, min_minor, -1):
-                glibc_version = _GLibCVersion(glibc_max.major, glibc_minor)
-                tag = "manylinux_{}_{}".format(*glibc_version)
-                if _is_compatible(arch, glibc_version):
-                    yield f"{tag}_{arch}"
-                # Handle the legacy manylinux1, manylinux2010, manylinux2014 tags.
-                if glibc_version in _LEGACY_MANYLINUX_MAP:
-                    legacy_tag = _LEGACY_MANYLINUX_MAP[glibc_version]
-                    if _is_compatible(arch, glibc_version):
-                        yield f"{legacy_tag}_{arch}"
diff --git a/src/poetry/core/_vendor/packaging/_musllinux.py b/src/poetry/core/_vendor/packaging/_musllinux.py
deleted file mode 100644
index 86419df..0000000
--- a/src/poetry/core/_vendor/packaging/_musllinux.py
+++ /dev/null
@@ -1,83 +0,0 @@
-"""PEP 656 support.
-
-This module implements logic to detect if the currently running Python is
-linked against musl, and what musl version is used.
-"""
-
-import functools
-import re
-import subprocess
-import sys
-from typing import Iterator, NamedTuple, Optional, Sequence
-
-from ._elffile import ELFFile
-
-
-class _MuslVersion(NamedTuple):
-    major: int
-    minor: int
-
-
-def _parse_musl_version(output: str) -> Optional[_MuslVersion]:
-    lines = [n for n in (n.strip() for n in output.splitlines()) if n]
-    if len(lines) < 2 or lines[0][:4] != "musl":
-        return None
-    m = re.match(r"Version (\d+)\.(\d+)", lines[1])
-    if not m:
-        return None
-    return _MuslVersion(major=int(m.group(1)), minor=int(m.group(2)))
-
-
-@functools.lru_cache()
-def _get_musl_version(executable: str) -> Optional[_MuslVersion]:
-    """Detect currently-running musl runtime version.
-
-    This is done by checking the specified executable's dynamic linking
-    information, and invoking the loader to parse its output for a version
-    string. If the loader is musl, the output would be something like::
-
-        musl libc (x86_64)
-        Version 1.2.2
-        Dynamic Program Loader
-    """
-    try:
-        with open(executable, "rb") as f:
-            ld = ELFFile(f).interpreter
-    except (OSError, TypeError, ValueError):
-        return None
-    if ld is None or "musl" not in ld:
-        return None
-    proc = subprocess.run([ld], stderr=subprocess.PIPE, text=True)
-    return _parse_musl_version(proc.stderr)
-
-
-def platform_tags(archs: Sequence[str]) -> Iterator[str]:
-    """Generate musllinux tags compatible to the current platform.
-
-    :param archs: Sequence of compatible architectures.
-        The first one shall be the closest to the actual architecture and be the part of
-        platform tag after the ``linux_`` prefix, e.g. ``x86_64``.
-        The ``linux_`` prefix is assumed as a prerequisite for the current platform to
-        be musllinux-compatible.
-
-    :returns: An iterator of compatible musllinux tags.
-    """
-    sys_musl = _get_musl_version(sys.executable)
-    if sys_musl is None:  # Python not dynamically linked against musl.
-        return
-    for arch in archs:
-        for minor in range(sys_musl.minor, -1, -1):
-            yield f"musllinux_{sys_musl.major}_{minor}_{arch}"
-
-
-if __name__ == "__main__":  # pragma: no cover
-    import sysconfig
-
-    plat = sysconfig.get_platform()
-    assert plat.startswith("linux-"), "not linux"
-
-    print("plat:", plat)
-    print("musl:", _get_musl_version(sys.executable))
-    print("tags:", end=" ")
-    for t in platform_tags(re.sub(r"[.-]", "_", plat.split("-", 1)[-1])):
-        print(t, end="\n      ")
diff --git a/src/poetry/core/_vendor/packaging/_parser.py b/src/poetry/core/_vendor/packaging/_parser.py
deleted file mode 100644
index 4576981..0000000
--- a/src/poetry/core/_vendor/packaging/_parser.py
+++ /dev/null
@@ -1,359 +0,0 @@
-"""Handwritten parser of dependency specifiers.
-
-The docstring for each __parse_* function contains ENBF-inspired grammar representing
-the implementation.
-"""
-
-import ast
-from typing import Any, List, NamedTuple, Optional, Tuple, Union
-
-from ._tokenizer import DEFAULT_RULES, Tokenizer
-
-
-class Node:
-    def __init__(self, value: str) -> None:
-        self.value = value
-
-    def __str__(self) -> str:
-        return self.value
-
-    def __repr__(self) -> str:
-        return f"<{self.__class__.__name__}('{self}')>"
-
-    def serialize(self) -> str:
-        raise NotImplementedError
-
-
-class Variable(Node):
-    def serialize(self) -> str:
-        return str(self)
-
-
-class Value(Node):
-    def serialize(self) -> str:
-        return f'"{self}"'
-
-
-class Op(Node):
-    def serialize(self) -> str:
-        return str(self)
-
-
-MarkerVar = Union[Variable, Value]
-MarkerItem = Tuple[MarkerVar, Op, MarkerVar]
-# MarkerAtom = Union[MarkerItem, List["MarkerAtom"]]
-# MarkerList = List[Union["MarkerList", MarkerAtom, str]]
-# mypy does not support recursive type definition
-# https://github.com/python/mypy/issues/731
-MarkerAtom = Any
-MarkerList = List[Any]
-
-
-class ParsedRequirement(NamedTuple):
-    name: str
-    url: str
-    extras: List[str]
-    specifier: str
-    marker: Optional[MarkerList]
-
-
-# --------------------------------------------------------------------------------------
-# Recursive descent parser for dependency specifier
-# --------------------------------------------------------------------------------------
-def parse_requirement(source: str) -> ParsedRequirement:
-    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))
-
-
-def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
-    """
-    requirement = WS? IDENTIFIER WS? extras WS? requirement_details
-    """
-    tokenizer.consume("WS")
-
-    name_token = tokenizer.expect(
-        "IDENTIFIER", expected="package name at the start of dependency specifier"
-    )
-    name = name_token.text
-    tokenizer.consume("WS")
-
-    extras = _parse_extras(tokenizer)
-    tokenizer.consume("WS")
-
-    url, specifier, marker = _parse_requirement_details(tokenizer)
-    tokenizer.expect("END", expected="end of dependency specifier")
-
-    return ParsedRequirement(name, url, extras, specifier, marker)
-
-
-def _parse_requirement_details(
-    tokenizer: Tokenizer,
-) -> Tuple[str, str, Optional[MarkerList]]:
-    """
-    requirement_details = AT URL (WS requirement_marker?)?
-                        | specifier WS? (requirement_marker)?
-    """
-
-    specifier = ""
-    url = ""
-    marker = None
-
-    if tokenizer.check("AT"):
-        tokenizer.read()
-        tokenizer.consume("WS")
-
-        url_start = tokenizer.position
-        url = tokenizer.expect("URL", expected="URL after @").text
-        if tokenizer.check("END", peek=True):
-            return (url, specifier, marker)
-
-        tokenizer.expect("WS", expected="whitespace after URL")
-
-        # The input might end after whitespace.
-        if tokenizer.check("END", peek=True):
-            return (url, specifier, marker)
-
-        marker = _parse_requirement_marker(
-            tokenizer, span_start=url_start, after="URL and whitespace"
-        )
-    else:
-        specifier_start = tokenizer.position
-        specifier = _parse_specifier(tokenizer)
-        tokenizer.consume("WS")
-
-        if tokenizer.check("END", peek=True):
-            return (url, specifier, marker)
-
-        marker = _parse_requirement_marker(
-            tokenizer,
-            span_start=specifier_start,
-            after=(
-                "version specifier"
-                if specifier
-                else "name and no valid version specifier"
-            ),
-        )
-
-    return (url, specifier, marker)
-
-
-def _parse_requirement_marker(
-    tokenizer: Tokenizer, *, span_start: int, after: str
-) -> MarkerList:
-    """
-    requirement_marker = SEMICOLON marker WS?
-    """
-
-    if not tokenizer.check("SEMICOLON"):
-        tokenizer.raise_syntax_error(
-            f"Expected end or semicolon (after {after})",
-            span_start=span_start,
-        )
-    tokenizer.read()
-
-    marker = _parse_marker(tokenizer)
-    tokenizer.consume("WS")
-
-    return marker
-
-
-def _parse_extras(tokenizer: Tokenizer) -> List[str]:
-    """
-    extras = (LEFT_BRACKET wsp* extras_list? wsp* RIGHT_BRACKET)?
-    """
-    if not tokenizer.check("LEFT_BRACKET", peek=True):
-        return []
-
-    with tokenizer.enclosing_tokens(
-        "LEFT_BRACKET",
-        "RIGHT_BRACKET",
-        around="extras",
-    ):
-        tokenizer.consume("WS")
-        extras = _parse_extras_list(tokenizer)
-        tokenizer.consume("WS")
-
-    return extras
-
-
-def _parse_extras_list(tokenizer: Tokenizer) -> List[str]:
-    """
-    extras_list = identifier (wsp* ',' wsp* identifier)*
-    """
-    extras: List[str] = []
-
-    if not tokenizer.check("IDENTIFIER"):
-        return extras
-
-    extras.append(tokenizer.read().text)
-
-    while True:
-        tokenizer.consume("WS")
-        if tokenizer.check("IDENTIFIER", peek=True):
-            tokenizer.raise_syntax_error("Expected comma between extra names")
-        elif not tokenizer.check("COMMA"):
-            break
-
-        tokenizer.read()
-        tokenizer.consume("WS")
-
-        extra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
-        extras.append(extra_token.text)
-
-    return extras
-
-
-def _parse_specifier(tokenizer: Tokenizer) -> str:
-    """
-    specifier = LEFT_PARENTHESIS WS? version_many WS? RIGHT_PARENTHESIS
-              | WS? version_many WS?
-    """
-    with tokenizer.enclosing_tokens(
-        "LEFT_PARENTHESIS",
-        "RIGHT_PARENTHESIS",
-        around="version specifier",
-    ):
-        tokenizer.consume("WS")
-        parsed_specifiers = _parse_version_many(tokenizer)
-        tokenizer.consume("WS")
-
-    return parsed_specifiers
-
-
-def _parse_version_many(tokenizer: Tokenizer) -> str:
-    """
-    version_many = (SPECIFIER (WS? COMMA WS? SPECIFIER)*)?
-    """
-    parsed_specifiers = ""
-    while tokenizer.check("SPECIFIER"):
-        span_start = tokenizer.position
-        parsed_specifiers += tokenizer.read().text
-        if tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
-            tokenizer.raise_syntax_error(
-                ".* suffix can only be used with `==` or `!=` operators",
-                span_start=span_start,
-                span_end=tokenizer.position + 1,
-            )
-        if tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
-            tokenizer.raise_syntax_error(
-                "Local version label can only be used with `==` or `!=` operators",
-                span_start=span_start,
-                span_end=tokenizer.position,
-            )
-        tokenizer.consume("WS")
-        if not tokenizer.check("COMMA"):
-            break
-        parsed_specifiers += tokenizer.read().text
-        tokenizer.consume("WS")
-
-    return parsed_specifiers
-
-
-# --------------------------------------------------------------------------------------
-# Recursive descent parser for marker expression
-# --------------------------------------------------------------------------------------
-def parse_marker(source: str) -> MarkerList:
-    return _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))
-
-
-def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
-    retval = _parse_marker(tokenizer)
-    tokenizer.expect("END", expected="end of marker expression")
-    return retval
-
-
-def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
-    """
-    marker = marker_atom (BOOLOP marker_atom)+
-    """
-    expression = [_parse_marker_atom(tokenizer)]
-    while tokenizer.check("BOOLOP"):
-        token = tokenizer.read()
-        expr_right = _parse_marker_atom(tokenizer)
-        expression.extend((token.text, expr_right))
-    return expression
-
-
-def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
-    """
-    marker_atom = WS? LEFT_PARENTHESIS WS? marker WS? RIGHT_PARENTHESIS WS?
-                | WS? marker_item WS?
-    """
-
-    tokenizer.consume("WS")
-    if tokenizer.check("LEFT_PARENTHESIS", peek=True):
-        with tokenizer.enclosing_tokens(
-            "LEFT_PARENTHESIS",
-            "RIGHT_PARENTHESIS",
-            around="marker expression",
-        ):
-            tokenizer.consume("WS")
-            marker: MarkerAtom = _parse_marker(tokenizer)
-            tokenizer.consume("WS")
-    else:
-        marker = _parse_marker_item(tokenizer)
-    tokenizer.consume("WS")
-    return marker
-
-
-def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
-    """
-    marker_item = WS? marker_var WS? marker_op WS? marker_var WS?
-    """
-    tokenizer.consume("WS")
-    marker_var_left = _parse_marker_var(tokenizer)
-    tokenizer.consume("WS")
-    marker_op = _parse_marker_op(tokenizer)
-    tokenizer.consume("WS")
-    marker_var_right = _parse_marker_var(tokenizer)
-    tokenizer.consume("WS")
-    return (marker_var_left, marker_op, marker_var_right)
-
-
-def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
-    """
-    marker_var = VARIABLE | QUOTED_STRING
-    """
-    if tokenizer.check("VARIABLE"):
-        return process_env_var(tokenizer.read().text.replace(".", "_"))
-    elif tokenizer.check("QUOTED_STRING"):
-        return process_python_str(tokenizer.read().text)
-    else:
-        tokenizer.raise_syntax_error(
-            message="Expected a marker variable or quoted string"
-        )
-
-
-def process_env_var(env_var: str) -> Variable:
-    if (
-        env_var == "platform_python_implementation"
-        or env_var == "python_implementation"
-    ):
-        return Variable("platform_python_implementation")
-    else:
-        return Variable(env_var)
-
-
-def process_python_str(python_str: str) -> Value:
-    value = ast.literal_eval(python_str)
-    return Value(str(value))
-
-
-def _parse_marker_op(tokenizer: Tokenizer) -> Op:
-    """
-    marker_op = IN | NOT IN | OP
-    """
-    if tokenizer.check("IN"):
-        tokenizer.read()
-        return Op("in")
-    elif tokenizer.check("NOT"):
-        tokenizer.read()
-        tokenizer.expect("WS", expected="whitespace after 'not'")
-        tokenizer.expect("IN", expected="'in' after 'not'")
-        return Op("not in")
-    elif tokenizer.check("OP"):
-        return Op(tokenizer.read().text)
-    else:
-        return tokenizer.raise_syntax_error(
-            "Expected marker operator, one of "
-            "<=, <, !=, ==, >=, >, ~=, ===, in, not in"
-        )
diff --git a/src/poetry/core/_vendor/packaging/_structures.py b/src/poetry/core/_vendor/packaging/_structures.py
deleted file mode 100644
index 90a6465..0000000
--- a/src/poetry/core/_vendor/packaging/_structures.py
+++ /dev/null
@@ -1,61 +0,0 @@
-# This file is dual licensed under the terms of the Apache License, Version
-# 2.0, and the BSD License. See the LICENSE file in the root of this repository
-# for complete details.
-
-
-class InfinityType:
-    def __repr__(self) -> str:
-        return "Infinity"
-
-    def __hash__(self) -> int:
-        return hash(repr(self))
-
-    def __lt__(self, other: object) -> bool:
-        return False
-
-    def __le__(self, other: object) -> bool:
-        return False
-
-    def __eq__(self, other: object) -> bool:
-        return isinstance(other, self.__class__)
-
-    def __gt__(self, other: object) -> bool:
-        return True
-
-    def __ge__(self, other: object) -> bool:
-        return True
-
-    def __neg__(self: object) -> "NegativeInfinityType":
-        return NegativeInfinity
-
-
-Infinity = InfinityType()
-
-
-class NegativeInfinityType:
-    def __repr__(self) -> str:
-        return "-Infinity"
-
-    def __hash__(self) -> int:
-        return hash(repr(self))
-
-    def __lt__(self, other: object) -> bool:
-        return True
-
-    def __le__(self, other: object) -> bool:
-        return True
-
-    def __eq__(self, other: object) -> bool:
-        return isinstance(other, self.__class__)
-
-    def __gt__(self, other: object) -> bool:
-        return False
-
-    def __ge__(self, other: object) -> bool:
-        return False
-
-    def __neg__(self: object) -> InfinityType:
-        return Infinity
-
-
-NegativeInfinity = NegativeInfinityType()
diff --git a/src/poetry/core/_vendor/packaging/_tokenizer.py b/src/poetry/core/_vendor/packaging/_tokenizer.py
deleted file mode 100644
index dd0d648..0000000
--- a/src/poetry/core/_vendor/packaging/_tokenizer.py
+++ /dev/null
@@ -1,192 +0,0 @@
-import contextlib
-import re
-from dataclasses import dataclass
-from typing import Dict, Iterator, NoReturn, Optional, Tuple, Union
-
-from .specifiers import Specifier
-
-
-@dataclass
-class Token:
-    name: str
-    text: str
-    position: int
-
-
-class ParserSyntaxError(Exception):
-    """The provided source text could not be parsed correctly."""
-
-    def __init__(
-        self,
-        message: str,
-        *,
-        source: str,
-        span: Tuple[int, int],
-    ) -> None:
-        self.span = span
-        self.message = message
-        self.source = source
-
-        super().__init__()
-
-    def __str__(self) -> str:
-        marker = " " * self.span[0] + "~" * (self.span[1] - self.span[0]) + "^"
-        return "\n    ".join([self.message, self.source, marker])
-
-
-DEFAULT_RULES: "Dict[str, Union[str, re.Pattern[str]]]" = {
-    "LEFT_PARENTHESIS": r"\(",
-    "RIGHT_PARENTHESIS": r"\)",
-    "LEFT_BRACKET": r"\[",
-    "RIGHT_BRACKET": r"\]",
-    "SEMICOLON": r";",
-    "COMMA": r",",
-    "QUOTED_STRING": re.compile(
-        r"""
-            (
-                ('[^']*')
-                |
-                ("[^"]*")
-            )
-        """,
-        re.VERBOSE,
-    ),
-    "OP": r"(===|==|~=|!=|<=|>=|<|>)",
-    "BOOLOP": r"\b(or|and)\b",
-    "IN": r"\bin\b",
-    "NOT": r"\bnot\b",
-    "VARIABLE": re.compile(
-        r"""
-            \b(
-                python_version
-                |python_full_version
-                |os[._]name
-                |sys[._]platform
-                |platform_(release|system)
-                |platform[._](version|machine|python_implementation)
-                |python_implementation
-                |implementation_(name|version)
-                |extra
-            )\b
-        """,
-        re.VERBOSE,
-    ),
-    "SPECIFIER": re.compile(
-        Specifier._operator_regex_str + Specifier._version_regex_str,
-        re.VERBOSE | re.IGNORECASE,
-    ),
-    "AT": r"\@",
-    "URL": r"[^ \t]+",
-    "IDENTIFIER": r"\b[a-zA-Z0-9][a-zA-Z0-9._-]*\b",
-    "VERSION_PREFIX_TRAIL": r"\.\*",
-    "VERSION_LOCAL_LABEL_TRAIL": r"\+[a-z0-9]+(?:[-_\.][a-z0-9]+)*",
-    "WS": r"[ \t]+",
-    "END": r"$",
-}
-
-
-class Tokenizer:
-    """Context-sensitive token parsing.
-
-    Provides methods to examine the input stream to check whether the next token
-    matches.
-    """
-
-    def __init__(
-        self,
-        source: str,
-        *,
-        rules: "Dict[str, Union[str, re.Pattern[str]]]",
-    ) -> None:
-        self.source = source
-        self.rules: Dict[str, re.Pattern[str]] = {
-            name: re.compile(pattern) for name, pattern in rules.items()
-        }
-        self.next_token: Optional[Token] = None
-        self.position = 0
-
-    def consume(self, name: str) -> None:
-        """Move beyond provided token name, if at current position."""
-        if self.check(name):
-            self.read()
-
-    def check(self, name: str, *, peek: bool = False) -> bool:
-        """Check whether the next token has the provided name.
-
-        By default, if the check succeeds, the token *must* be read before
-        another check. If `peek` is set to `True`, the token is not loaded and
-        would need to be checked again.
-        """
-        assert (
-            self.next_token is None
-        ), f"Cannot check for {name!r}, already have {self.next_token!r}"
-        assert name in self.rules, f"Unknown token name: {name!r}"
-
-        expression = self.rules[name]
-
-        match = expression.match(self.source, self.position)
-        if match is None:
-            return False
-        if not peek:
-            self.next_token = Token(name, match[0], self.position)
-        return True
-
-    def expect(self, name: str, *, expected: str) -> Token:
-        """Expect a certain token name next, failing with a syntax error otherwise.
-
-        The token is *not* read.
-        """
-        if not self.check(name):
-            raise self.raise_syntax_error(f"Expected {expected}")
-        return self.read()
-
-    def read(self) -> Token:
-        """Consume the next token and return it."""
-        token = self.next_token
-        assert token is not None
-
-        self.position += len(token.text)
-        self.next_token = None
-
-        return token
-
-    def raise_syntax_error(
-        self,
-        message: str,
-        *,
-        span_start: Optional[int] = None,
-        span_end: Optional[int] = None,
-    ) -> NoReturn:
-        """Raise ParserSyntaxError at the given position."""
-        span = (
-            self.position if span_start is None else span_start,
-            self.position if span_end is None else span_end,
-        )
-        raise ParserSyntaxError(
-            message,
-            source=self.source,
-            span=span,
-        )
-
-    @contextlib.contextmanager
-    def enclosing_tokens(
-        self, open_token: str, close_token: str, *, around: str
-    ) -> Iterator[None]:
-        if self.check(open_token):
-            open_position = self.position
-            self.read()
-        else:
-            open_position = None
-
-        yield
-
-        if open_position is None:
-            return
-
-        if not self.check(close_token):
-            self.raise_syntax_error(
-                f"Expected matching {close_token} for {open_token}, after {around}",
-                span_start=open_position,
-            )
-
-        self.read()
diff --git a/src/poetry/core/_vendor/packaging/markers.py b/src/poetry/core/_vendor/packaging/markers.py
deleted file mode 100644
index 8b98fca..0000000
--- a/src/poetry/core/_vendor/packaging/markers.py
+++ /dev/null
@@ -1,252 +0,0 @@
-# This file is dual licensed under the terms of the Apache License, Version
-# 2.0, and the BSD License. See the LICENSE file in the root of this repository
-# for complete details.
-
-import operator
-import os
-import platform
-import sys
-from typing import Any, Callable, Dict, List, Optional, Tuple, Union
-
-from ._parser import (
-    MarkerAtom,
-    MarkerList,
-    Op,
-    Value,
-    Variable,
-    parse_marker as _parse_marker,
-)
-from ._tokenizer import ParserSyntaxError
-from .specifiers import InvalidSpecifier, Specifier
-from .utils import canonicalize_name
-
-__all__ = [
-    "InvalidMarker",
-    "UndefinedComparison",
-    "UndefinedEnvironmentName",
-    "Marker",
-    "default_environment",
-]
-
-Operator = Callable[[str, str], bool]
-
-
-class InvalidMarker(ValueError):
-    """
-    An invalid marker was found, users should refer to PEP 508.
-    """
-
-
-class UndefinedComparison(ValueError):
-    """
-    An invalid operation was attempted on a value that doesn't support it.
-    """
-
-
-class UndefinedEnvironmentName(ValueError):
-    """
-    A name was attempted to be used that does not exist inside of the
-    environment.
-    """
-
-
-def _normalize_extra_values(results: Any) -> Any:
-    """
-    Normalize extra values.
-    """
-    if isinstance(results[0], tuple):
-        lhs, op, rhs = results[0]
-        if isinstance(lhs, Variable) and lhs.value == "extra":
-            normalized_extra = canonicalize_name(rhs.value)
-            rhs = Value(normalized_extra)
-        elif isinstance(rhs, Variable) and rhs.value == "extra":
-            normalized_extra = canonicalize_name(lhs.value)
-            lhs = Value(normalized_extra)
-        results[0] = lhs, op, rhs
-    return results
-
-
-def _format_marker(
-    marker: Union[List[str], MarkerAtom, str], first: Optional[bool] = True
-) -> str:
-
-    assert isinstance(marker, (list, tuple, str))
-
-    # Sometimes we have a structure like [[...]] which is a single item list
-    # where the single item is itself it's own list. In that case we want skip
-    # the rest of this function so that we don't get extraneous () on the
-    # outside.
-    if (
-        isinstance(marker, list)
-        and len(marker) == 1
-        and isinstance(marker[0], (list, tuple))
-    ):
-        return _format_marker(marker[0])
-
-    if isinstance(marker, list):
-        inner = (_format_marker(m, first=False) for m in marker)
-        if first:
-            return " ".join(inner)
-        else:
-            return "(" + " ".join(inner) + ")"
-    elif isinstance(marker, tuple):
-        return " ".join([m.serialize() for m in marker])
-    else:
-        return marker
-
-
-_operators: Dict[str, Operator] = {
-    "in": lambda lhs, rhs: lhs in rhs,
-    "not in": lambda lhs, rhs: lhs not in rhs,
-    "<": operator.lt,
-    "<=": operator.le,
-    "==": operator.eq,
-    "!=": operator.ne,
-    ">=": operator.ge,
-    ">": operator.gt,
-}
-
-
-def _eval_op(lhs: str, op: Op, rhs: str) -> bool:
-    try:
-        spec = Specifier("".join([op.serialize(), rhs]))
-    except InvalidSpecifier:
-        pass
-    else:
-        return spec.contains(lhs, prereleases=True)
-
-    oper: Optional[Operator] = _operators.get(op.serialize())
-    if oper is None:
-        raise UndefinedComparison(f"Undefined {op!r} on {lhs!r} and {rhs!r}.")
-
-    return oper(lhs, rhs)
-
-
-def _normalize(*values: str, key: str) -> Tuple[str, ...]:
-    # PEP 685 – Comparison of extra names for optional distribution dependencies
-    # https://peps.python.org/pep-0685/
-    # > When comparing extra names, tools MUST normalize the names being
-    # > compared using the semantics outlined in PEP 503 for names
-    if key == "extra":
-        return tuple(canonicalize_name(v) for v in values)
-
-    # other environment markers don't have such standards
-    return values
-
-
-def _evaluate_markers(markers: MarkerList, environment: Dict[str, str]) -> bool:
-    groups: List[List[bool]] = [[]]
-
-    for marker in markers:
-        assert isinstance(marker, (list, tuple, str))
-
-        if isinstance(marker, list):
-            groups[-1].append(_evaluate_markers(marker, environment))
-        elif isinstance(marker, tuple):
-            lhs, op, rhs = marker
-
-            if isinstance(lhs, Variable):
-                environment_key = lhs.value
-                lhs_value = environment[environment_key]
-                rhs_value = rhs.value
-            else:
-                lhs_value = lhs.value
-                environment_key = rhs.value
-                rhs_value = environment[environment_key]
-
-            lhs_value, rhs_value = _normalize(lhs_value, rhs_value, key=environment_key)
-            groups[-1].append(_eval_op(lhs_value, op, rhs_value))
-        else:
-            assert marker in ["and", "or"]
-            if marker == "or":
-                groups.append([])
-
-    return any(all(item) for item in groups)
-
-
-def format_full_version(info: "sys._version_info") -> str:
-    version = "{0.major}.{0.minor}.{0.micro}".format(info)
-    kind = info.releaselevel
-    if kind != "final":
-        version += kind[0] + str(info.serial)
-    return version
-
-
-def default_environment() -> Dict[str, str]:
-    iver = format_full_version(sys.implementation.version)
-    implementation_name = sys.implementation.name
-    return {
-        "implementation_name": implementation_name,
-        "implementation_version": iver,
-        "os_name": os.name,
-        "platform_machine": platform.machine(),
-        "platform_release": platform.release(),
-        "platform_system": platform.system(),
-        "platform_version": platform.version(),
-        "python_full_version": platform.python_version(),
-        "platform_python_implementation": platform.python_implementation(),
-        "python_version": ".".join(platform.python_version_tuple()[:2]),
-        "sys_platform": sys.platform,
-    }
-
-
-class Marker:
-    def __init__(self, marker: str) -> None:
-        # Note: We create a Marker object without calling this constructor in
-        #       packaging.requirements.Requirement. If any additional logic is
-        #       added here, make sure to mirror/adapt Requirement.
-        try:
-            self._markers = _normalize_extra_values(_parse_marker(marker))
-            # The attribute `_markers` can be described in terms of a recursive type:
-            # MarkerList = List[Union[Tuple[Node, ...], str, MarkerList]]
-            #
-            # For example, the following expression:
-            # python_version > "3.6" or (python_version == "3.6" and os_name == "unix")
-            #
-            # is parsed into:
-            # [
-            #     (<Variable('python_version')>, <Op('>')>, <Value('3.6')>),
-            #     'and',
-            #     [
-            #         (<Variable('python_version')>, <Op('==')>, <Value('3.6')>),
-            #         'or',
-            #         (<Variable('os_name')>, <Op('==')>, <Value('unix')>)
-            #     ]
-            # ]
-        except ParserSyntaxError as e:
-            raise InvalidMarker(str(e)) from e
-
-    def __str__(self) -> str:
-        return _format_marker(self._markers)
-
-    def __repr__(self) -> str:
-        return f"<Marker('{self}')>"
-
-    def __hash__(self) -> int:
-        return hash((self.__class__.__name__, str(self)))
-
-    def __eq__(self, other: Any) -> bool:
-        if not isinstance(other, Marker):
-            return NotImplemented
-
-        return str(self) == str(other)
-
-    def evaluate(self, environment: Optional[Dict[str, str]] = None) -> bool:
-        """Evaluate a marker.
-
-        Return the boolean from evaluating the given marker against the
-        environment. environment is an optional argument to override all or
-        part of the determined environment.
-
-        The environment is determined from the current Python process.
-        """
-        current_environment = default_environment()
-        current_environment["extra"] = ""
-        if environment is not None:
-            current_environment.update(environment)
-            # The API used to allow setting extra to None. We need to handle this
-            # case for backwards compatibility.
-            if current_environment["extra"] is None:
-                current_environment["extra"] = ""
-
-        return _evaluate_markers(self._markers, current_environment)
diff --git a/src/poetry/core/_vendor/packaging/metadata.py b/src/poetry/core/_vendor/packaging/metadata.py
deleted file mode 100644
index 7b0e6a9..0000000
--- a/src/poetry/core/_vendor/packaging/metadata.py
+++ /dev/null
@@ -1,822 +0,0 @@
-import email.feedparser
-import email.header
-import email.message
-import email.parser
-import email.policy
-import sys
-import typing
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    Generic,
-    List,
-    Optional,
-    Tuple,
-    Type,
-    Union,
-    cast,
-)
-
-from . import requirements, specifiers, utils, version as version_module
-
-T = typing.TypeVar("T")
-if sys.version_info[:2] >= (3, 8):  # pragma: no cover
-    from typing import Literal, TypedDict
-else:  # pragma: no cover
-    if typing.TYPE_CHECKING:
-        from typing_extensions import Literal, TypedDict
-    else:
-        try:
-            from typing_extensions import Literal, TypedDict
-        except ImportError:
-
-            class Literal:
-                def __init_subclass__(*_args, **_kwargs):
-                    pass
-
-            class TypedDict:
-                def __init_subclass__(*_args, **_kwargs):
-                    pass
-
-
-try:
-    ExceptionGroup = __builtins__.ExceptionGroup  # type: ignore[attr-defined]
-except AttributeError:
-
-    class ExceptionGroup(Exception):  # type: ignore[no-redef]  # noqa: N818
-        """A minimal implementation of :external:exc:`ExceptionGroup` from Python 3.11.
-
-        If :external:exc:`ExceptionGroup` is already defined by Python itself,
-        that version is used instead.
-        """
-
-        message: str
-        exceptions: List[Exception]
-
-        def __init__(self, message: str, exceptions: List[Exception]) -> None:
-            self.message = message
-            self.exceptions = exceptions
-
-        def __repr__(self) -> str:
-            return f"{self.__class__.__name__}({self.message!r}, {self.exceptions!r})"
-
-
-class InvalidMetadata(ValueError):
-    """A metadata field contains invalid data."""
-
-    field: str
-    """The name of the field that contains invalid data."""
-
-    def __init__(self, field: str, message: str) -> None:
-        self.field = field
-        super().__init__(message)
-
-
-# The RawMetadata class attempts to make as few assumptions about the underlying
-# serialization formats as possible. The idea is that as long as a serialization
-# formats offer some very basic primitives in *some* way then we can support
-# serializing to and from that format.
-class RawMetadata(TypedDict, total=False):
-    """A dictionary of raw core metadata.
-
-    Each field in core metadata maps to a key of this dictionary (when data is
-    provided). The key is lower-case and underscores are used instead of dashes
-    compared to the equivalent core metadata field. Any core metadata field that
-    can be specified multiple times or can hold multiple values in a single
-    field have a key with a plural name. See :class:`Metadata` whose attributes
-    match the keys of this dictionary.
-
-    Core metadata fields that can be specified multiple times are stored as a
-    list or dict depending on which is appropriate for the field. Any fields
-    which hold multiple values in a single field are stored as a list.
-
-    """
-
-    # Metadata 1.0 - PEP 241
-    metadata_version: str
-    name: str
-    version: str
-    platforms: List[str]
-    summary: str
-    description: str
-    keywords: List[str]
-    home_page: str
-    author: str
-    author_email: str
-    license: str
-
-    # Metadata 1.1 - PEP 314
-    supported_platforms: List[str]
-    download_url: str
-    classifiers: List[str]
-    requires: List[str]
-    provides: List[str]
-    obsoletes: List[str]
-
-    # Metadata 1.2 - PEP 345
-    maintainer: str
-    maintainer_email: str
-    requires_dist: List[str]
-    provides_dist: List[str]
-    obsoletes_dist: List[str]
-    requires_python: str
-    requires_external: List[str]
-    project_urls: Dict[str, str]
-
-    # Metadata 2.0
-    # PEP 426 attempted to completely revamp the metadata format
-    # but got stuck without ever being able to build consensus on
-    # it and ultimately ended up withdrawn.
-    #
-    # However, a number of tools had started emitting METADATA with
-    # `2.0` Metadata-Version, so for historical reasons, this version
-    # was skipped.
-
-    # Metadata 2.1 - PEP 566
-    description_content_type: str
-    provides_extra: List[str]
-
-    # Metadata 2.2 - PEP 643
-    dynamic: List[str]
-
-    # Metadata 2.3 - PEP 685
-    # No new fields were added in PEP 685, just some edge case were
-    # tightened up to provide better interoptability.
-
-
-_STRING_FIELDS = {
-    "author",
-    "author_email",
-    "description",
-    "description_content_type",
-    "download_url",
-    "home_page",
-    "license",
-    "maintainer",
-    "maintainer_email",
-    "metadata_version",
-    "name",
-    "requires_python",
-    "summary",
-    "version",
-}
-
-_LIST_FIELDS = {
-    "classifiers",
-    "dynamic",
-    "obsoletes",
-    "obsoletes_dist",
-    "platforms",
-    "provides",
-    "provides_dist",
-    "provides_extra",
-    "requires",
-    "requires_dist",
-    "requires_external",
-    "supported_platforms",
-}
-
-_DICT_FIELDS = {
-    "project_urls",
-}
-
-
-def _parse_keywords(data: str) -> List[str]:
-    """Split a string of comma-separate keyboards into a list of keywords."""
-    return [k.strip() for k in data.split(",")]
-
-
-def _parse_project_urls(data: List[str]) -> Dict[str, str]:
-    """Parse a list of label/URL string pairings separated by a comma."""
-    urls = {}
-    for pair in data:
-        # Our logic is slightly tricky here as we want to try and do
-        # *something* reasonable with malformed data.
-        #
-        # The main thing that we have to worry about, is data that does
-        # not have a ',' at all to split the label from the Value. There
-        # isn't a singular right answer here, and we will fail validation
-        # later on (if the caller is validating) so it doesn't *really*
-        # matter, but since the missing value has to be an empty str
-        # and our return value is dict[str, str], if we let the key
-        # be the missing value, then they'd have multiple '' values that
-        # overwrite each other in a accumulating dict.
-        #
-        # The other potentional issue is that it's possible to have the
-        # same label multiple times in the metadata, with no solid "right"
-        # answer with what to do in that case. As such, we'll do the only
-        # thing we can, which is treat the field as unparseable and add it
-        # to our list of unparsed fields.
-        parts = [p.strip() for p in pair.split(",", 1)]
-        parts.extend([""] * (max(0, 2 - len(parts))))  # Ensure 2 items
-
-        # TODO: The spec doesn't say anything about if the keys should be
-        #       considered case sensitive or not... logically they should
-        #       be case-preserving and case-insensitive, but doing that
-        #       would open up more cases where we might have duplicate
-        #       entries.
-        label, url = parts
-        if label in urls:
-            # The label already exists in our set of urls, so this field
-            # is unparseable, and we can just add the whole thing to our
-            # unparseable data and stop processing it.
-            raise KeyError("duplicate labels in project urls")
-        urls[label] = url
-
-    return urls
-
-
-def _get_payload(msg: email.message.Message, source: Union[bytes, str]) -> str:
-    """Get the body of the message."""
-    # If our source is a str, then our caller has managed encodings for us,
-    # and we don't need to deal with it.
-    if isinstance(source, str):
-        payload: str = msg.get_payload()
-        return payload
-    # If our source is a bytes, then we're managing the encoding and we need
-    # to deal with it.
-    else:
-        bpayload: bytes = msg.get_payload(decode=True)
-        try:
-            return bpayload.decode("utf8", "strict")
-        except UnicodeDecodeError:
-            raise ValueError("payload in an invalid encoding")
-
-
-# The various parse_FORMAT functions here are intended to be as lenient as
-# possible in their parsing, while still returning a correctly typed
-# RawMetadata.
-#
-# To aid in this, we also generally want to do as little touching of the
-# data as possible, except where there are possibly some historic holdovers
-# that make valid data awkward to work with.
-#
-# While this is a lower level, intermediate format than our ``Metadata``
-# class, some light touch ups can make a massive difference in usability.
-
-# Map METADATA fields to RawMetadata.
-_EMAIL_TO_RAW_MAPPING = {
-    "author": "author",
-    "author-email": "author_email",
-    "classifier": "classifiers",
-    "description": "description",
-    "description-content-type": "description_content_type",
-    "download-url": "download_url",
-    "dynamic": "dynamic",
-    "home-page": "home_page",
-    "keywords": "keywords",
-    "license": "license",
-    "maintainer": "maintainer",
-    "maintainer-email": "maintainer_email",
-    "metadata-version": "metadata_version",
-    "name": "name",
-    "obsoletes": "obsoletes",
-    "obsoletes-dist": "obsoletes_dist",
-    "platform": "platforms",
-    "project-url": "project_urls",
-    "provides": "provides",
-    "provides-dist": "provides_dist",
-    "provides-extra": "provides_extra",
-    "requires": "requires",
-    "requires-dist": "requires_dist",
-    "requires-external": "requires_external",
-    "requires-python": "requires_python",
-    "summary": "summary",
-    "supported-platform": "supported_platforms",
-    "version": "version",
-}
-_RAW_TO_EMAIL_MAPPING = {raw: email for email, raw in _EMAIL_TO_RAW_MAPPING.items()}
-
-
-def parse_email(data: Union[bytes, str]) -> Tuple[RawMetadata, Dict[str, List[str]]]:
-    """Parse a distribution's metadata stored as email headers (e.g. from ``METADATA``).
-
-    This function returns a two-item tuple of dicts. The first dict is of
-    recognized fields from the core metadata specification. Fields that can be
-    parsed and translated into Python's built-in types are converted
-    appropriately. All other fields are left as-is. Fields that are allowed to
-    appear multiple times are stored as lists.
-
-    The second dict contains all other fields from the metadata. This includes
-    any unrecognized fields. It also includes any fields which are expected to
-    be parsed into a built-in type but were not formatted appropriately. Finally,
-    any fields that are expected to appear only once but are repeated are
-    included in this dict.
-
-    """
-    raw: Dict[str, Union[str, List[str], Dict[str, str]]] = {}
-    unparsed: Dict[str, List[str]] = {}
-
-    if isinstance(data, str):
-        parsed = email.parser.Parser(policy=email.policy.compat32).parsestr(data)
-    else:
-        parsed = email.parser.BytesParser(policy=email.policy.compat32).parsebytes(data)
-
-    # We have to wrap parsed.keys() in a set, because in the case of multiple
-    # values for a key (a list), the key will appear multiple times in the
-    # list of keys, but we're avoiding that by using get_all().
-    for name in frozenset(parsed.keys()):
-        # Header names in RFC are case insensitive, so we'll normalize to all
-        # lower case to make comparisons easier.
-        name = name.lower()
-
-        # We use get_all() here, even for fields that aren't multiple use,
-        # because otherwise someone could have e.g. two Name fields, and we
-        # would just silently ignore it rather than doing something about it.
-        headers = parsed.get_all(name) or []
-
-        # The way the email module works when parsing bytes is that it
-        # unconditionally decodes the bytes as ascii using the surrogateescape
-        # handler. When you pull that data back out (such as with get_all() ),
-        # it looks to see if the str has any surrogate escapes, and if it does
-        # it wraps it in a Header object instead of returning the string.
-        #
-        # As such, we'll look for those Header objects, and fix up the encoding.
-        value = []
-        # Flag if we have run into any issues processing the headers, thus
-        # signalling that the data belongs in 'unparsed'.
-        valid_encoding = True
-        for h in headers:
-            # It's unclear if this can return more types than just a Header or
-            # a str, so we'll just assert here to make sure.
-            assert isinstance(h, (email.header.Header, str))
-
-            # If it's a header object, we need to do our little dance to get
-            # the real data out of it. In cases where there is invalid data
-            # we're going to end up with mojibake, but there's no obvious, good
-            # way around that without reimplementing parts of the Header object
-            # ourselves.
-            #
-            # That should be fine since, if mojibacked happens, this key is
-            # going into the unparsed dict anyways.
-            if isinstance(h, email.header.Header):
-                # The Header object stores it's data as chunks, and each chunk
-                # can be independently encoded, so we'll need to check each
-                # of them.
-                chunks: List[Tuple[bytes, Optional[str]]] = []
-                for bin, encoding in email.header.decode_header(h):
-                    try:
-                        bin.decode("utf8", "strict")
-                    except UnicodeDecodeError:
-                        # Enable mojibake.
-                        encoding = "latin1"
-                        valid_encoding = False
-                    else:
-                        encoding = "utf8"
-                    chunks.append((bin, encoding))
-
-                # Turn our chunks back into a Header object, then let that
-                # Header object do the right thing to turn them into a
-                # string for us.
-                value.append(str(email.header.make_header(chunks)))
-            # This is already a string, so just add it.
-            else:
-                value.append(h)
-
-        # We've processed all of our values to get them into a list of str,
-        # but we may have mojibake data, in which case this is an unparsed
-        # field.
-        if not valid_encoding:
-            unparsed[name] = value
-            continue
-
-        raw_name = _EMAIL_TO_RAW_MAPPING.get(name)
-        if raw_name is None:
-            # This is a bit of a weird situation, we've encountered a key that
-            # we don't know what it means, so we don't know whether it's meant
-            # to be a list or not.
-            #
-            # Since we can't really tell one way or another, we'll just leave it
-            # as a list, even though it may be a single item list, because that's
-            # what makes the most sense for email headers.
-            unparsed[name] = value
-            continue
-
-        # If this is one of our string fields, then we'll check to see if our
-        # value is a list of a single item. If it is then we'll assume that
-        # it was emitted as a single string, and unwrap the str from inside
-        # the list.
-        #
-        # If it's any other kind of data, then we haven't the faintest clue
-        # what we should parse it as, and we have to just add it to our list
-        # of unparsed stuff.
-        if raw_name in _STRING_FIELDS and len(value) == 1:
-            raw[raw_name] = value[0]
-        # If this is one of our list of string fields, then we can just assign
-        # the value, since email *only* has strings, and our get_all() call
-        # above ensures that this is a list.
-        elif raw_name in _LIST_FIELDS:
-            raw[raw_name] = value
-        # Special Case: Keywords
-        # The keywords field is implemented in the metadata spec as a str,
-        # but it conceptually is a list of strings, and is serialized using
-        # ", ".join(keywords), so we'll do some light data massaging to turn
-        # this into what it logically is.
-        elif raw_name == "keywords" and len(value) == 1:
-            raw[raw_name] = _parse_keywords(value[0])
-        # Special Case: Project-URL
-        # The project urls is implemented in the metadata spec as a list of
-        # specially-formatted strings that represent a key and a value, which
-        # is fundamentally a mapping, however the email format doesn't support
-        # mappings in a sane way, so it was crammed into a list of strings
-        # instead.
-        #
-        # We will do a little light data massaging to turn this into a map as
-        # it logically should be.
-        elif raw_name == "project_urls":
-            try:
-                raw[raw_name] = _parse_project_urls(value)
-            except KeyError:
-                unparsed[name] = value
-        # Nothing that we've done has managed to parse this, so it'll just
-        # throw it in our unparseable data and move on.
-        else:
-            unparsed[name] = value
-
-    # We need to support getting the Description from the message payload in
-    # addition to getting it from the the headers. This does mean, though, there
-    # is the possibility of it being set both ways, in which case we put both
-    # in 'unparsed' since we don't know which is right.
-    try:
-        payload = _get_payload(parsed, data)
-    except ValueError:
-        unparsed.setdefault("description", []).append(
-            parsed.get_payload(decode=isinstance(data, bytes))
-        )
-    else:
-        if payload:
-            # Check to see if we've already got a description, if so then both
-            # it, and this body move to unparseable.
-            if "description" in raw:
-                description_header = cast(str, raw.pop("description"))
-                unparsed.setdefault("description", []).extend(
-                    [description_header, payload]
-                )
-            elif "description" in unparsed:
-                unparsed["description"].append(payload)
-            else:
-                raw["description"] = payload
-
-    # We need to cast our `raw` to a metadata, because a TypedDict only support
-    # literal key names, but we're computing our key names on purpose, but the
-    # way this function is implemented, our `TypedDict` can only have valid key
-    # names.
-    return cast(RawMetadata, raw), unparsed
-
-
-_NOT_FOUND = object()
-
-
-# Keep the two values in sync.
-_VALID_METADATA_VERSIONS = ["1.0", "1.1", "1.2", "2.1", "2.2", "2.3"]
-_MetadataVersion = Literal["1.0", "1.1", "1.2", "2.1", "2.2", "2.3"]
-
-_REQUIRED_ATTRS = frozenset(["metadata_version", "name", "version"])
-
-
-class _Validator(Generic[T]):
-    """Validate a metadata field.
-
-    All _process_*() methods correspond to a core metadata field. The method is
-    called with the field's raw value. If the raw value is valid it is returned
-    in its "enriched" form (e.g. ``version.Version`` for the ``Version`` field).
-    If the raw value is invalid, :exc:`InvalidMetadata` is raised (with a cause
-    as appropriate).
-    """
-
-    name: str
-    raw_name: str
-    added: _MetadataVersion
-
-    def __init__(
-        self,
-        *,
-        added: _MetadataVersion = "1.0",
-    ) -> None:
-        self.added = added
-
-    def __set_name__(self, _owner: "Metadata", name: str) -> None:
-        self.name = name
-        self.raw_name = _RAW_TO_EMAIL_MAPPING[name]
-
-    def __get__(self, instance: "Metadata", _owner: Type["Metadata"]) -> T:
-        # With Python 3.8, the caching can be replaced with functools.cached_property().
-        # No need to check the cache as attribute lookup will resolve into the
-        # instance's __dict__ before __get__ is called.
-        cache = instance.__dict__
-        try:
-            value = instance._raw[self.name]  # type: ignore[literal-required]
-        except KeyError:
-            if self.name in _STRING_FIELDS:
-                value = ""
-            elif self.name in _LIST_FIELDS:
-                value = []
-            elif self.name in _DICT_FIELDS:
-                value = {}
-            else:  # pragma: no cover
-                assert False
-
-        try:
-            converter: Callable[[Any], T] = getattr(self, f"_process_{self.name}")
-        except AttributeError:
-            pass
-        else:
-            value = converter(value)
-
-        cache[self.name] = value
-        try:
-            del instance._raw[self.name]  # type: ignore[misc]
-        except KeyError:
-            pass
-
-        return cast(T, value)
-
-    def _invalid_metadata(
-        self, msg: str, cause: Optional[Exception] = None
-    ) -> InvalidMetadata:
-        exc = InvalidMetadata(
-            self.raw_name, msg.format_map({"field": repr(self.raw_name)})
-        )
-        exc.__cause__ = cause
-        return exc
-
-    def _process_metadata_version(self, value: str) -> _MetadataVersion:
-        # Implicitly makes Metadata-Version required.
-        if value not in _VALID_METADATA_VERSIONS:
-            raise self._invalid_metadata(f"{value!r} is not a valid metadata version")
-        return cast(_MetadataVersion, value)
-
-    def _process_name(self, value: str) -> str:
-        if not value:
-            raise self._invalid_metadata("{field} is a required field")
-        # Validate the name as a side-effect.
-        try:
-            utils.canonicalize_name(value, validate=True)
-        except utils.InvalidName as exc:
-            raise self._invalid_metadata(
-                f"{value!r} is invalid for {{field}}", cause=exc
-            )
-        else:
-            return value
-
-    def _process_version(self, value: str) -> version_module.Version:
-        if not value:
-            raise self._invalid_metadata("{field} is a required field")
-        try:
-            return version_module.parse(value)
-        except version_module.InvalidVersion as exc:
-            raise self._invalid_metadata(
-                f"{value!r} is invalid for {{field}}", cause=exc
-            )
-
-    def _process_summary(self, value: str) -> str:
-        """Check the field contains no newlines."""
-        if "\n" in value:
-            raise self._invalid_metadata("{field} must be a single line")
-        return value
-
-    def _process_description_content_type(self, value: str) -> str:
-        content_types = {"text/plain", "text/x-rst", "text/markdown"}
-        message = email.message.EmailMessage()
-        message["content-type"] = value
-
-        content_type, parameters = (
-            # Defaults to `text/plain` if parsing failed.
-            message.get_content_type().lower(),
-            message["content-type"].params,
-        )
-        # Check if content-type is valid or defaulted to `text/plain` and thus was
-        # not parseable.
-        if content_type not in content_types or content_type not in value.lower():
-            raise self._invalid_metadata(
-                f"{{field}} must be one of {list(content_types)}, not {value!r}"
-            )
-
-        charset = parameters.get("charset", "UTF-8")
-        if charset != "UTF-8":
-            raise self._invalid_metadata(
-                f"{{field}} can only specify the UTF-8 charset, not {list(charset)}"
-            )
-
-        markdown_variants = {"GFM", "CommonMark"}
-        variant = parameters.get("variant", "GFM")  # Use an acceptable default.
-        if content_type == "text/markdown" and variant not in markdown_variants:
-            raise self._invalid_metadata(
-                f"valid Markdown variants for {{field}} are {list(markdown_variants)}, "
-                f"not {variant!r}",
-            )
-        return value
-
-    def _process_dynamic(self, value: List[str]) -> List[str]:
-        for dynamic_field in map(str.lower, value):
-            if dynamic_field in {"name", "version", "metadata-version"}:
-                raise self._invalid_metadata(
-                    f"{value!r} is not allowed as a dynamic field"
-                )
-            elif dynamic_field not in _EMAIL_TO_RAW_MAPPING:
-                raise self._invalid_metadata(f"{value!r} is not a valid dynamic field")
-        return list(map(str.lower, value))
-
-    def _process_provides_extra(
-        self,
-        value: List[str],
-    ) -> List[utils.NormalizedName]:
-        normalized_names = []
-        try:
-            for name in value:
-                normalized_names.append(utils.canonicalize_name(name, validate=True))
-        except utils.InvalidName as exc:
-            raise self._invalid_metadata(
-                f"{name!r} is invalid for {{field}}", cause=exc
-            )
-        else:
-            return normalized_names
-
-    def _process_requires_python(self, value: str) -> specifiers.SpecifierSet:
-        try:
-            return specifiers.SpecifierSet(value)
-        except specifiers.InvalidSpecifier as exc:
-            raise self._invalid_metadata(
-                f"{value!r} is invalid for {{field}}", cause=exc
-            )
-
-    def _process_requires_dist(
-        self,
-        value: List[str],
-    ) -> List[requirements.Requirement]:
-        reqs = []
-        try:
-            for req in value:
-                reqs.append(requirements.Requirement(req))
-        except requirements.InvalidRequirement as exc:
-            raise self._invalid_metadata(f"{req!r} is invalid for {{field}}", cause=exc)
-        else:
-            return reqs
-
-
-class Metadata:
-    """Representation of distribution metadata.
-
-    Compared to :class:`RawMetadata`, this class provides objects representing
-    metadata fields instead of only using built-in types. Any invalid metadata
-    will cause :exc:`InvalidMetadata` to be raised (with a
-    :py:attr:`~BaseException.__cause__` attribute as appropriate).
-    """
-
-    _raw: RawMetadata
-
-    @classmethod
-    def from_raw(cls, data: RawMetadata, *, validate: bool = True) -> "Metadata":
-        """Create an instance from :class:`RawMetadata`.
-
-        If *validate* is true, all metadata will be validated. All exceptions
-        related to validation will be gathered and raised as an :class:`ExceptionGroup`.
-        """
-        ins = cls()
-        ins._raw = data.copy()  # Mutations occur due to caching enriched values.
-
-        if validate:
-            exceptions: List[InvalidMetadata] = []
-            try:
-                metadata_version = ins.metadata_version
-                metadata_age = _VALID_METADATA_VERSIONS.index(metadata_version)
-            except InvalidMetadata as metadata_version_exc:
-                exceptions.append(metadata_version_exc)
-                metadata_version = None
-
-            # Make sure to check for the fields that are present, the required
-            # fields (so their absence can be reported).
-            fields_to_check = frozenset(ins._raw) | _REQUIRED_ATTRS
-            # Remove fields that have already been checked.
-            fields_to_check -= {"metadata_version"}
-
-            for key in fields_to_check:
-                try:
-                    if metadata_version:
-                        # Can't use getattr() as that triggers descriptor protocol which
-                        # will fail due to no value for the instance argument.
-                        try:
-                            field_metadata_version = cls.__dict__[key].added
-                        except KeyError:
-                            exc = InvalidMetadata(key, f"unrecognized field: {key!r}")
-                            exceptions.append(exc)
-                            continue
-                        field_age = _VALID_METADATA_VERSIONS.index(
-                            field_metadata_version
-                        )
-                        if field_age > metadata_age:
-                            field = _RAW_TO_EMAIL_MAPPING[key]
-                            exc = InvalidMetadata(
-                                field,
-                                "{field} introduced in metadata version "
-                                "{field_metadata_version}, not {metadata_version}",
-                            )
-                            exceptions.append(exc)
-                            continue
-                    getattr(ins, key)
-                except InvalidMetadata as exc:
-                    exceptions.append(exc)
-
-            if exceptions:
-                raise ExceptionGroup("invalid metadata", exceptions)
-
-        return ins
-
-    @classmethod
-    def from_email(
-        cls, data: Union[bytes, str], *, validate: bool = True
-    ) -> "Metadata":
-        """Parse metadata from email headers.
-
-        If *validate* is true, the metadata will be validated. All exceptions
-        related to validation will be gathered and raised as an :class:`ExceptionGroup`.
-        """
-        exceptions: list[InvalidMetadata] = []
-        raw, unparsed = parse_email(data)
-
-        if validate:
-            for unparsed_key in unparsed:
-                if unparsed_key in _EMAIL_TO_RAW_MAPPING:
-                    message = f"{unparsed_key!r} has invalid data"
-                else:
-                    message = f"unrecognized field: {unparsed_key!r}"
-                exceptions.append(InvalidMetadata(unparsed_key, message))
-
-            if exceptions:
-                raise ExceptionGroup("unparsed", exceptions)
-
-        try:
-            return cls.from_raw(raw, validate=validate)
-        except ExceptionGroup as exc_group:
-            exceptions.extend(exc_group.exceptions)
-            raise ExceptionGroup("invalid or unparsed metadata", exceptions) from None
-
-    metadata_version: _Validator[_MetadataVersion] = _Validator()
-    """:external:ref:`core-metadata-metadata-version`
-    (required; validated to be a valid metadata version)"""
-    name: _Validator[str] = _Validator()
-    """:external:ref:`core-metadata-name`
-    (required; validated using :func:`~packaging.utils.canonicalize_name` and its
-    *validate* parameter)"""
-    version: _Validator[version_module.Version] = _Validator()
-    """:external:ref:`core-metadata-version` (required)"""
-    dynamic: _Validator[List[str]] = _Validator(
-        added="2.2",
-    )
-    """:external:ref:`core-metadata-dynamic`
-    (validated against core metadata field names and lowercased)"""
-    platforms: _Validator[List[str]] = _Validator()
-    """:external:ref:`core-metadata-platform`"""
-    supported_platforms: _Validator[List[str]] = _Validator(added="1.1")
-    """:external:ref:`core-metadata-supported-platform`"""
-    summary: _Validator[str] = _Validator()
-    """:external:ref:`core-metadata-summary` (validated to contain no newlines)"""
-    description: _Validator[str] = _Validator()  # TODO 2.1: can be in body
-    """:external:ref:`core-metadata-description`"""
-    description_content_type: _Validator[str] = _Validator(added="2.1")
-    """:external:ref:`core-metadata-description-content-type` (validated)"""
-    keywords: _Validator[List[str]] = _Validator()
-    """:external:ref:`core-metadata-keywords`"""
-    home_page: _Validator[str] = _Validator()
-    """:external:ref:`core-metadata-home-page`"""
-    download_url: _Validator[str] = _Validator(added="1.1")
-    """:external:ref:`core-metadata-download-url`"""
-    author: _Validator[str] = _Validator()
-    """:external:ref:`core-metadata-author`"""
-    author_email: _Validator[str] = _Validator()
-    """:external:ref:`core-metadata-author-email`"""
-    maintainer: _Validator[str] = _Validator(added="1.2")
-    """:external:ref:`core-metadata-maintainer`"""
-    maintainer_email: _Validator[str] = _Validator(added="1.2")
-    """:external:ref:`core-metadata-maintainer-email`"""
-    license: _Validator[str] = _Validator()
-    """:external:ref:`core-metadata-license`"""
-    classifiers: _Validator[List[str]] = _Validator(added="1.1")
-    """:external:ref:`core-metadata-classifier`"""
-    requires_dist: _Validator[List[requirements.Requirement]] = _Validator(added="1.2")
-    """:external:ref:`core-metadata-requires-dist`"""
-    requires_python: _Validator[specifiers.SpecifierSet] = _Validator(added="1.2")
-    """:external:ref:`core-metadata-requires-python`"""
-    # Because `Requires-External` allows for non-PEP 440 version specifiers, we
-    # don't do any processing on the values.
-    requires_external: _Validator[List[str]] = _Validator(added="1.2")
-    """:external:ref:`core-metadata-requires-external`"""
-    project_urls: _Validator[Dict[str, str]] = _Validator(added="1.2")
-    """:external:ref:`core-metadata-project-url`"""
-    # PEP 685 lets us raise an error if an extra doesn't pass `Name` validation
-    # regardless of metadata version.
-    provides_extra: _Validator[List[utils.NormalizedName]] = _Validator(
-        added="2.1",
-    )
-    """:external:ref:`core-metadata-provides-extra`"""
-    provides_dist: _Validator[List[str]] = _Validator(added="1.2")
-    """:external:ref:`core-metadata-provides-dist`"""
-    obsoletes_dist: _Validator[List[str]] = _Validator(added="1.2")
-    """:external:ref:`core-metadata-obsoletes-dist`"""
-    requires: _Validator[List[str]] = _Validator(added="1.1")
-    """``Requires`` (deprecated)"""
-    provides: _Validator[List[str]] = _Validator(added="1.1")
-    """``Provides`` (deprecated)"""
-    obsoletes: _Validator[List[str]] = _Validator(added="1.1")
-    """``Obsoletes`` (deprecated)"""
diff --git a/src/poetry/core/_vendor/packaging/py.typed b/src/poetry/core/_vendor/packaging/py.typed
deleted file mode 100644
index e69de29..0000000
diff --git a/src/poetry/core/_vendor/packaging/requirements.py b/src/poetry/core/_vendor/packaging/requirements.py
deleted file mode 100644
index 0c00eba..0000000
--- a/src/poetry/core/_vendor/packaging/requirements.py
+++ /dev/null
@@ -1,90 +0,0 @@
-# This file is dual licensed under the terms of the Apache License, Version
-# 2.0, and the BSD License. See the LICENSE file in the root of this repository
-# for complete details.
-
-from typing import Any, Iterator, Optional, Set
-
-from ._parser import parse_requirement as _parse_requirement
-from ._tokenizer import ParserSyntaxError
-from .markers import Marker, _normalize_extra_values
-from .specifiers import SpecifierSet
-from .utils import canonicalize_name
-
-
-class InvalidRequirement(ValueError):
-    """
-    An invalid requirement was found, users should refer to PEP 508.
-    """
-
-
-class Requirement:
-    """Parse a requirement.
-
-    Parse a given requirement string into its parts, such as name, specifier,
-    URL, and extras. Raises InvalidRequirement on a badly-formed requirement
-    string.
-    """
-
-    # TODO: Can we test whether something is contained within a requirement?
-    #       If so how do we do that? Do we need to test against the _name_ of
-    #       the thing as well as the version? What about the markers?
-    # TODO: Can we normalize the name and extra name?
-
-    def __init__(self, requirement_string: str) -> None:
-        try:
-            parsed = _parse_requirement(requirement_string)
-        except ParserSyntaxError as e:
-            raise InvalidRequirement(str(e)) from e
-
-        self.name: str = parsed.name
-        self.url: Optional[str] = parsed.url or None
-        self.extras: Set[str] = set(parsed.extras if parsed.extras else [])
-        self.specifier: SpecifierSet = SpecifierSet(parsed.specifier)
-        self.marker: Optional[Marker] = None
-        if parsed.marker is not None:
-            self.marker = Marker.__new__(Marker)
-            self.marker._markers = _normalize_extra_values(parsed.marker)
-
-    def _iter_parts(self, name: str) -> Iterator[str]:
-        yield name
-
-        if self.extras:
-            formatted_extras = ",".join(sorted(self.extras))
-            yield f"[{formatted_extras}]"
-
-        if self.specifier:
-            yield str(self.specifier)
-
-        if self.url:
-            yield f"@ {self.url}"
-            if self.marker:
-                yield " "
-
-        if self.marker:
-            yield f"; {self.marker}"
-
-    def __str__(self) -> str:
-        return "".join(self._iter_parts(self.name))
-
-    def __repr__(self) -> str:
-        return f"<Requirement('{self}')>"
-
-    def __hash__(self) -> int:
-        return hash(
-            (
-                self.__class__.__name__,
-                *self._iter_parts(canonicalize_name(self.name)),
-            )
-        )
-
-    def __eq__(self, other: Any) -> bool:
-        if not isinstance(other, Requirement):
-            return NotImplemented
-
-        return (
-            canonicalize_name(self.name) == canonicalize_name(other.name)
-            and self.extras == other.extras
-            and self.specifier == other.specifier
-            and self.url == other.url
-            and self.marker == other.marker
-        )
diff --git a/src/poetry/core/_vendor/packaging/specifiers.py b/src/poetry/core/_vendor/packaging/specifiers.py
deleted file mode 100644
index ba8fe37..0000000
--- a/src/poetry/core/_vendor/packaging/specifiers.py
+++ /dev/null
@@ -1,1008 +0,0 @@
-# This file is dual licensed under the terms of the Apache License, Version
-# 2.0, and the BSD License. See the LICENSE file in the root of this repository
-# for complete details.
-"""
-.. testsetup::
-
-    from packaging.specifiers import Specifier, SpecifierSet, InvalidSpecifier
-    from packaging.version import Version
-"""
-
-import abc
-import itertools
-import re
-from typing import (
-    Callable,
-    Iterable,
-    Iterator,
-    List,
-    Optional,
-    Set,
-    Tuple,
-    TypeVar,
-    Union,
-)
-
-from .utils import canonicalize_version
-from .version import Version
-
-UnparsedVersion = Union[Version, str]
-UnparsedVersionVar = TypeVar("UnparsedVersionVar", bound=UnparsedVersion)
-CallableOperator = Callable[[Version, str], bool]
-
-
-def _coerce_version(version: UnparsedVersion) -> Version:
-    if not isinstance(version, Version):
-        version = Version(version)
-    return version
-
-
-class InvalidSpecifier(ValueError):
-    """
-    Raised when attempting to create a :class:`Specifier` with a specifier
-    string that is invalid.
-
-    >>> Specifier("lolwat")
-    Traceback (most recent call last):
-        ...
-    packaging.specifiers.InvalidSpecifier: Invalid specifier: 'lolwat'
-    """
-
-
-class BaseSpecifier(metaclass=abc.ABCMeta):
-    @abc.abstractmethod
-    def __str__(self) -> str:
-        """
-        Returns the str representation of this Specifier-like object. This
-        should be representative of the Specifier itself.
-        """
-
-    @abc.abstractmethod
-    def __hash__(self) -> int:
-        """
-        Returns a hash value for this Specifier-like object.
-        """
-
-    @abc.abstractmethod
-    def __eq__(self, other: object) -> bool:
-        """
-        Returns a boolean representing whether or not the two Specifier-like
-        objects are equal.
-
-        :param other: The other object to check against.
-        """
-
-    @property
-    @abc.abstractmethod
-    def prereleases(self) -> Optional[bool]:
-        """Whether or not pre-releases as a whole are allowed.
-
-        This can be set to either ``True`` or ``False`` to explicitly enable or disable
-        prereleases or it can be set to ``None`` (the default) to use default semantics.
-        """
-
-    @prereleases.setter
-    def prereleases(self, value: bool) -> None:
-        """Setter for :attr:`prereleases`.
-
-        :param value: The value to set.
-        """
-
-    @abc.abstractmethod
-    def contains(self, item: str, prereleases: Optional[bool] = None) -> bool:
-        """
-        Determines if the given item is contained within this specifier.
-        """
-
-    @abc.abstractmethod
-    def filter(
-        self, iterable: Iterable[UnparsedVersionVar], prereleases: Optional[bool] = None
-    ) -> Iterator[UnparsedVersionVar]:
-        """
-        Takes an iterable of items and filters them so that only items which
-        are contained within this specifier are allowed in it.
-        """
-
-
-class Specifier(BaseSpecifier):
-    """This class abstracts handling of version specifiers.
-
-    .. tip::
-
-        It is generally not required to instantiate this manually. You should instead
-        prefer to work with :class:`SpecifierSet` instead, which can parse
-        comma-separated version specifiers (which is what package metadata contains).
-    """
-
-    _operator_regex_str = r"""
-        (?P<operator>(~=|==|!=|<=|>=|<|>|===))
-        """
-    _version_regex_str = r"""
-        (?P<version>
-            (?:
-                # The identity operators allow for an escape hatch that will
-                # do an exact string match of the version you wish to install.
-                # This will not be parsed by PEP 440 and we cannot determine
-                # any semantic meaning from it. This operator is discouraged
-                # but included entirely as an escape hatch.
-                (?<====)  # Only match for the identity operator
-                \s*
-                [^\s;)]*  # The arbitrary version can be just about anything,
-                          # we match everything except for whitespace, a
-                          # semi-colon for marker support, and a closing paren
-                          # since versions can be enclosed in them.
-            )
-            |
-            (?:
-                # The (non)equality operators allow for wild card and local
-                # versions to be specified so we have to define these two
-                # operators separately to enable that.
-                (?<===|!=)            # Only match for equals and not equals
-
-                \s*
-                v?
-                (?:[0-9]+!)?          # epoch
-                [0-9]+(?:\.[0-9]+)*   # release
-
-                # You cannot use a wild card and a pre-release, post-release, a dev or
-                # local version together so group them with a | and make them optional.
-                (?:
-                    \.\*  # Wild card syntax of .*
-                    |
-                    (?:                                  # pre release
-                        [-_\.]?
-                        (alpha|beta|preview|pre|a|b|c|rc)
-                        [-_\.]?
-                        [0-9]*
-                    )?
-                    (?:                                  # post release
-                        (?:-[0-9]+)|(?:[-_\.]?(post|rev|r)[-_\.]?[0-9]*)
-                    )?
-                    (?:[-_\.]?dev[-_\.]?[0-9]*)?         # dev release
-                    (?:\+[a-z0-9]+(?:[-_\.][a-z0-9]+)*)? # local
-                )?
-            )
-            |
-            (?:
-                # The compatible operator requires at least two digits in the
-                # release segment.
-                (?<=~=)               # Only match for the compatible operator
-
-                \s*
-                v?
-                (?:[0-9]+!)?          # epoch
-                [0-9]+(?:\.[0-9]+)+   # release  (We have a + instead of a *)
-                (?:                   # pre release
-                    [-_\.]?
-                    (alpha|beta|preview|pre|a|b|c|rc)
-                    [-_\.]?
-                    [0-9]*
-                )?
-                (?:                                   # post release
-                    (?:-[0-9]+)|(?:[-_\.]?(post|rev|r)[-_\.]?[0-9]*)
-                )?
-                (?:[-_\.]?dev[-_\.]?[0-9]*)?          # dev release
-            )
-            |
-            (?:
-                # All other operators only allow a sub set of what the
-                # (non)equality operators do. Specifically they do not allow
-                # local versions to be specified nor do they allow the prefix
-                # matching wild cards.
-                (?<!==|!=|~=)         # We have special cases for these
-                                      # operators so we want to make sure they
-                                      # don't match here.
-
-                \s*
-                v?
-                (?:[0-9]+!)?          # epoch
-                [0-9]+(?:\.[0-9]+)*   # release
-                (?:                   # pre release
-                    [-_\.]?
-                    (alpha|beta|preview|pre|a|b|c|rc)
-                    [-_\.]?
-                    [0-9]*
-                )?
-                (?:                                   # post release
-                    (?:-[0-9]+)|(?:[-_\.]?(post|rev|r)[-_\.]?[0-9]*)
-                )?
-                (?:[-_\.]?dev[-_\.]?[0-9]*)?          # dev release
-            )
-        )
-        """
-
-    _regex = re.compile(
-        r"^\s*" + _operator_regex_str + _version_regex_str + r"\s*$",
-        re.VERBOSE | re.IGNORECASE,
-    )
-
-    _operators = {
-        "~=": "compatible",
-        "==": "equal",
-        "!=": "not_equal",
-        "<=": "less_than_equal",
-        ">=": "greater_than_equal",
-        "<": "less_than",
-        ">": "greater_than",
-        "===": "arbitrary",
-    }
-
-    def __init__(self, spec: str = "", prereleases: Optional[bool] = None) -> None:
-        """Initialize a Specifier instance.
-
-        :param spec:
-            The string representation of a specifier which will be parsed and
-            normalized before use.
-        :param prereleases:
-            This tells the specifier if it should accept prerelease versions if
-            applicable or not. The default of ``None`` will autodetect it from the
-            given specifiers.
-        :raises InvalidSpecifier:
-            If the given specifier is invalid (i.e. bad syntax).
-        """
-        match = self._regex.search(spec)
-        if not match:
-            raise InvalidSpecifier(f"Invalid specifier: '{spec}'")
-
-        self._spec: Tuple[str, str] = (
-            match.group("operator").strip(),
-            match.group("version").strip(),
-        )
-
-        # Store whether or not this Specifier should accept prereleases
-        self._prereleases = prereleases
-
-    # https://github.com/python/mypy/pull/13475#pullrequestreview-1079784515
-    @property  # type: ignore[override]
-    def prereleases(self) -> bool:
-        # If there is an explicit prereleases set for this, then we'll just
-        # blindly use that.
-        if self._prereleases is not None:
-            return self._prereleases
-
-        # Look at all of our specifiers and determine if they are inclusive
-        # operators, and if they are if they are including an explicit
-        # prerelease.
-        operator, version = self._spec
-        if operator in ["==", ">=", "<=", "~=", "==="]:
-            # The == specifier can include a trailing .*, if it does we
-            # want to remove before parsing.
-            if operator == "==" and version.endswith(".*"):
-                version = version[:-2]
-
-            # Parse the version, and if it is a pre-release than this
-            # specifier allows pre-releases.
-            if Version(version).is_prerelease:
-                return True
-
-        return False
-
-    @prereleases.setter
-    def prereleases(self, value: bool) -> None:
-        self._prereleases = value
-
-    @property
-    def operator(self) -> str:
-        """The operator of this specifier.
-
-        >>> Specifier("==1.2.3").operator
-        '=='
-        """
-        return self._spec[0]
-
-    @property
-    def version(self) -> str:
-        """The version of this specifier.
-
-        >>> Specifier("==1.2.3").version
-        '1.2.3'
-        """
-        return self._spec[1]
-
-    def __repr__(self) -> str:
-        """A representation of the Specifier that shows all internal state.
-
-        >>> Specifier('>=1.0.0')
-        <Specifier('>=1.0.0')>
-        >>> Specifier('>=1.0.0', prereleases=False)
-        <Specifier('>=1.0.0', prereleases=False)>
-        >>> Specifier('>=1.0.0', prereleases=True)
-        <Specifier('>=1.0.0', prereleases=True)>
-        """
-        pre = (
-            f", prereleases={self.prereleases!r}"
-            if self._prereleases is not None
-            else ""
-        )
-
-        return f"<{self.__class__.__name__}({str(self)!r}{pre})>"
-
-    def __str__(self) -> str:
-        """A string representation of the Specifier that can be round-tripped.
-
-        >>> str(Specifier('>=1.0.0'))
-        '>=1.0.0'
-        >>> str(Specifier('>=1.0.0', prereleases=False))
-        '>=1.0.0'
-        """
-        return "{}{}".format(*self._spec)
-
-    @property
-    def _canonical_spec(self) -> Tuple[str, str]:
-        canonical_version = canonicalize_version(
-            self._spec[1],
-            strip_trailing_zero=(self._spec[0] != "~="),
-        )
-        return self._spec[0], canonical_version
-
-    def __hash__(self) -> int:
-        return hash(self._canonical_spec)
-
-    def __eq__(self, other: object) -> bool:
-        """Whether or not the two Specifier-like objects are equal.
-
-        :param other: The other object to check against.
-
-        The value of :attr:`prereleases` is ignored.
-
-        >>> Specifier("==1.2.3") == Specifier("== 1.2.3.0")
-        True
-        >>> (Specifier("==1.2.3", prereleases=False) ==
-        ...  Specifier("==1.2.3", prereleases=True))
-        True
-        >>> Specifier("==1.2.3") == "==1.2.3"
-        True
-        >>> Specifier("==1.2.3") == Specifier("==1.2.4")
-        False
-        >>> Specifier("==1.2.3") == Specifier("~=1.2.3")
-        False
-        """
-        if isinstance(other, str):
-            try:
-                other = self.__class__(str(other))
-            except InvalidSpecifier:
-                return NotImplemented
-        elif not isinstance(other, self.__class__):
-            return NotImplemented
-
-        return self._canonical_spec == other._canonical_spec
-
-    def _get_operator(self, op: str) -> CallableOperator:
-        operator_callable: CallableOperator = getattr(
-            self, f"_compare_{self._operators[op]}"
-        )
-        return operator_callable
-
-    def _compare_compatible(self, prospective: Version, spec: str) -> bool:
-
-        # Compatible releases have an equivalent combination of >= and ==. That
-        # is that ~=2.2 is equivalent to >=2.2,==2.*. This allows us to
-        # implement this in terms of the other specifiers instead of
-        # implementing it ourselves. The only thing we need to do is construct
-        # the other specifiers.
-
-        # We want everything but the last item in the version, but we want to
-        # ignore suffix segments.
-        prefix = ".".join(
-            list(itertools.takewhile(_is_not_suffix, _version_split(spec)))[:-1]
-        )
-
-        # Add the prefix notation to the end of our string
-        prefix += ".*"
-
-        return self._get_operator(">=")(prospective, spec) and self._get_operator("==")(
-            prospective, prefix
-        )
-
-    def _compare_equal(self, prospective: Version, spec: str) -> bool:
-
-        # We need special logic to handle prefix matching
-        if spec.endswith(".*"):
-            # In the case of prefix matching we want to ignore local segment.
-            normalized_prospective = canonicalize_version(
-                prospective.public, strip_trailing_zero=False
-            )
-            # Get the normalized version string ignoring the trailing .*
-            normalized_spec = canonicalize_version(spec[:-2], strip_trailing_zero=False)
-            # Split the spec out by dots, and pretend that there is an implicit
-            # dot in between a release segment and a pre-release segment.
-            split_spec = _version_split(normalized_spec)
-
-            # Split the prospective version out by dots, and pretend that there
-            # is an implicit dot in between a release segment and a pre-release
-            # segment.
-            split_prospective = _version_split(normalized_prospective)
-
-            # 0-pad the prospective version before shortening it to get the correct
-            # shortened version.
-            padded_prospective, _ = _pad_version(split_prospective, split_spec)
-
-            # Shorten the prospective version to be the same length as the spec
-            # so that we can determine if the specifier is a prefix of the
-            # prospective version or not.
-            shortened_prospective = padded_prospective[: len(split_spec)]
-
-            return shortened_prospective == split_spec
-        else:
-            # Convert our spec string into a Version
-            spec_version = Version(spec)
-
-            # If the specifier does not have a local segment, then we want to
-            # act as if the prospective version also does not have a local
-            # segment.
-            if not spec_version.local:
-                prospective = Version(prospective.public)
-
-            return prospective == spec_version
-
-    def _compare_not_equal(self, prospective: Version, spec: str) -> bool:
-        return not self._compare_equal(prospective, spec)
-
-    def _compare_less_than_equal(self, prospective: Version, spec: str) -> bool:
-
-        # NB: Local version identifiers are NOT permitted in the version
-        # specifier, so local version labels can be universally removed from
-        # the prospective version.
-        return Version(prospective.public) <= Version(spec)
-
-    def _compare_greater_than_equal(self, prospective: Version, spec: str) -> bool:
-
-        # NB: Local version identifiers are NOT permitted in the version
-        # specifier, so local version labels can be universally removed from
-        # the prospective version.
-        return Version(prospective.public) >= Version(spec)
-
-    def _compare_less_than(self, prospective: Version, spec_str: str) -> bool:
-
-        # Convert our spec to a Version instance, since we'll want to work with
-        # it as a version.
-        spec = Version(spec_str)
-
-        # Check to see if the prospective version is less than the spec
-        # version. If it's not we can short circuit and just return False now
-        # instead of doing extra unneeded work.
-        if not prospective < spec:
-            return False
-
-        # This special case is here so that, unless the specifier itself
-        # includes is a pre-release version, that we do not accept pre-release
-        # versions for the version mentioned in the specifier (e.g. <3.1 should
-        # not match 3.1.dev0, but should match 3.0.dev0).
-        if not spec.is_prerelease and prospective.is_prerelease:
-            if Version(prospective.base_version) == Version(spec.base_version):
-                return False
-
-        # If we've gotten to here, it means that prospective version is both
-        # less than the spec version *and* it's not a pre-release of the same
-        # version in the spec.
-        return True
-
-    def _compare_greater_than(self, prospective: Version, spec_str: str) -> bool:
-
-        # Convert our spec to a Version instance, since we'll want to work with
-        # it as a version.
-        spec = Version(spec_str)
-
-        # Check to see if the prospective version is greater than the spec
-        # version. If it's not we can short circuit and just return False now
-        # instead of doing extra unneeded work.
-        if not prospective > spec:
-            return False
-
-        # This special case is here so that, unless the specifier itself
-        # includes is a post-release version, that we do not accept
-        # post-release versions for the version mentioned in the specifier
-        # (e.g. >3.1 should not match 3.0.post0, but should match 3.2.post0).
-        if not spec.is_postrelease and prospective.is_postrelease:
-            if Version(prospective.base_version) == Version(spec.base_version):
-                return False
-
-        # Ensure that we do not allow a local version of the version mentioned
-        # in the specifier, which is technically greater than, to match.
-        if prospective.local is not None:
-            if Version(prospective.base_version) == Version(spec.base_version):
-                return False
-
-        # If we've gotten to here, it means that prospective version is both
-        # greater than the spec version *and* it's not a pre-release of the
-        # same version in the spec.
-        return True
-
-    def _compare_arbitrary(self, prospective: Version, spec: str) -> bool:
-        return str(prospective).lower() == str(spec).lower()
-
-    def __contains__(self, item: Union[str, Version]) -> bool:
-        """Return whether or not the item is contained in this specifier.
-
-        :param item: The item to check for.
-
-        This is used for the ``in`` operator and behaves the same as
-        :meth:`contains` with no ``prereleases`` argument passed.
-
-        >>> "1.2.3" in Specifier(">=1.2.3")
-        True
-        >>> Version("1.2.3") in Specifier(">=1.2.3")
-        True
-        >>> "1.0.0" in Specifier(">=1.2.3")
-        False
-        >>> "1.3.0a1" in Specifier(">=1.2.3")
-        False
-        >>> "1.3.0a1" in Specifier(">=1.2.3", prereleases=True)
-        True
-        """
-        return self.contains(item)
-
-    def contains(
-        self, item: UnparsedVersion, prereleases: Optional[bool] = None
-    ) -> bool:
-        """Return whether or not the item is contained in this specifier.
-
-        :param item:
-            The item to check for, which can be a version string or a
-            :class:`Version` instance.
-        :param prereleases:
-            Whether or not to match prereleases with this Specifier. If set to
-            ``None`` (the default), it uses :attr:`prereleases` to determine
-            whether or not prereleases are allowed.
-
-        >>> Specifier(">=1.2.3").contains("1.2.3")
-        True
-        >>> Specifier(">=1.2.3").contains(Version("1.2.3"))
-        True
-        >>> Specifier(">=1.2.3").contains("1.0.0")
-        False
-        >>> Specifier(">=1.2.3").contains("1.3.0a1")
-        False
-        >>> Specifier(">=1.2.3", prereleases=True).contains("1.3.0a1")
-        True
-        >>> Specifier(">=1.2.3").contains("1.3.0a1", prereleases=True)
-        True
-        """
-
-        # Determine if prereleases are to be allowed or not.
-        if prereleases is None:
-            prereleases = self.prereleases
-
-        # Normalize item to a Version, this allows us to have a shortcut for
-        # "2.0" in Specifier(">=2")
-        normalized_item = _coerce_version(item)
-
-        # Determine if we should be supporting prereleases in this specifier
-        # or not, if we do not support prereleases than we can short circuit
-        # logic if this version is a prereleases.
-        if normalized_item.is_prerelease and not prereleases:
-            return False
-
-        # Actually do the comparison to determine if this item is contained
-        # within this Specifier or not.
-        operator_callable: CallableOperator = self._get_operator(self.operator)
-        return operator_callable(normalized_item, self.version)
-
-    def filter(
-        self, iterable: Iterable[UnparsedVersionVar], prereleases: Optional[bool] = None
-    ) -> Iterator[UnparsedVersionVar]:
-        """Filter items in the given iterable, that match the specifier.
-
-        :param iterable:
-            An iterable that can contain version strings and :class:`Version` instances.
-            The items in the iterable will be filtered according to the specifier.
-        :param prereleases:
-            Whether or not to allow prereleases in the returned iterator. If set to
-            ``None`` (the default), it will be intelligently decide whether to allow
-            prereleases or not (based on the :attr:`prereleases` attribute, and
-            whether the only versions matching are prereleases).
-
-        This method is smarter than just ``filter(Specifier().contains, [...])``
-        because it implements the rule from :pep:`440` that a prerelease item
-        SHOULD be accepted if no other versions match the given specifier.
-
-        >>> list(Specifier(">=1.2.3").filter(["1.2", "1.3", "1.5a1"]))
-        ['1.3']
-        >>> list(Specifier(">=1.2.3").filter(["1.2", "1.2.3", "1.3", Version("1.4")]))
-        ['1.2.3', '1.3', <Version('1.4')>]
-        >>> list(Specifier(">=1.2.3").filter(["1.2", "1.5a1"]))
-        ['1.5a1']
-        >>> list(Specifier(">=1.2.3").filter(["1.3", "1.5a1"], prereleases=True))
-        ['1.3', '1.5a1']
-        >>> list(Specifier(">=1.2.3", prereleases=True).filter(["1.3", "1.5a1"]))
-        ['1.3', '1.5a1']
-        """
-
-        yielded = False
-        found_prereleases = []
-
-        kw = {"prereleases": prereleases if prereleases is not None else True}
-
-        # Attempt to iterate over all the values in the iterable and if any of
-        # them match, yield them.
-        for version in iterable:
-            parsed_version = _coerce_version(version)
-
-            if self.contains(parsed_version, **kw):
-                # If our version is a prerelease, and we were not set to allow
-                # prereleases, then we'll store it for later in case nothing
-                # else matches this specifier.
-                if parsed_version.is_prerelease and not (
-                    prereleases or self.prereleases
-                ):
-                    found_prereleases.append(version)
-                # Either this is not a prerelease, or we should have been
-                # accepting prereleases from the beginning.
-                else:
-                    yielded = True
-                    yield version
-
-        # Now that we've iterated over everything, determine if we've yielded
-        # any values, and if we have not and we have any prereleases stored up
-        # then we will go ahead and yield the prereleases.
-        if not yielded and found_prereleases:
-            for version in found_prereleases:
-                yield version
-
-
-_prefix_regex = re.compile(r"^([0-9]+)((?:a|b|c|rc)[0-9]+)$")
-
-
-def _version_split(version: str) -> List[str]:
-    result: List[str] = []
-    for item in version.split("."):
-        match = _prefix_regex.search(item)
-        if match:
-            result.extend(match.groups())
-        else:
-            result.append(item)
-    return result
-
-
-def _is_not_suffix(segment: str) -> bool:
-    return not any(
-        segment.startswith(prefix) for prefix in ("dev", "a", "b", "rc", "post")
-    )
-
-
-def _pad_version(left: List[str], right: List[str]) -> Tuple[List[str], List[str]]:
-    left_split, right_split = [], []
-
-    # Get the release segment of our versions
-    left_split.append(list(itertools.takewhile(lambda x: x.isdigit(), left)))
-    right_split.append(list(itertools.takewhile(lambda x: x.isdigit(), right)))
-
-    # Get the rest of our versions
-    left_split.append(left[len(left_split[0]) :])
-    right_split.append(right[len(right_split[0]) :])
-
-    # Insert our padding
-    left_split.insert(1, ["0"] * max(0, len(right_split[0]) - len(left_split[0])))
-    right_split.insert(1, ["0"] * max(0, len(left_split[0]) - len(right_split[0])))
-
-    return (list(itertools.chain(*left_split)), list(itertools.chain(*right_split)))
-
-
-class SpecifierSet(BaseSpecifier):
-    """This class abstracts handling of a set of version specifiers.
-
-    It can be passed a single specifier (``>=3.0``), a comma-separated list of
-    specifiers (``>=3.0,!=3.1``), or no specifier at all.
-    """
-
-    def __init__(
-        self, specifiers: str = "", prereleases: Optional[bool] = None
-    ) -> None:
-        """Initialize a SpecifierSet instance.
-
-        :param specifiers:
-            The string representation of a specifier or a comma-separated list of
-            specifiers which will be parsed and normalized before use.
-        :param prereleases:
-            This tells the SpecifierSet if it should accept prerelease versions if
-            applicable or not. The default of ``None`` will autodetect it from the
-            given specifiers.
-
-        :raises InvalidSpecifier:
-            If the given ``specifiers`` are not parseable than this exception will be
-            raised.
-        """
-
-        # Split on `,` to break each individual specifier into it's own item, and
-        # strip each item to remove leading/trailing whitespace.
-        split_specifiers = [s.strip() for s in specifiers.split(",") if s.strip()]
-
-        # Parsed each individual specifier, attempting first to make it a
-        # Specifier.
-        parsed: Set[Specifier] = set()
-        for specifier in split_specifiers:
-            parsed.add(Specifier(specifier))
-
-        # Turn our parsed specifiers into a frozen set and save them for later.
-        self._specs = frozenset(parsed)
-
-        # Store our prereleases value so we can use it later to determine if
-        # we accept prereleases or not.
-        self._prereleases = prereleases
-
-    @property
-    def prereleases(self) -> Optional[bool]:
-        # If we have been given an explicit prerelease modifier, then we'll
-        # pass that through here.
-        if self._prereleases is not None:
-            return self._prereleases
-
-        # If we don't have any specifiers, and we don't have a forced value,
-        # then we'll just return None since we don't know if this should have
-        # pre-releases or not.
-        if not self._specs:
-            return None
-
-        # Otherwise we'll see if any of the given specifiers accept
-        # prereleases, if any of them do we'll return True, otherwise False.
-        return any(s.prereleases for s in self._specs)
-
-    @prereleases.setter
-    def prereleases(self, value: bool) -> None:
-        self._prereleases = value
-
-    def __repr__(self) -> str:
-        """A representation of the specifier set that shows all internal state.
-
-        Note that the ordering of the individual specifiers within the set may not
-        match the input string.
-
-        >>> SpecifierSet('>=1.0.0,!=2.0.0')
-        <SpecifierSet('!=2.0.0,>=1.0.0')>
-        >>> SpecifierSet('>=1.0.0,!=2.0.0', prereleases=False)
-        <SpecifierSet('!=2.0.0,>=1.0.0', prereleases=False)>
-        >>> SpecifierSet('>=1.0.0,!=2.0.0', prereleases=True)
-        <SpecifierSet('!=2.0.0,>=1.0.0', prereleases=True)>
-        """
-        pre = (
-            f", prereleases={self.prereleases!r}"
-            if self._prereleases is not None
-            else ""
-        )
-
-        return f"<SpecifierSet({str(self)!r}{pre})>"
-
-    def __str__(self) -> str:
-        """A string representation of the specifier set that can be round-tripped.
-
-        Note that the ordering of the individual specifiers within the set may not
-        match the input string.
-
-        >>> str(SpecifierSet(">=1.0.0,!=1.0.1"))
-        '!=1.0.1,>=1.0.0'
-        >>> str(SpecifierSet(">=1.0.0,!=1.0.1", prereleases=False))
-        '!=1.0.1,>=1.0.0'
-        """
-        return ",".join(sorted(str(s) for s in self._specs))
-
-    def __hash__(self) -> int:
-        return hash(self._specs)
-
-    def __and__(self, other: Union["SpecifierSet", str]) -> "SpecifierSet":
-        """Return a SpecifierSet which is a combination of the two sets.
-
-        :param other: The other object to combine with.
-
-        >>> SpecifierSet(">=1.0.0,!=1.0.1") & '<=2.0.0,!=2.0.1'
-        <SpecifierSet('!=1.0.1,!=2.0.1,<=2.0.0,>=1.0.0')>
-        >>> SpecifierSet(">=1.0.0,!=1.0.1") & SpecifierSet('<=2.0.0,!=2.0.1')
-        <SpecifierSet('!=1.0.1,!=2.0.1,<=2.0.0,>=1.0.0')>
-        """
-        if isinstance(other, str):
-            other = SpecifierSet(other)
-        elif not isinstance(other, SpecifierSet):
-            return NotImplemented
-
-        specifier = SpecifierSet()
-        specifier._specs = frozenset(self._specs | other._specs)
-
-        if self._prereleases is None and other._prereleases is not None:
-            specifier._prereleases = other._prereleases
-        elif self._prereleases is not None and other._prereleases is None:
-            specifier._prereleases = self._prereleases
-        elif self._prereleases == other._prereleases:
-            specifier._prereleases = self._prereleases
-        else:
-            raise ValueError(
-                "Cannot combine SpecifierSets with True and False prerelease "
-                "overrides."
-            )
-
-        return specifier
-
-    def __eq__(self, other: object) -> bool:
-        """Whether or not the two SpecifierSet-like objects are equal.
-
-        :param other: The other object to check against.
-
-        The value of :attr:`prereleases` is ignored.
-
-        >>> SpecifierSet(">=1.0.0,!=1.0.1") == SpecifierSet(">=1.0.0,!=1.0.1")
-        True
-        >>> (SpecifierSet(">=1.0.0,!=1.0.1", prereleases=False) ==
-        ...  SpecifierSet(">=1.0.0,!=1.0.1", prereleases=True))
-        True
-        >>> SpecifierSet(">=1.0.0,!=1.0.1") == ">=1.0.0,!=1.0.1"
-        True
-        >>> SpecifierSet(">=1.0.0,!=1.0.1") == SpecifierSet(">=1.0.0")
-        False
-        >>> SpecifierSet(">=1.0.0,!=1.0.1") == SpecifierSet(">=1.0.0,!=1.0.2")
-        False
-        """
-        if isinstance(other, (str, Specifier)):
-            other = SpecifierSet(str(other))
-        elif not isinstance(other, SpecifierSet):
-            return NotImplemented
-
-        return self._specs == other._specs
-
-    def __len__(self) -> int:
-        """Returns the number of specifiers in this specifier set."""
-        return len(self._specs)
-
-    def __iter__(self) -> Iterator[Specifier]:
-        """
-        Returns an iterator over all the underlying :class:`Specifier` instances
-        in this specifier set.
-
-        >>> sorted(SpecifierSet(">=1.0.0,!=1.0.1"), key=str)
-        [<Specifier('!=1.0.1')>, <Specifier('>=1.0.0')>]
-        """
-        return iter(self._specs)
-
-    def __contains__(self, item: UnparsedVersion) -> bool:
-        """Return whether or not the item is contained in this specifier.
-
-        :param item: The item to check for.
-
-        This is used for the ``in`` operator and behaves the same as
-        :meth:`contains` with no ``prereleases`` argument passed.
-
-        >>> "1.2.3" in SpecifierSet(">=1.0.0,!=1.0.1")
-        True
-        >>> Version("1.2.3") in SpecifierSet(">=1.0.0,!=1.0.1")
-        True
-        >>> "1.0.1" in SpecifierSet(">=1.0.0,!=1.0.1")
-        False
-        >>> "1.3.0a1" in SpecifierSet(">=1.0.0,!=1.0.1")
-        False
-        >>> "1.3.0a1" in SpecifierSet(">=1.0.0,!=1.0.1", prereleases=True)
-        True
-        """
-        return self.contains(item)
-
-    def contains(
-        self,
-        item: UnparsedVersion,
-        prereleases: Optional[bool] = None,
-        installed: Optional[bool] = None,
-    ) -> bool:
-        """Return whether or not the item is contained in this SpecifierSet.
-
-        :param item:
-            The item to check for, which can be a version string or a
-            :class:`Version` instance.
-        :param prereleases:
-            Whether or not to match prereleases with this SpecifierSet. If set to
-            ``None`` (the default), it uses :attr:`prereleases` to determine
-            whether or not prereleases are allowed.
-
-        >>> SpecifierSet(">=1.0.0,!=1.0.1").contains("1.2.3")
-        True
-        >>> SpecifierSet(">=1.0.0,!=1.0.1").contains(Version("1.2.3"))
-        True
-        >>> SpecifierSet(">=1.0.0,!=1.0.1").contains("1.0.1")
-        False
-        >>> SpecifierSet(">=1.0.0,!=1.0.1").contains("1.3.0a1")
-        False
-        >>> SpecifierSet(">=1.0.0,!=1.0.1", prereleases=True).contains("1.3.0a1")
-        True
-        >>> SpecifierSet(">=1.0.0,!=1.0.1").contains("1.3.0a1", prereleases=True)
-        True
-        """
-        # Ensure that our item is a Version instance.
-        if not isinstance(item, Version):
-            item = Version(item)
-
-        # Determine if we're forcing a prerelease or not, if we're not forcing
-        # one for this particular filter call, then we'll use whatever the
-        # SpecifierSet thinks for whether or not we should support prereleases.
-        if prereleases is None:
-            prereleases = self.prereleases
-
-        # We can determine if we're going to allow pre-releases by looking to
-        # see if any of the underlying items supports them. If none of them do
-        # and this item is a pre-release then we do not allow it and we can
-        # short circuit that here.
-        # Note: This means that 1.0.dev1 would not be contained in something
-        #       like >=1.0.devabc however it would be in >=1.0.debabc,>0.0.dev0
-        if not prereleases and item.is_prerelease:
-            return False
-
-        if installed and item.is_prerelease:
-            item = Version(item.base_version)
-
-        # We simply dispatch to the underlying specs here to make sure that the
-        # given version is contained within all of them.
-        # Note: This use of all() here means that an empty set of specifiers
-        #       will always return True, this is an explicit design decision.
-        return all(s.contains(item, prereleases=prereleases) for s in self._specs)
-
-    def filter(
-        self, iterable: Iterable[UnparsedVersionVar], prereleases: Optional[bool] = None
-    ) -> Iterator[UnparsedVersionVar]:
-        """Filter items in the given iterable, that match the specifiers in this set.
-
-        :param iterable:
-            An iterable that can contain version strings and :class:`Version` instances.
-            The items in the iterable will be filtered according to the specifier.
-        :param prereleases:
-            Whether or not to allow prereleases in the returned iterator. If set to
-            ``None`` (the default), it will be intelligently decide whether to allow
-            prereleases or not (based on the :attr:`prereleases` attribute, and
-            whether the only versions matching are prereleases).
-
-        This method is smarter than just ``filter(SpecifierSet(...).contains, [...])``
-        because it implements the rule from :pep:`440` that a prerelease item
-        SHOULD be accepted if no other versions match the given specifier.
-
-        >>> list(SpecifierSet(">=1.2.3").filter(["1.2", "1.3", "1.5a1"]))
-        ['1.3']
-        >>> list(SpecifierSet(">=1.2.3").filter(["1.2", "1.3", Version("1.4")]))
-        ['1.3', <Version('1.4')>]
-        >>> list(SpecifierSet(">=1.2.3").filter(["1.2", "1.5a1"]))
-        []
-        >>> list(SpecifierSet(">=1.2.3").filter(["1.3", "1.5a1"], prereleases=True))
-        ['1.3', '1.5a1']
-        >>> list(SpecifierSet(">=1.2.3", prereleases=True).filter(["1.3", "1.5a1"]))
-        ['1.3', '1.5a1']
-
-        An "empty" SpecifierSet will filter items based on the presence of prerelease
-        versions in the set.
-
-        >>> list(SpecifierSet("").filter(["1.3", "1.5a1"]))
-        ['1.3']
-        >>> list(SpecifierSet("").filter(["1.5a1"]))
-        ['1.5a1']
-        >>> list(SpecifierSet("", prereleases=True).filter(["1.3", "1.5a1"]))
-        ['1.3', '1.5a1']
-        >>> list(SpecifierSet("").filter(["1.3", "1.5a1"], prereleases=True))
-        ['1.3', '1.5a1']
-        """
-        # Determine if we're forcing a prerelease or not, if we're not forcing
-        # one for this particular filter call, then we'll use whatever the
-        # SpecifierSet thinks for whether or not we should support prereleases.
-        if prereleases is None:
-            prereleases = self.prereleases
-
-        # If we have any specifiers, then we want to wrap our iterable in the
-        # filter method for each one, this will act as a logical AND amongst
-        # each specifier.
-        if self._specs:
-            for spec in self._specs:
-                iterable = spec.filter(iterable, prereleases=bool(prereleases))
-            return iter(iterable)
-        # If we do not have any specifiers, then we need to have a rough filter
-        # which will filter out any pre-releases, unless there are no final
-        # releases.
-        else:
-            filtered: List[UnparsedVersionVar] = []
-            found_prereleases: List[UnparsedVersionVar] = []
-
-            for item in iterable:
-                parsed_version = _coerce_version(item)
-
-                # Store any item which is a pre-release for later unless we've
-                # already found a final version or we are accepting prereleases
-                if parsed_version.is_prerelease and not prereleases:
-                    if not filtered:
-                        found_prereleases.append(item)
-                else:
-                    filtered.append(item)
-
-            # If we've found no items except for pre-releases, then we'll go
-            # ahead and use the pre-releases
-            if not filtered and found_prereleases and prereleases is None:
-                return iter(found_prereleases)
-
-            return iter(filtered)
diff --git a/src/poetry/core/_vendor/packaging/tags.py b/src/poetry/core/_vendor/packaging/tags.py
deleted file mode 100644
index 37f33b1..0000000
--- a/src/poetry/core/_vendor/packaging/tags.py
+++ /dev/null
@@ -1,553 +0,0 @@
-# This file is dual licensed under the terms of the Apache License, Version
-# 2.0, and the BSD License. See the LICENSE file in the root of this repository
-# for complete details.
-
-import logging
-import platform
-import struct
-import subprocess
-import sys
-import sysconfig
-from importlib.machinery import EXTENSION_SUFFIXES
-from typing import (
-    Dict,
-    FrozenSet,
-    Iterable,
-    Iterator,
-    List,
-    Optional,
-    Sequence,
-    Tuple,
-    Union,
-    cast,
-)
-
-from . import _manylinux, _musllinux
-
-logger = logging.getLogger(__name__)
-
-PythonVersion = Sequence[int]
-MacVersion = Tuple[int, int]
-
-INTERPRETER_SHORT_NAMES: Dict[str, str] = {
-    "python": "py",  # Generic.
-    "cpython": "cp",
-    "pypy": "pp",
-    "ironpython": "ip",
-    "jython": "jy",
-}
-
-
-_32_BIT_INTERPRETER = struct.calcsize("P") == 4
-
-
-class Tag:
-    """
-    A representation of the tag triple for a wheel.
-
-    Instances are considered immutable and thus are hashable. Equality checking
-    is also supported.
-    """
-
-    __slots__ = ["_interpreter", "_abi", "_platform", "_hash"]
-
-    def __init__(self, interpreter: str, abi: str, platform: str) -> None:
-        self._interpreter = interpreter.lower()
-        self._abi = abi.lower()
-        self._platform = platform.lower()
-        # The __hash__ of every single element in a Set[Tag] will be evaluated each time
-        # that a set calls its `.disjoint()` method, which may be called hundreds of
-        # times when scanning a page of links for packages with tags matching that
-        # Set[Tag]. Pre-computing the value here produces significant speedups for
-        # downstream consumers.
-        self._hash = hash((self._interpreter, self._abi, self._platform))
-
-    @property
-    def interpreter(self) -> str:
-        return self._interpreter
-
-    @property
-    def abi(self) -> str:
-        return self._abi
-
-    @property
-    def platform(self) -> str:
-        return self._platform
-
-    def __eq__(self, other: object) -> bool:
-        if not isinstance(other, Tag):
-            return NotImplemented
-
-        return (
-            (self._hash == other._hash)  # Short-circuit ASAP for perf reasons.
-            and (self._platform == other._platform)
-            and (self._abi == other._abi)
-            and (self._interpreter == other._interpreter)
-        )
-
-    def __hash__(self) -> int:
-        return self._hash
-
-    def __str__(self) -> str:
-        return f"{self._interpreter}-{self._abi}-{self._platform}"
-
-    def __repr__(self) -> str:
-        return f"<{self} @ {id(self)}>"
-
-
-def parse_tag(tag: str) -> FrozenSet[Tag]:
-    """
-    Parses the provided tag (e.g. `py3-none-any`) into a frozenset of Tag instances.
-
-    Returning a set is required due to the possibility that the tag is a
-    compressed tag set.
-    """
-    tags = set()
-    interpreters, abis, platforms = tag.split("-")
-    for interpreter in interpreters.split("."):
-        for abi in abis.split("."):
-            for platform_ in platforms.split("."):
-                tags.add(Tag(interpreter, abi, platform_))
-    return frozenset(tags)
-
-
-def _get_config_var(name: str, warn: bool = False) -> Union[int, str, None]:
-    value: Union[int, str, None] = sysconfig.get_config_var(name)
-    if value is None and warn:
-        logger.debug(
-            "Config variable '%s' is unset, Python ABI tag may be incorrect", name
-        )
-    return value
-
-
-def _normalize_string(string: str) -> str:
-    return string.replace(".", "_").replace("-", "_").replace(" ", "_")
-
-
-def _abi3_applies(python_version: PythonVersion) -> bool:
-    """
-    Determine if the Python version supports abi3.
-
-    PEP 384 was first implemented in Python 3.2.
-    """
-    return len(python_version) > 1 and tuple(python_version) >= (3, 2)
-
-
-def _cpython_abis(py_version: PythonVersion, warn: bool = False) -> List[str]:
-    py_version = tuple(py_version)  # To allow for version comparison.
-    abis = []
-    version = _version_nodot(py_version[:2])
-    debug = pymalloc = ucs4 = ""
-    with_debug = _get_config_var("Py_DEBUG", warn)
-    has_refcount = hasattr(sys, "gettotalrefcount")
-    # Windows doesn't set Py_DEBUG, so checking for support of debug-compiled
-    # extension modules is the best option.
-    # https://github.com/pypa/pip/issues/3383#issuecomment-173267692
-    has_ext = "_d.pyd" in EXTENSION_SUFFIXES
-    if with_debug or (with_debug is None and (has_refcount or has_ext)):
-        debug = "d"
-    if py_version < (3, 8):
-        with_pymalloc = _get_config_var("WITH_PYMALLOC", warn)
-        if with_pymalloc or with_pymalloc is None:
-            pymalloc = "m"
-        if py_version < (3, 3):
-            unicode_size = _get_config_var("Py_UNICODE_SIZE", warn)
-            if unicode_size == 4 or (
-                unicode_size is None and sys.maxunicode == 0x10FFFF
-            ):
-                ucs4 = "u"
-    elif debug:
-        # Debug builds can also load "normal" extension modules.
-        # We can also assume no UCS-4 or pymalloc requirement.
-        abis.append(f"cp{version}")
-    abis.insert(
-        0,
-        "cp{version}{debug}{pymalloc}{ucs4}".format(
-            version=version, debug=debug, pymalloc=pymalloc, ucs4=ucs4
-        ),
-    )
-    return abis
-
-
-def cpython_tags(
-    python_version: Optional[PythonVersion] = None,
-    abis: Optional[Iterable[str]] = None,
-    platforms: Optional[Iterable[str]] = None,
-    *,
-    warn: bool = False,
-) -> Iterator[Tag]:
-    """
-    Yields the tags for a CPython interpreter.
-
-    The tags consist of:
-    - cp<python_version>-<abi>-<platform>
-    - cp<python_version>-abi3-<platform>
-    - cp<python_version>-none-<platform>
-    - cp<less than python_version>-abi3-<platform>  # Older Python versions down to 3.2.
-
-    If python_version only specifies a major version then user-provided ABIs and
-    the 'none' ABItag will be used.
-
-    If 'abi3' or 'none' are specified in 'abis' then they will be yielded at
-    their normal position and not at the beginning.
-    """
-    if not python_version:
-        python_version = sys.version_info[:2]
-
-    interpreter = f"cp{_version_nodot(python_version[:2])}"
-
-    if abis is None:
-        if len(python_version) > 1:
-            abis = _cpython_abis(python_version, warn)
-        else:
-            abis = []
-    abis = list(abis)
-    # 'abi3' and 'none' are explicitly handled later.
-    for explicit_abi in ("abi3", "none"):
-        try:
-            abis.remove(explicit_abi)
-        except ValueError:
-            pass
-
-    platforms = list(platforms or platform_tags())
-    for abi in abis:
-        for platform_ in platforms:
-            yield Tag(interpreter, abi, platform_)
-    if _abi3_applies(python_version):
-        yield from (Tag(interpreter, "abi3", platform_) for platform_ in platforms)
-    yield from (Tag(interpreter, "none", platform_) for platform_ in platforms)
-
-    if _abi3_applies(python_version):
-        for minor_version in range(python_version[1] - 1, 1, -1):
-            for platform_ in platforms:
-                interpreter = "cp{version}".format(
-                    version=_version_nodot((python_version[0], minor_version))
-                )
-                yield Tag(interpreter, "abi3", platform_)
-
-
-def _generic_abi() -> List[str]:
-    """
-    Return the ABI tag based on EXT_SUFFIX.
-    """
-    # The following are examples of `EXT_SUFFIX`.
-    # We want to keep the parts which are related to the ABI and remove the
-    # parts which are related to the platform:
-    # - linux:   '.cpython-310-x86_64-linux-gnu.so' => cp310
-    # - mac:     '.cpython-310-darwin.so'           => cp310
-    # - win:     '.cp310-win_amd64.pyd'             => cp310
-    # - win:     '.pyd'                             => cp37 (uses _cpython_abis())
-    # - pypy:    '.pypy38-pp73-x86_64-linux-gnu.so' => pypy38_pp73
-    # - graalpy: '.graalpy-38-native-x86_64-darwin.dylib'
-    #                                               => graalpy_38_native
-
-    ext_suffix = _get_config_var("EXT_SUFFIX", warn=True)
-    if not isinstance(ext_suffix, str) or ext_suffix[0] != ".":
-        raise SystemError("invalid sysconfig.get_config_var('EXT_SUFFIX')")
-    parts = ext_suffix.split(".")
-    if len(parts) < 3:
-        # CPython3.7 and earlier uses ".pyd" on Windows.
-        return _cpython_abis(sys.version_info[:2])
-    soabi = parts[1]
-    if soabi.startswith("cpython"):
-        # non-windows
-        abi = "cp" + soabi.split("-")[1]
-    elif soabi.startswith("cp"):
-        # windows
-        abi = soabi.split("-")[0]
-    elif soabi.startswith("pypy"):
-        abi = "-".join(soabi.split("-")[:2])
-    elif soabi.startswith("graalpy"):
-        abi = "-".join(soabi.split("-")[:3])
-    elif soabi:
-        # pyston, ironpython, others?
-        abi = soabi
-    else:
-        return []
-    return [_normalize_string(abi)]
-
-
-def generic_tags(
-    interpreter: Optional[str] = None,
-    abis: Optional[Iterable[str]] = None,
-    platforms: Optional[Iterable[str]] = None,
-    *,
-    warn: bool = False,
-) -> Iterator[Tag]:
-    """
-    Yields the tags for a generic interpreter.
-
-    The tags consist of:
-    - <interpreter>-<abi>-<platform>
-
-    The "none" ABI will be added if it was not explicitly provided.
-    """
-    if not interpreter:
-        interp_name = interpreter_name()
-        interp_version = interpreter_version(warn=warn)
-        interpreter = "".join([interp_name, interp_version])
-    if abis is None:
-        abis = _generic_abi()
-    else:
-        abis = list(abis)
-    platforms = list(platforms or platform_tags())
-    if "none" not in abis:
-        abis.append("none")
-    for abi in abis:
-        for platform_ in platforms:
-            yield Tag(interpreter, abi, platform_)
-
-
-def _py_interpreter_range(py_version: PythonVersion) -> Iterator[str]:
-    """
-    Yields Python versions in descending order.
-
-    After the latest version, the major-only version will be yielded, and then
-    all previous versions of that major version.
-    """
-    if len(py_version) > 1:
-        yield f"py{_version_nodot(py_version[:2])}"
-    yield f"py{py_version[0]}"
-    if len(py_version) > 1:
-        for minor in range(py_version[1] - 1, -1, -1):
-            yield f"py{_version_nodot((py_version[0], minor))}"
-
-
-def compatible_tags(
-    python_version: Optional[PythonVersion] = None,
-    interpreter: Optional[str] = None,
-    platforms: Optional[Iterable[str]] = None,
-) -> Iterator[Tag]:
-    """
-    Yields the sequence of tags that are compatible with a specific version of Python.
-
-    The tags consist of:
-    - py*-none-<platform>
-    - <interpreter>-none-any  # ... if `interpreter` is provided.
-    - py*-none-any
-    """
-    if not python_version:
-        python_version = sys.version_info[:2]
-    platforms = list(platforms or platform_tags())
-    for version in _py_interpreter_range(python_version):
-        for platform_ in platforms:
-            yield Tag(version, "none", platform_)
-    if interpreter:
-        yield Tag(interpreter, "none", "any")
-    for version in _py_interpreter_range(python_version):
-        yield Tag(version, "none", "any")
-
-
-def _mac_arch(arch: str, is_32bit: bool = _32_BIT_INTERPRETER) -> str:
-    if not is_32bit:
-        return arch
-
-    if arch.startswith("ppc"):
-        return "ppc"
-
-    return "i386"
-
-
-def _mac_binary_formats(version: MacVersion, cpu_arch: str) -> List[str]:
-    formats = [cpu_arch]
-    if cpu_arch == "x86_64":
-        if version < (10, 4):
-            return []
-        formats.extend(["intel", "fat64", "fat32"])
-
-    elif cpu_arch == "i386":
-        if version < (10, 4):
-            return []
-        formats.extend(["intel", "fat32", "fat"])
-
-    elif cpu_arch == "ppc64":
-        # TODO: Need to care about 32-bit PPC for ppc64 through 10.2?
-        if version > (10, 5) or version < (10, 4):
-            return []
-        formats.append("fat64")
-
-    elif cpu_arch == "ppc":
-        if version > (10, 6):
-            return []
-        formats.extend(["fat32", "fat"])
-
-    if cpu_arch in {"arm64", "x86_64"}:
-        formats.append("universal2")
-
-    if cpu_arch in {"x86_64", "i386", "ppc64", "ppc", "intel"}:
-        formats.append("universal")
-
-    return formats
-
-
-def mac_platforms(
-    version: Optional[MacVersion] = None, arch: Optional[str] = None
-) -> Iterator[str]:
-    """
-    Yields the platform tags for a macOS system.
-
-    The `version` parameter is a two-item tuple specifying the macOS version to
-    generate platform tags for. The `arch` parameter is the CPU architecture to
-    generate platform tags for. Both parameters default to the appropriate value
-    for the current system.
-    """
-    version_str, _, cpu_arch = platform.mac_ver()
-    if version is None:
-        version = cast("MacVersion", tuple(map(int, version_str.split(".")[:2])))
-        if version == (10, 16):
-            # When built against an older macOS SDK, Python will report macOS 10.16
-            # instead of the real version.
-            version_str = subprocess.run(
-                [
-                    sys.executable,
-                    "-sS",
-                    "-c",
-                    "import platform; print(platform.mac_ver()[0])",
-                ],
-                check=True,
-                env={"SYSTEM_VERSION_COMPAT": "0"},
-                stdout=subprocess.PIPE,
-                text=True,
-            ).stdout
-            version = cast("MacVersion", tuple(map(int, version_str.split(".")[:2])))
-    else:
-        version = version
-    if arch is None:
-        arch = _mac_arch(cpu_arch)
-    else:
-        arch = arch
-
-    if (10, 0) <= version and version < (11, 0):
-        # Prior to Mac OS 11, each yearly release of Mac OS bumped the
-        # "minor" version number.  The major version was always 10.
-        for minor_version in range(version[1], -1, -1):
-            compat_version = 10, minor_version
-            binary_formats = _mac_binary_formats(compat_version, arch)
-            for binary_format in binary_formats:
-                yield "macosx_{major}_{minor}_{binary_format}".format(
-                    major=10, minor=minor_version, binary_format=binary_format
-                )
-
-    if version >= (11, 0):
-        # Starting with Mac OS 11, each yearly release bumps the major version
-        # number.   The minor versions are now the midyear updates.
-        for major_version in range(version[0], 10, -1):
-            compat_version = major_version, 0
-            binary_formats = _mac_binary_formats(compat_version, arch)
-            for binary_format in binary_formats:
-                yield "macosx_{major}_{minor}_{binary_format}".format(
-                    major=major_version, minor=0, binary_format=binary_format
-                )
-
-    if version >= (11, 0):
-        # Mac OS 11 on x86_64 is compatible with binaries from previous releases.
-        # Arm64 support was introduced in 11.0, so no Arm binaries from previous
-        # releases exist.
-        #
-        # However, the "universal2" binary format can have a
-        # macOS version earlier than 11.0 when the x86_64 part of the binary supports
-        # that version of macOS.
-        if arch == "x86_64":
-            for minor_version in range(16, 3, -1):
-                compat_version = 10, minor_version
-                binary_formats = _mac_binary_formats(compat_version, arch)
-                for binary_format in binary_formats:
-                    yield "macosx_{major}_{minor}_{binary_format}".format(
-                        major=compat_version[0],
-                        minor=compat_version[1],
-                        binary_format=binary_format,
-                    )
-        else:
-            for minor_version in range(16, 3, -1):
-                compat_version = 10, minor_version
-                binary_format = "universal2"
-                yield "macosx_{major}_{minor}_{binary_format}".format(
-                    major=compat_version[0],
-                    minor=compat_version[1],
-                    binary_format=binary_format,
-                )
-
-
-def _linux_platforms(is_32bit: bool = _32_BIT_INTERPRETER) -> Iterator[str]:
-    linux = _normalize_string(sysconfig.get_platform())
-    if not linux.startswith("linux_"):
-        # we should never be here, just yield the sysconfig one and return
-        yield linux
-        return
-    if is_32bit:
-        if linux == "linux_x86_64":
-            linux = "linux_i686"
-        elif linux == "linux_aarch64":
-            linux = "linux_armv8l"
-    _, arch = linux.split("_", 1)
-    archs = {"armv8l": ["armv8l", "armv7l"]}.get(arch, [arch])
-    yield from _manylinux.platform_tags(archs)
-    yield from _musllinux.platform_tags(archs)
-    for arch in archs:
-        yield f"linux_{arch}"
-
-
-def _generic_platforms() -> Iterator[str]:
-    yield _normalize_string(sysconfig.get_platform())
-
-
-def platform_tags() -> Iterator[str]:
-    """
-    Provides the platform tags for this installation.
-    """
-    if platform.system() == "Darwin":
-        return mac_platforms()
-    elif platform.system() == "Linux":
-        return _linux_platforms()
-    else:
-        return _generic_platforms()
-
-
-def interpreter_name() -> str:
-    """
-    Returns the name of the running interpreter.
-
-    Some implementations have a reserved, two-letter abbreviation which will
-    be returned when appropriate.
-    """
-    name = sys.implementation.name
-    return INTERPRETER_SHORT_NAMES.get(name) or name
-
-
-def interpreter_version(*, warn: bool = False) -> str:
-    """
-    Returns the version of the running interpreter.
-    """
-    version = _get_config_var("py_version_nodot", warn=warn)
-    if version:
-        version = str(version)
-    else:
-        version = _version_nodot(sys.version_info[:2])
-    return version
-
-
-def _version_nodot(version: PythonVersion) -> str:
-    return "".join(map(str, version))
-
-
-def sys_tags(*, warn: bool = False) -> Iterator[Tag]:
-    """
-    Returns the sequence of tag triples for the running interpreter.
-
-    The order of the sequence corresponds to priority order for the
-    interpreter, from most to least important.
-    """
-
-    interp_name = interpreter_name()
-    if interp_name == "cp":
-        yield from cpython_tags(warn=warn)
-    else:
-        yield from generic_tags()
-
-    if interp_name == "pp":
-        interp = "pp3"
-    elif interp_name == "cp":
-        interp = "cp" + interpreter_version(warn=warn)
-    else:
-        interp = None
-    yield from compatible_tags(interpreter=interp)
diff --git a/src/poetry/core/_vendor/packaging/utils.py b/src/poetry/core/_vendor/packaging/utils.py
deleted file mode 100644
index c2c2f75..0000000
--- a/src/poetry/core/_vendor/packaging/utils.py
+++ /dev/null
@@ -1,172 +0,0 @@
-# This file is dual licensed under the terms of the Apache License, Version
-# 2.0, and the BSD License. See the LICENSE file in the root of this repository
-# for complete details.
-
-import re
-from typing import FrozenSet, NewType, Tuple, Union, cast
-
-from .tags import Tag, parse_tag
-from .version import InvalidVersion, Version
-
-BuildTag = Union[Tuple[()], Tuple[int, str]]
-NormalizedName = NewType("NormalizedName", str)
-
-
-class InvalidName(ValueError):
-    """
-    An invalid distribution name; users should refer to the packaging user guide.
-    """
-
-
-class InvalidWheelFilename(ValueError):
-    """
-    An invalid wheel filename was found, users should refer to PEP 427.
-    """
-
-
-class InvalidSdistFilename(ValueError):
-    """
-    An invalid sdist filename was found, users should refer to the packaging user guide.
-    """
-
-
-# Core metadata spec for `Name`
-_validate_regex = re.compile(
-    r"^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$", re.IGNORECASE
-)
-_canonicalize_regex = re.compile(r"[-_.]+")
-_normalized_regex = re.compile(r"^([a-z0-9]|[a-z0-9]([a-z0-9-](?!--))*[a-z0-9])$")
-# PEP 427: The build number must start with a digit.
-_build_tag_regex = re.compile(r"(\d+)(.*)")
-
-
-def canonicalize_name(name: str, *, validate: bool = False) -> NormalizedName:
-    if validate and not _validate_regex.match(name):
-        raise InvalidName(f"name is invalid: {name!r}")
-    # This is taken from PEP 503.
-    value = _canonicalize_regex.sub("-", name).lower()
-    return cast(NormalizedName, value)
-
-
-def is_normalized_name(name: str) -> bool:
-    return _normalized_regex.match(name) is not None
-
-
-def canonicalize_version(
-    version: Union[Version, str], *, strip_trailing_zero: bool = True
-) -> str:
-    """
-    This is very similar to Version.__str__, but has one subtle difference
-    with the way it handles the release segment.
-    """
-    if isinstance(version, str):
-        try:
-            parsed = Version(version)
-        except InvalidVersion:
-            # Legacy versions cannot be normalized
-            return version
-    else:
-        parsed = version
-
-    parts = []
-
-    # Epoch
-    if parsed.epoch != 0:
-        parts.append(f"{parsed.epoch}!")
-
-    # Release segment
-    release_segment = ".".join(str(x) for x in parsed.release)
-    if strip_trailing_zero:
-        # NB: This strips trailing '.0's to normalize
-        release_segment = re.sub(r"(\.0)+$", "", release_segment)
-    parts.append(release_segment)
-
-    # Pre-release
-    if parsed.pre is not None:
-        parts.append("".join(str(x) for x in parsed.pre))
-
-    # Post-release
-    if parsed.post is not None:
-        parts.append(f".post{parsed.post}")
-
-    # Development release
-    if parsed.dev is not None:
-        parts.append(f".dev{parsed.dev}")
-
-    # Local version segment
-    if parsed.local is not None:
-        parts.append(f"+{parsed.local}")
-
-    return "".join(parts)
-
-
-def parse_wheel_filename(
-    filename: str,
-) -> Tuple[NormalizedName, Version, BuildTag, FrozenSet[Tag]]:
-    if not filename.endswith(".whl"):
-        raise InvalidWheelFilename(
-            f"Invalid wheel filename (extension must be '.whl'): {filename}"
-        )
-
-    filename = filename[:-4]
-    dashes = filename.count("-")
-    if dashes not in (4, 5):
-        raise InvalidWheelFilename(
-            f"Invalid wheel filename (wrong number of parts): {filename}"
-        )
-
-    parts = filename.split("-", dashes - 2)
-    name_part = parts[0]
-    # See PEP 427 for the rules on escaping the project name.
-    if "__" in name_part or re.match(r"^[\w\d._]*$", name_part, re.UNICODE) is None:
-        raise InvalidWheelFilename(f"Invalid project name: {filename}")
-    name = canonicalize_name(name_part)
-
-    try:
-        version = Version(parts[1])
-    except InvalidVersion as e:
-        raise InvalidWheelFilename(
-            f"Invalid wheel filename (invalid version): {filename}"
-        ) from e
-
-    if dashes == 5:
-        build_part = parts[2]
-        build_match = _build_tag_regex.match(build_part)
-        if build_match is None:
-            raise InvalidWheelFilename(
-                f"Invalid build number: {build_part} in '{filename}'"
-            )
-        build = cast(BuildTag, (int(build_match.group(1)), build_match.group(2)))
-    else:
-        build = ()
-    tags = parse_tag(parts[-1])
-    return (name, version, build, tags)
-
-
-def parse_sdist_filename(filename: str) -> Tuple[NormalizedName, Version]:
-    if filename.endswith(".tar.gz"):
-        file_stem = filename[: -len(".tar.gz")]
-    elif filename.endswith(".zip"):
-        file_stem = filename[: -len(".zip")]
-    else:
-        raise InvalidSdistFilename(
-            f"Invalid sdist filename (extension must be '.tar.gz' or '.zip'):"
-            f" {filename}"
-        )
-
-    # We are requiring a PEP 440 version, which cannot contain dashes,
-    # so we split on the last dash.
-    name_part, sep, version_part = file_stem.rpartition("-")
-    if not sep:
-        raise InvalidSdistFilename(f"Invalid sdist filename: {filename}")
-
-    name = canonicalize_name(name_part)
-
-    try:
-        version = Version(version_part)
-    except InvalidVersion as e:
-        raise InvalidSdistFilename(
-            f"Invalid sdist filename (invalid version): {filename}"
-        ) from e
-
-    return (name, version)
diff --git a/src/poetry/core/_vendor/packaging/version.py b/src/poetry/core/_vendor/packaging/version.py
deleted file mode 100644
index 5faab9b..0000000
--- a/src/poetry/core/_vendor/packaging/version.py
+++ /dev/null
@@ -1,563 +0,0 @@
-# This file is dual licensed under the terms of the Apache License, Version
-# 2.0, and the BSD License. See the LICENSE file in the root of this repository
-# for complete details.
-"""
-.. testsetup::
-
-    from packaging.version import parse, Version
-"""
-
-import itertools
-import re
-from typing import Any, Callable, NamedTuple, Optional, SupportsInt, Tuple, Union
-
-from ._structures import Infinity, InfinityType, NegativeInfinity, NegativeInfinityType
-
-__all__ = ["VERSION_PATTERN", "parse", "Version", "InvalidVersion"]
-
-LocalType = Tuple[Union[int, str], ...]
-
-CmpPrePostDevType = Union[InfinityType, NegativeInfinityType, Tuple[str, int]]
-CmpLocalType = Union[
-    NegativeInfinityType,
-    Tuple[Union[Tuple[int, str], Tuple[NegativeInfinityType, Union[int, str]]], ...],
-]
-CmpKey = Tuple[
-    int,
-    Tuple[int, ...],
-    CmpPrePostDevType,
-    CmpPrePostDevType,
-    CmpPrePostDevType,
-    CmpLocalType,
-]
-VersionComparisonMethod = Callable[[CmpKey, CmpKey], bool]
-
-
-class _Version(NamedTuple):
-    epoch: int
-    release: Tuple[int, ...]
-    dev: Optional[Tuple[str, int]]
-    pre: Optional[Tuple[str, int]]
-    post: Optional[Tuple[str, int]]
-    local: Optional[LocalType]
-
-
-def parse(version: str) -> "Version":
-    """Parse the given version string.
-
-    >>> parse('1.0.dev1')
-    <Version('1.0.dev1')>
-
-    :param version: The version string to parse.
-    :raises InvalidVersion: When the version string is not a valid version.
-    """
-    return Version(version)
-
-
-class InvalidVersion(ValueError):
-    """Raised when a version string is not a valid version.
-
-    >>> Version("invalid")
-    Traceback (most recent call last):
-        ...
-    packaging.version.InvalidVersion: Invalid version: 'invalid'
-    """
-
-
-class _BaseVersion:
-    _key: Tuple[Any, ...]
-
-    def __hash__(self) -> int:
-        return hash(self._key)
-
-    # Please keep the duplicated `isinstance` check
-    # in the six comparisons hereunder
-    # unless you find a way to avoid adding overhead function calls.
-    def __lt__(self, other: "_BaseVersion") -> bool:
-        if not isinstance(other, _BaseVersion):
-            return NotImplemented
-
-        return self._key < other._key
-
-    def __le__(self, other: "_BaseVersion") -> bool:
-        if not isinstance(other, _BaseVersion):
-            return NotImplemented
-
-        return self._key <= other._key
-
-    def __eq__(self, other: object) -> bool:
-        if not isinstance(other, _BaseVersion):
-            return NotImplemented
-
-        return self._key == other._key
-
-    def __ge__(self, other: "_BaseVersion") -> bool:
-        if not isinstance(other, _BaseVersion):
-            return NotImplemented
-
-        return self._key >= other._key
-
-    def __gt__(self, other: "_BaseVersion") -> bool:
-        if not isinstance(other, _BaseVersion):
-            return NotImplemented
-
-        return self._key > other._key
-
-    def __ne__(self, other: object) -> bool:
-        if not isinstance(other, _BaseVersion):
-            return NotImplemented
-
-        return self._key != other._key
-
-
-# Deliberately not anchored to the start and end of the string, to make it
-# easier for 3rd party code to reuse
-_VERSION_PATTERN = r"""
-    v?
-    (?:
-        (?:(?P<epoch>[0-9]+)!)?                           # epoch
-        (?P<release>[0-9]+(?:\.[0-9]+)*)                  # release segment
-        (?P<pre>                                          # pre-release
-            [-_\.]?
-            (?P<pre_l>alpha|a|beta|b|preview|pre|c|rc)
-            [-_\.]?
-            (?P<pre_n>[0-9]+)?
-        )?
-        (?P<post>                                         # post release
-            (?:-(?P<post_n1>[0-9]+))
-            |
-            (?:
-                [-_\.]?
-                (?P<post_l>post|rev|r)
-                [-_\.]?
-                (?P<post_n2>[0-9]+)?
-            )
-        )?
-        (?P<dev>                                          # dev release
-            [-_\.]?
-            (?P<dev_l>dev)
-            [-_\.]?
-            (?P<dev_n>[0-9]+)?
-        )?
-    )
-    (?:\+(?P<local>[a-z0-9]+(?:[-_\.][a-z0-9]+)*))?       # local version
-"""
-
-VERSION_PATTERN = _VERSION_PATTERN
-"""
-A string containing the regular expression used to match a valid version.
-
-The pattern is not anchored at either end, and is intended for embedding in larger
-expressions (for example, matching a version number as part of a file name). The
-regular expression should be compiled with the ``re.VERBOSE`` and ``re.IGNORECASE``
-flags set.
-
-:meta hide-value:
-"""
-
-
-class Version(_BaseVersion):
-    """This class abstracts handling of a project's versions.
-
-    A :class:`Version` instance is comparison aware and can be compared and
-    sorted using the standard Python interfaces.
-
-    >>> v1 = Version("1.0a5")
-    >>> v2 = Version("1.0")
-    >>> v1
-    <Version('1.0a5')>
-    >>> v2
-    <Version('1.0')>
-    >>> v1 < v2
-    True
-    >>> v1 == v2
-    False
-    >>> v1 > v2
-    False
-    >>> v1 >= v2
-    False
-    >>> v1 <= v2
-    True
-    """
-
-    _regex = re.compile(r"^\s*" + VERSION_PATTERN + r"\s*$", re.VERBOSE | re.IGNORECASE)
-    _key: CmpKey
-
-    def __init__(self, version: str) -> None:
-        """Initialize a Version object.
-
-        :param version:
-            The string representation of a version which will be parsed and normalized
-            before use.
-        :raises InvalidVersion:
-            If the ``version`` does not conform to PEP 440 in any way then this
-            exception will be raised.
-        """
-
-        # Validate the version and parse it into pieces
-        match = self._regex.search(version)
-        if not match:
-            raise InvalidVersion(f"Invalid version: '{version}'")
-
-        # Store the parsed out pieces of the version
-        self._version = _Version(
-            epoch=int(match.group("epoch")) if match.group("epoch") else 0,
-            release=tuple(int(i) for i in match.group("release").split(".")),
-            pre=_parse_letter_version(match.group("pre_l"), match.group("pre_n")),
-            post=_parse_letter_version(
-                match.group("post_l"), match.group("post_n1") or match.group("post_n2")
-            ),
-            dev=_parse_letter_version(match.group("dev_l"), match.group("dev_n")),
-            local=_parse_local_version(match.group("local")),
-        )
-
-        # Generate a key which will be used for sorting
-        self._key = _cmpkey(
-            self._version.epoch,
-            self._version.release,
-            self._version.pre,
-            self._version.post,
-            self._version.dev,
-            self._version.local,
-        )
-
-    def __repr__(self) -> str:
-        """A representation of the Version that shows all internal state.
-
-        >>> Version('1.0.0')
-        <Version('1.0.0')>
-        """
-        return f"<Version('{self}')>"
-
-    def __str__(self) -> str:
-        """A string representation of the version that can be rounded-tripped.
-
-        >>> str(Version("1.0a5"))
-        '1.0a5'
-        """
-        parts = []
-
-        # Epoch
-        if self.epoch != 0:
-            parts.append(f"{self.epoch}!")
-
-        # Release segment
-        parts.append(".".join(str(x) for x in self.release))
-
-        # Pre-release
-        if self.pre is not None:
-            parts.append("".join(str(x) for x in self.pre))
-
-        # Post-release
-        if self.post is not None:
-            parts.append(f".post{self.post}")
-
-        # Development release
-        if self.dev is not None:
-            parts.append(f".dev{self.dev}")
-
-        # Local version segment
-        if self.local is not None:
-            parts.append(f"+{self.local}")
-
-        return "".join(parts)
-
-    @property
-    def epoch(self) -> int:
-        """The epoch of the version.
-
-        >>> Version("2.0.0").epoch
-        0
-        >>> Version("1!2.0.0").epoch
-        1
-        """
-        return self._version.epoch
-
-    @property
-    def release(self) -> Tuple[int, ...]:
-        """The components of the "release" segment of the version.
-
-        >>> Version("1.2.3").release
-        (1, 2, 3)
-        >>> Version("2.0.0").release
-        (2, 0, 0)
-        >>> Version("1!2.0.0.post0").release
-        (2, 0, 0)
-
-        Includes trailing zeroes but not the epoch or any pre-release / development /
-        post-release suffixes.
-        """
-        return self._version.release
-
-    @property
-    def pre(self) -> Optional[Tuple[str, int]]:
-        """The pre-release segment of the version.
-
-        >>> print(Version("1.2.3").pre)
-        None
-        >>> Version("1.2.3a1").pre
-        ('a', 1)
-        >>> Version("1.2.3b1").pre
-        ('b', 1)
-        >>> Version("1.2.3rc1").pre
-        ('rc', 1)
-        """
-        return self._version.pre
-
-    @property
-    def post(self) -> Optional[int]:
-        """The post-release number of the version.
-
-        >>> print(Version("1.2.3").post)
-        None
-        >>> Version("1.2.3.post1").post
-        1
-        """
-        return self._version.post[1] if self._version.post else None
-
-    @property
-    def dev(self) -> Optional[int]:
-        """The development number of the version.
-
-        >>> print(Version("1.2.3").dev)
-        None
-        >>> Version("1.2.3.dev1").dev
-        1
-        """
-        return self._version.dev[1] if self._version.dev else None
-
-    @property
-    def local(self) -> Optional[str]:
-        """The local version segment of the version.
-
-        >>> print(Version("1.2.3").local)
-        None
-        >>> Version("1.2.3+abc").local
-        'abc'
-        """
-        if self._version.local:
-            return ".".join(str(x) for x in self._version.local)
-        else:
-            return None
-
-    @property
-    def public(self) -> str:
-        """The public portion of the version.
-
-        >>> Version("1.2.3").public
-        '1.2.3'
-        >>> Version("1.2.3+abc").public
-        '1.2.3'
-        >>> Version("1.2.3+abc.dev1").public
-        '1.2.3'
-        """
-        return str(self).split("+", 1)[0]
-
-    @property
-    def base_version(self) -> str:
-        """The "base version" of the version.
-
-        >>> Version("1.2.3").base_version
-        '1.2.3'
-        >>> Version("1.2.3+abc").base_version
-        '1.2.3'
-        >>> Version("1!1.2.3+abc.dev1").base_version
-        '1!1.2.3'
-
-        The "base version" is the public version of the project without any pre or post
-        release markers.
-        """
-        parts = []
-
-        # Epoch
-        if self.epoch != 0:
-            parts.append(f"{self.epoch}!")
-
-        # Release segment
-        parts.append(".".join(str(x) for x in self.release))
-
-        return "".join(parts)
-
-    @property
-    def is_prerelease(self) -> bool:
-        """Whether this version is a pre-release.
-
-        >>> Version("1.2.3").is_prerelease
-        False
-        >>> Version("1.2.3a1").is_prerelease
-        True
-        >>> Version("1.2.3b1").is_prerelease
-        True
-        >>> Version("1.2.3rc1").is_prerelease
-        True
-        >>> Version("1.2.3dev1").is_prerelease
-        True
-        """
-        return self.dev is not None or self.pre is not None
-
-    @property
-    def is_postrelease(self) -> bool:
-        """Whether this version is a post-release.
-
-        >>> Version("1.2.3").is_postrelease
-        False
-        >>> Version("1.2.3.post1").is_postrelease
-        True
-        """
-        return self.post is not None
-
-    @property
-    def is_devrelease(self) -> bool:
-        """Whether this version is a development release.
-
-        >>> Version("1.2.3").is_devrelease
-        False
-        >>> Version("1.2.3.dev1").is_devrelease
-        True
-        """
-        return self.dev is not None
-
-    @property
-    def major(self) -> int:
-        """The first item of :attr:`release` or ``0`` if unavailable.
-
-        >>> Version("1.2.3").major
-        1
-        """
-        return self.release[0] if len(self.release) >= 1 else 0
-
-    @property
-    def minor(self) -> int:
-        """The second item of :attr:`release` or ``0`` if unavailable.
-
-        >>> Version("1.2.3").minor
-        2
-        >>> Version("1").minor
-        0
-        """
-        return self.release[1] if len(self.release) >= 2 else 0
-
-    @property
-    def micro(self) -> int:
-        """The third item of :attr:`release` or ``0`` if unavailable.
-
-        >>> Version("1.2.3").micro
-        3
-        >>> Version("1").micro
-        0
-        """
-        return self.release[2] if len(self.release) >= 3 else 0
-
-
-def _parse_letter_version(
-    letter: Optional[str], number: Union[str, bytes, SupportsInt, None]
-) -> Optional[Tuple[str, int]]:
-
-    if letter:
-        # We consider there to be an implicit 0 in a pre-release if there is
-        # not a numeral associated with it.
-        if number is None:
-            number = 0
-
-        # We normalize any letters to their lower case form
-        letter = letter.lower()
-
-        # We consider some words to be alternate spellings of other words and
-        # in those cases we want to normalize the spellings to our preferred
-        # spelling.
-        if letter == "alpha":
-            letter = "a"
-        elif letter == "beta":
-            letter = "b"
-        elif letter in ["c", "pre", "preview"]:
-            letter = "rc"
-        elif letter in ["rev", "r"]:
-            letter = "post"
-
-        return letter, int(number)
-    if not letter and number:
-        # We assume if we are given a number, but we are not given a letter
-        # then this is using the implicit post release syntax (e.g. 1.0-1)
-        letter = "post"
-
-        return letter, int(number)
-
-    return None
-
-
-_local_version_separators = re.compile(r"[\._-]")
-
-
-def _parse_local_version(local: Optional[str]) -> Optional[LocalType]:
-    """
-    Takes a string like abc.1.twelve and turns it into ("abc", 1, "twelve").
-    """
-    if local is not None:
-        return tuple(
-            part.lower() if not part.isdigit() else int(part)
-            for part in _local_version_separators.split(local)
-        )
-    return None
-
-
-def _cmpkey(
-    epoch: int,
-    release: Tuple[int, ...],
-    pre: Optional[Tuple[str, int]],
-    post: Optional[Tuple[str, int]],
-    dev: Optional[Tuple[str, int]],
-    local: Optional[LocalType],
-) -> CmpKey:
-
-    # When we compare a release version, we want to compare it with all of the
-    # trailing zeros removed. So we'll use a reverse the list, drop all the now
-    # leading zeros until we come to something non zero, then take the rest
-    # re-reverse it back into the correct order and make it a tuple and use
-    # that for our sorting key.
-    _release = tuple(
-        reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release))))
-    )
-
-    # We need to "trick" the sorting algorithm to put 1.0.dev0 before 1.0a0.
-    # We'll do this by abusing the pre segment, but we _only_ want to do this
-    # if there is not a pre or a post segment. If we have one of those then
-    # the normal sorting rules will handle this case correctly.
-    if pre is None and post is None and dev is not None:
-        _pre: CmpPrePostDevType = NegativeInfinity
-    # Versions without a pre-release (except as noted above) should sort after
-    # those with one.
-    elif pre is None:
-        _pre = Infinity
-    else:
-        _pre = pre
-
-    # Versions without a post segment should sort before those with one.
-    if post is None:
-        _post: CmpPrePostDevType = NegativeInfinity
-
-    else:
-        _post = post
-
-    # Versions without a development segment should sort after those with one.
-    if dev is None:
-        _dev: CmpPrePostDevType = Infinity
-
-    else:
-        _dev = dev
-
-    if local is None:
-        # Versions without a local segment should sort before those with one.
-        _local: CmpLocalType = NegativeInfinity
-    else:
-        # Versions with a local segment need that segment parsed to implement
-        # the sorting rules in PEP440.
-        # - Alpha numeric segments sort before numeric segments
-        # - Alpha numeric segments sort lexicographically
-        # - Numeric segments sort numerically
-        # - Shorter versions sort before longer versions when the prefixes
-        #   match exactly
-        _local = tuple(
-            (i, "") if isinstance(i, int) else (NegativeInfinity, i) for i in local
-        )
-
-    return epoch, _release, _pre, _post, _dev, _local
diff --git a/src/poetry/core/_vendor/tomli/LICENSE b/src/poetry/core/_vendor/tomli/LICENSE
deleted file mode 100644
index e859590..0000000
--- a/src/poetry/core/_vendor/tomli/LICENSE
+++ /dev/null
@@ -1,21 +0,0 @@
-MIT License
-
-Copyright (c) 2021 Taneli Hukkinen
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
diff --git a/src/poetry/core/_vendor/tomli/__init__.py b/src/poetry/core/_vendor/tomli/__init__.py
deleted file mode 100644
index 4c6ec97..0000000
--- a/src/poetry/core/_vendor/tomli/__init__.py
+++ /dev/null
@@ -1,11 +0,0 @@
-# SPDX-License-Identifier: MIT
-# SPDX-FileCopyrightText: 2021 Taneli Hukkinen
-# Licensed to PSF under a Contributor Agreement.
-
-__all__ = ("loads", "load", "TOMLDecodeError")
-__version__ = "2.0.1"  # DO NOT EDIT THIS LINE MANUALLY. LET bump2version UTILITY DO IT
-
-from ._parser import TOMLDecodeError, load, loads
-
-# Pretend this exception was created here.
-TOMLDecodeError.__module__ = __name__
diff --git a/src/poetry/core/_vendor/tomli/_parser.py b/src/poetry/core/_vendor/tomli/_parser.py
deleted file mode 100644
index f1bb0aa..0000000
--- a/src/poetry/core/_vendor/tomli/_parser.py
+++ /dev/null
@@ -1,691 +0,0 @@
-# SPDX-License-Identifier: MIT
-# SPDX-FileCopyrightText: 2021 Taneli Hukkinen
-# Licensed to PSF under a Contributor Agreement.
-
-from __future__ import annotations
-
-from collections.abc import Iterable
-import string
-from types import MappingProxyType
-from typing import Any, BinaryIO, NamedTuple
-
-from ._re import (
-    RE_DATETIME,
-    RE_LOCALTIME,
-    RE_NUMBER,
-    match_to_datetime,
-    match_to_localtime,
-    match_to_number,
-)
-from ._types import Key, ParseFloat, Pos
-
-ASCII_CTRL = frozenset(chr(i) for i in range(32)) | frozenset(chr(127))
-
-# Neither of these sets include quotation mark or backslash. They are
-# currently handled as separate cases in the parser functions.
-ILLEGAL_BASIC_STR_CHARS = ASCII_CTRL - frozenset("\t")
-ILLEGAL_MULTILINE_BASIC_STR_CHARS = ASCII_CTRL - frozenset("\t\n")
-
-ILLEGAL_LITERAL_STR_CHARS = ILLEGAL_BASIC_STR_CHARS
-ILLEGAL_MULTILINE_LITERAL_STR_CHARS = ILLEGAL_MULTILINE_BASIC_STR_CHARS
-
-ILLEGAL_COMMENT_CHARS = ILLEGAL_BASIC_STR_CHARS
-
-TOML_WS = frozenset(" \t")
-TOML_WS_AND_NEWLINE = TOML_WS | frozenset("\n")
-BARE_KEY_CHARS = frozenset(string.ascii_letters + string.digits + "-_")
-KEY_INITIAL_CHARS = BARE_KEY_CHARS | frozenset("\"'")
-HEXDIGIT_CHARS = frozenset(string.hexdigits)
-
-BASIC_STR_ESCAPE_REPLACEMENTS = MappingProxyType(
-    {
-        "\\b": "\u0008",  # backspace
-        "\\t": "\u0009",  # tab
-        "\\n": "\u000A",  # linefeed
-        "\\f": "\u000C",  # form feed
-        "\\r": "\u000D",  # carriage return
-        '\\"': "\u0022",  # quote
-        "\\\\": "\u005C",  # backslash
-    }
-)
-
-
-class TOMLDecodeError(ValueError):
-    """An error raised if a document is not valid TOML."""
-
-
-def load(__fp: BinaryIO, *, parse_float: ParseFloat = float) -> dict[str, Any]:
-    """Parse TOML from a binary file object."""
-    b = __fp.read()
-    try:
-        s = b.decode()
-    except AttributeError:
-        raise TypeError(
-            "File must be opened in binary mode, e.g. use `open('foo.toml', 'rb')`"
-        ) from None
-    return loads(s, parse_float=parse_float)
-
-
-def loads(__s: str, *, parse_float: ParseFloat = float) -> dict[str, Any]:  # noqa: C901
-    """Parse TOML from a string."""
-
-    # The spec allows converting "\r\n" to "\n", even in string
-    # literals. Let's do so to simplify parsing.
-    src = __s.replace("\r\n", "\n")
-    pos = 0
-    out = Output(NestedDict(), Flags())
-    header: Key = ()
-    parse_float = make_safe_parse_float(parse_float)
-
-    # Parse one statement at a time
-    # (typically means one line in TOML source)
-    while True:
-        # 1. Skip line leading whitespace
-        pos = skip_chars(src, pos, TOML_WS)
-
-        # 2. Parse rules. Expect one of the following:
-        #    - end of file
-        #    - end of line
-        #    - comment
-        #    - key/value pair
-        #    - append dict to list (and move to its namespace)
-        #    - create dict (and move to its namespace)
-        # Skip trailing whitespace when applicable.
-        try:
-            char = src[pos]
-        except IndexError:
-            break
-        if char == "\n":
-            pos += 1
-            continue
-        if char in KEY_INITIAL_CHARS:
-            pos = key_value_rule(src, pos, out, header, parse_float)
-            pos = skip_chars(src, pos, TOML_WS)
-        elif char == "[":
-            try:
-                second_char: str | None = src[pos + 1]
-            except IndexError:
-                second_char = None
-            out.flags.finalize_pending()
-            if second_char == "[":
-                pos, header = create_list_rule(src, pos, out)
-            else:
-                pos, header = create_dict_rule(src, pos, out)
-            pos = skip_chars(src, pos, TOML_WS)
-        elif char != "#":
-            raise suffixed_err(src, pos, "Invalid statement")
-
-        # 3. Skip comment
-        pos = skip_comment(src, pos)
-
-        # 4. Expect end of line or end of file
-        try:
-            char = src[pos]
-        except IndexError:
-            break
-        if char != "\n":
-            raise suffixed_err(
-                src, pos, "Expected newline or end of document after a statement"
-            )
-        pos += 1
-
-    return out.data.dict
-
-
-class Flags:
-    """Flags that map to parsed keys/namespaces."""
-
-    # Marks an immutable namespace (inline array or inline table).
-    FROZEN = 0
-    # Marks a nest that has been explicitly created and can no longer
-    # be opened using the "[table]" syntax.
-    EXPLICIT_NEST = 1
-
-    def __init__(self) -> None:
-        self._flags: dict[str, dict] = {}
-        self._pending_flags: set[tuple[Key, int]] = set()
-
-    def add_pending(self, key: Key, flag: int) -> None:
-        self._pending_flags.add((key, flag))
-
-    def finalize_pending(self) -> None:
-        for key, flag in self._pending_flags:
-            self.set(key, flag, recursive=False)
-        self._pending_flags.clear()
-
-    def unset_all(self, key: Key) -> None:
-        cont = self._flags
-        for k in key[:-1]:
-            if k not in cont:
-                return
-            cont = cont[k]["nested"]
-        cont.pop(key[-1], None)
-
-    def set(self, key: Key, flag: int, *, recursive: bool) -> None:  # noqa: A003
-        cont = self._flags
-        key_parent, key_stem = key[:-1], key[-1]
-        for k in key_parent:
-            if k not in cont:
-                cont[k] = {"flags": set(), "recursive_flags": set(), "nested": {}}
-            cont = cont[k]["nested"]
-        if key_stem not in cont:
-            cont[key_stem] = {"flags": set(), "recursive_flags": set(), "nested": {}}
-        cont[key_stem]["recursive_flags" if recursive else "flags"].add(flag)
-
-    def is_(self, key: Key, flag: int) -> bool:
-        if not key:
-            return False  # document root has no flags
-        cont = self._flags
-        for k in key[:-1]:
-            if k not in cont:
-                return False
-            inner_cont = cont[k]
-            if flag in inner_cont["recursive_flags"]:
-                return True
-            cont = inner_cont["nested"]
-        key_stem = key[-1]
-        if key_stem in cont:
-            cont = cont[key_stem]
-            return flag in cont["flags"] or flag in cont["recursive_flags"]
-        return False
-
-
-class NestedDict:
-    def __init__(self) -> None:
-        # The parsed content of the TOML document
-        self.dict: dict[str, Any] = {}
-
-    def get_or_create_nest(
-        self,
-        key: Key,
-        *,
-        access_lists: bool = True,
-    ) -> dict:
-        cont: Any = self.dict
-        for k in key:
-            if k not in cont:
-                cont[k] = {}
-            cont = cont[k]
-            if access_lists and isinstance(cont, list):
-                cont = cont[-1]
-            if not isinstance(cont, dict):
-                raise KeyError("There is no nest behind this key")
-        return cont
-
-    def append_nest_to_list(self, key: Key) -> None:
-        cont = self.get_or_create_nest(key[:-1])
-        last_key = key[-1]
-        if last_key in cont:
-            list_ = cont[last_key]
-            if not isinstance(list_, list):
-                raise KeyError("An object other than list found behind this key")
-            list_.append({})
-        else:
-            cont[last_key] = [{}]
-
-
-class Output(NamedTuple):
-    data: NestedDict
-    flags: Flags
-
-
-def skip_chars(src: str, pos: Pos, chars: Iterable[str]) -> Pos:
-    try:
-        while src[pos] in chars:
-            pos += 1
-    except IndexError:
-        pass
-    return pos
-
-
-def skip_until(
-    src: str,
-    pos: Pos,
-    expect: str,
-    *,
-    error_on: frozenset[str],
-    error_on_eof: bool,
-) -> Pos:
-    try:
-        new_pos = src.index(expect, pos)
-    except ValueError:
-        new_pos = len(src)
-        if error_on_eof:
-            raise suffixed_err(src, new_pos, f"Expected {expect!r}") from None
-
-    if not error_on.isdisjoint(src[pos:new_pos]):
-        while src[pos] not in error_on:
-            pos += 1
-        raise suffixed_err(src, pos, f"Found invalid character {src[pos]!r}")
-    return new_pos
-
-
-def skip_comment(src: str, pos: Pos) -> Pos:
-    try:
-        char: str | None = src[pos]
-    except IndexError:
-        char = None
-    if char == "#":
-        return skip_until(
-            src, pos + 1, "\n", error_on=ILLEGAL_COMMENT_CHARS, error_on_eof=False
-        )
-    return pos
-
-
-def skip_comments_and_array_ws(src: str, pos: Pos) -> Pos:
-    while True:
-        pos_before_skip = pos
-        pos = skip_chars(src, pos, TOML_WS_AND_NEWLINE)
-        pos = skip_comment(src, pos)
-        if pos == pos_before_skip:
-            return pos
-
-
-def create_dict_rule(src: str, pos: Pos, out: Output) -> tuple[Pos, Key]:
-    pos += 1  # Skip "["
-    pos = skip_chars(src, pos, TOML_WS)
-    pos, key = parse_key(src, pos)
-
-    if out.flags.is_(key, Flags.EXPLICIT_NEST) or out.flags.is_(key, Flags.FROZEN):
-        raise suffixed_err(src, pos, f"Cannot declare {key} twice")
-    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)
-    try:
-        out.data.get_or_create_nest(key)
-    except KeyError:
-        raise suffixed_err(src, pos, "Cannot overwrite a value") from None
-
-    if not src.startswith("]", pos):
-        raise suffixed_err(src, pos, "Expected ']' at the end of a table declaration")
-    return pos + 1, key
-
-
-def create_list_rule(src: str, pos: Pos, out: Output) -> tuple[Pos, Key]:
-    pos += 2  # Skip "[["
-    pos = skip_chars(src, pos, TOML_WS)
-    pos, key = parse_key(src, pos)
-
-    if out.flags.is_(key, Flags.FROZEN):
-        raise suffixed_err(src, pos, f"Cannot mutate immutable namespace {key}")
-    # Free the namespace now that it points to another empty list item...
-    out.flags.unset_all(key)
-    # ...but this key precisely is still prohibited from table declaration
-    out.flags.set(key, Flags.EXPLICIT_NEST, recursive=False)
-    try:
-        out.data.append_nest_to_list(key)
-    except KeyError:
-        raise suffixed_err(src, pos, "Cannot overwrite a value") from None
-
-    if not src.startswith("]]", pos):
-        raise suffixed_err(src, pos, "Expected ']]' at the end of an array declaration")
-    return pos + 2, key
-
-
-def key_value_rule(
-    src: str, pos: Pos, out: Output, header: Key, parse_float: ParseFloat
-) -> Pos:
-    pos, key, value = parse_key_value_pair(src, pos, parse_float)
-    key_parent, key_stem = key[:-1], key[-1]
-    abs_key_parent = header + key_parent
-
-    relative_path_cont_keys = (header + key[:i] for i in range(1, len(key)))
-    for cont_key in relative_path_cont_keys:
-        # Check that dotted key syntax does not redefine an existing table
-        if out.flags.is_(cont_key, Flags.EXPLICIT_NEST):
-            raise suffixed_err(src, pos, f"Cannot redefine namespace {cont_key}")
-        # Containers in the relative path can't be opened with the table syntax or
-        # dotted key/value syntax in following table sections.
-        out.flags.add_pending(cont_key, Flags.EXPLICIT_NEST)
-
-    if out.flags.is_(abs_key_parent, Flags.FROZEN):
-        raise suffixed_err(
-            src, pos, f"Cannot mutate immutable namespace {abs_key_parent}"
-        )
-
-    try:
-        nest = out.data.get_or_create_nest(abs_key_parent)
-    except KeyError:
-        raise suffixed_err(src, pos, "Cannot overwrite a value") from None
-    if key_stem in nest:
-        raise suffixed_err(src, pos, "Cannot overwrite a value")
-    # Mark inline table and array namespaces recursively immutable
-    if isinstance(value, (dict, list)):
-        out.flags.set(header + key, Flags.FROZEN, recursive=True)
-    nest[key_stem] = value
-    return pos
-
-
-def parse_key_value_pair(
-    src: str, pos: Pos, parse_float: ParseFloat
-) -> tuple[Pos, Key, Any]:
-    pos, key = parse_key(src, pos)
-    try:
-        char: str | None = src[pos]
-    except IndexError:
-        char = None
-    if char != "=":
-        raise suffixed_err(src, pos, "Expected '=' after a key in a key/value pair")
-    pos += 1
-    pos = skip_chars(src, pos, TOML_WS)
-    pos, value = parse_value(src, pos, parse_float)
-    return pos, key, value
-
-
-def parse_key(src: str, pos: Pos) -> tuple[Pos, Key]:
-    pos, key_part = parse_key_part(src, pos)
-    key: Key = (key_part,)
-    pos = skip_chars(src, pos, TOML_WS)
-    while True:
-        try:
-            char: str | None = src[pos]
-        except IndexError:
-            char = None
-        if char != ".":
-            return pos, key
-        pos += 1
-        pos = skip_chars(src, pos, TOML_WS)
-        pos, key_part = parse_key_part(src, pos)
-        key += (key_part,)
-        pos = skip_chars(src, pos, TOML_WS)
-
-
-def parse_key_part(src: str, pos: Pos) -> tuple[Pos, str]:
-    try:
-        char: str | None = src[pos]
-    except IndexError:
-        char = None
-    if char in BARE_KEY_CHARS:
-        start_pos = pos
-        pos = skip_chars(src, pos, BARE_KEY_CHARS)
-        return pos, src[start_pos:pos]
-    if char == "'":
-        return parse_literal_str(src, pos)
-    if char == '"':
-        return parse_one_line_basic_str(src, pos)
-    raise suffixed_err(src, pos, "Invalid initial character for a key part")
-
-
-def parse_one_line_basic_str(src: str, pos: Pos) -> tuple[Pos, str]:
-    pos += 1
-    return parse_basic_str(src, pos, multiline=False)
-
-
-def parse_array(src: str, pos: Pos, parse_float: ParseFloat) -> tuple[Pos, list]:
-    pos += 1
-    array: list = []
-
-    pos = skip_comments_and_array_ws(src, pos)
-    if src.startswith("]", pos):
-        return pos + 1, array
-    while True:
-        pos, val = parse_value(src, pos, parse_float)
-        array.append(val)
-        pos = skip_comments_and_array_ws(src, pos)
-
-        c = src[pos : pos + 1]
-        if c == "]":
-            return pos + 1, array
-        if c != ",":
-            raise suffixed_err(src, pos, "Unclosed array")
-        pos += 1
-
-        pos = skip_comments_and_array_ws(src, pos)
-        if src.startswith("]", pos):
-            return pos + 1, array
-
-
-def parse_inline_table(src: str, pos: Pos, parse_float: ParseFloat) -> tuple[Pos, dict]:
-    pos += 1
-    nested_dict = NestedDict()
-    flags = Flags()
-
-    pos = skip_chars(src, pos, TOML_WS)
-    if src.startswith("}", pos):
-        return pos + 1, nested_dict.dict
-    while True:
-        pos, key, value = parse_key_value_pair(src, pos, parse_float)
-        key_parent, key_stem = key[:-1], key[-1]
-        if flags.is_(key, Flags.FROZEN):
-            raise suffixed_err(src, pos, f"Cannot mutate immutable namespace {key}")
-        try:
-            nest = nested_dict.get_or_create_nest(key_parent, access_lists=False)
-        except KeyError:
-            raise suffixed_err(src, pos, "Cannot overwrite a value") from None
-        if key_stem in nest:
-            raise suffixed_err(src, pos, f"Duplicate inline table key {key_stem!r}")
-        nest[key_stem] = value
-        pos = skip_chars(src, pos, TOML_WS)
-        c = src[pos : pos + 1]
-        if c == "}":
-            return pos + 1, nested_dict.dict
-        if c != ",":
-            raise suffixed_err(src, pos, "Unclosed inline table")
-        if isinstance(value, (dict, list)):
-            flags.set(key, Flags.FROZEN, recursive=True)
-        pos += 1
-        pos = skip_chars(src, pos, TOML_WS)
-
-
-def parse_basic_str_escape(
-    src: str, pos: Pos, *, multiline: bool = False
-) -> tuple[Pos, str]:
-    escape_id = src[pos : pos + 2]
-    pos += 2
-    if multiline and escape_id in {"\\ ", "\\\t", "\\\n"}:
-        # Skip whitespace until next non-whitespace character or end of
-        # the doc. Error if non-whitespace is found before newline.
-        if escape_id != "\\\n":
-            pos = skip_chars(src, pos, TOML_WS)
-            try:
-                char = src[pos]
-            except IndexError:
-                return pos, ""
-            if char != "\n":
-                raise suffixed_err(src, pos, "Unescaped '\\' in a string")
-            pos += 1
-        pos = skip_chars(src, pos, TOML_WS_AND_NEWLINE)
-        return pos, ""
-    if escape_id == "\\u":
-        return parse_hex_char(src, pos, 4)
-    if escape_id == "\\U":
-        return parse_hex_char(src, pos, 8)
-    try:
-        return pos, BASIC_STR_ESCAPE_REPLACEMENTS[escape_id]
-    except KeyError:
-        raise suffixed_err(src, pos, "Unescaped '\\' in a string") from None
-
-
-def parse_basic_str_escape_multiline(src: str, pos: Pos) -> tuple[Pos, str]:
-    return parse_basic_str_escape(src, pos, multiline=True)
-
-
-def parse_hex_char(src: str, pos: Pos, hex_len: int) -> tuple[Pos, str]:
-    hex_str = src[pos : pos + hex_len]
-    if len(hex_str) != hex_len or not HEXDIGIT_CHARS.issuperset(hex_str):
-        raise suffixed_err(src, pos, "Invalid hex value")
-    pos += hex_len
-    hex_int = int(hex_str, 16)
-    if not is_unicode_scalar_value(hex_int):
-        raise suffixed_err(src, pos, "Escaped character is not a Unicode scalar value")
-    return pos, chr(hex_int)
-
-
-def parse_literal_str(src: str, pos: Pos) -> tuple[Pos, str]:
-    pos += 1  # Skip starting apostrophe
-    start_pos = pos
-    pos = skip_until(
-        src, pos, "'", error_on=ILLEGAL_LITERAL_STR_CHARS, error_on_eof=True
-    )
-    return pos + 1, src[start_pos:pos]  # Skip ending apostrophe
-
-
-def parse_multiline_str(src: str, pos: Pos, *, literal: bool) -> tuple[Pos, str]:
-    pos += 3
-    if src.startswith("\n", pos):
-        pos += 1
-
-    if literal:
-        delim = "'"
-        end_pos = skip_until(
-            src,
-            pos,
-            "'''",
-            error_on=ILLEGAL_MULTILINE_LITERAL_STR_CHARS,
-            error_on_eof=True,
-        )
-        result = src[pos:end_pos]
-        pos = end_pos + 3
-    else:
-        delim = '"'
-        pos, result = parse_basic_str(src, pos, multiline=True)
-
-    # Add at maximum two extra apostrophes/quotes if the end sequence
-    # is 4 or 5 chars long instead of just 3.
-    if not src.startswith(delim, pos):
-        return pos, result
-    pos += 1
-    if not src.startswith(delim, pos):
-        return pos, result + delim
-    pos += 1
-    return pos, result + (delim * 2)
-
-
-def parse_basic_str(src: str, pos: Pos, *, multiline: bool) -> tuple[Pos, str]:
-    if multiline:
-        error_on = ILLEGAL_MULTILINE_BASIC_STR_CHARS
-        parse_escapes = parse_basic_str_escape_multiline
-    else:
-        error_on = ILLEGAL_BASIC_STR_CHARS
-        parse_escapes = parse_basic_str_escape
-    result = ""
-    start_pos = pos
-    while True:
-        try:
-            char = src[pos]
-        except IndexError:
-            raise suffixed_err(src, pos, "Unterminated string") from None
-        if char == '"':
-            if not multiline:
-                return pos + 1, result + src[start_pos:pos]
-            if src.startswith('"""', pos):
-                return pos + 3, result + src[start_pos:pos]
-            pos += 1
-            continue
-        if char == "\\":
-            result += src[start_pos:pos]
-            pos, parsed_escape = parse_escapes(src, pos)
-            result += parsed_escape
-            start_pos = pos
-            continue
-        if char in error_on:
-            raise suffixed_err(src, pos, f"Illegal character {char!r}")
-        pos += 1
-
-
-def parse_value(  # noqa: C901
-    src: str, pos: Pos, parse_float: ParseFloat
-) -> tuple[Pos, Any]:
-    try:
-        char: str | None = src[pos]
-    except IndexError:
-        char = None
-
-    # IMPORTANT: order conditions based on speed of checking and likelihood
-
-    # Basic strings
-    if char == '"':
-        if src.startswith('"""', pos):
-            return parse_multiline_str(src, pos, literal=False)
-        return parse_one_line_basic_str(src, pos)
-
-    # Literal strings
-    if char == "'":
-        if src.startswith("'''", pos):
-            return parse_multiline_str(src, pos, literal=True)
-        return parse_literal_str(src, pos)
-
-    # Booleans
-    if char == "t":
-        if src.startswith("true", pos):
-            return pos + 4, True
-    if char == "f":
-        if src.startswith("false", pos):
-            return pos + 5, False
-
-    # Arrays
-    if char == "[":
-        return parse_array(src, pos, parse_float)
-
-    # Inline tables
-    if char == "{":
-        return parse_inline_table(src, pos, parse_float)
-
-    # Dates and times
-    datetime_match = RE_DATETIME.match(src, pos)
-    if datetime_match:
-        try:
-            datetime_obj = match_to_datetime(datetime_match)
-        except ValueError as e:
-            raise suffixed_err(src, pos, "Invalid date or datetime") from e
-        return datetime_match.end(), datetime_obj
-    localtime_match = RE_LOCALTIME.match(src, pos)
-    if localtime_match:
-        return localtime_match.end(), match_to_localtime(localtime_match)
-
-    # Integers and "normal" floats.
-    # The regex will greedily match any type starting with a decimal
-    # char, so needs to be located after handling of dates and times.
-    number_match = RE_NUMBER.match(src, pos)
-    if number_match:
-        return number_match.end(), match_to_number(number_match, parse_float)
-
-    # Special floats
-    first_three = src[pos : pos + 3]
-    if first_three in {"inf", "nan"}:
-        return pos + 3, parse_float(first_three)
-    first_four = src[pos : pos + 4]
-    if first_four in {"-inf", "+inf", "-nan", "+nan"}:
-        return pos + 4, parse_float(first_four)
-
-    raise suffixed_err(src, pos, "Invalid value")
-
-
-def suffixed_err(src: str, pos: Pos, msg: str) -> TOMLDecodeError:
-    """Return a `TOMLDecodeError` where error message is suffixed with
-    coordinates in source."""
-
-    def coord_repr(src: str, pos: Pos) -> str:
-        if pos >= len(src):
-            return "end of document"
-        line = src.count("\n", 0, pos) + 1
-        if line == 1:
-            column = pos + 1
-        else:
-            column = pos - src.rindex("\n", 0, pos)
-        return f"line {line}, column {column}"
-
-    return TOMLDecodeError(f"{msg} (at {coord_repr(src, pos)})")
-
-
-def is_unicode_scalar_value(codepoint: int) -> bool:
-    return (0 <= codepoint <= 55295) or (57344 <= codepoint <= 1114111)
-
-
-def make_safe_parse_float(parse_float: ParseFloat) -> ParseFloat:
-    """A decorator to make `parse_float` safe.
-
-    `parse_float` must not return dicts or lists, because these types
-    would be mixed with parsed TOML tables and arrays, thus confusing
-    the parser. The returned decorated callable raises `ValueError`
-    instead of returning illegal types.
-    """
-    # The default `float` callable never returns illegal types. Optimize it.
-    if parse_float is float:  # type: ignore[comparison-overlap]
-        return float
-
-    def safe_parse_float(float_str: str) -> Any:
-        float_value = parse_float(float_str)
-        if isinstance(float_value, (dict, list)):
-            raise ValueError("parse_float must not return dicts or lists")
-        return float_value
-
-    return safe_parse_float
diff --git a/src/poetry/core/_vendor/tomli/_re.py b/src/poetry/core/_vendor/tomli/_re.py
deleted file mode 100644
index 994bb74..0000000
--- a/src/poetry/core/_vendor/tomli/_re.py
+++ /dev/null
@@ -1,107 +0,0 @@
-# SPDX-License-Identifier: MIT
-# SPDX-FileCopyrightText: 2021 Taneli Hukkinen
-# Licensed to PSF under a Contributor Agreement.
-
-from __future__ import annotations
-
-from datetime import date, datetime, time, timedelta, timezone, tzinfo
-from functools import lru_cache
-import re
-from typing import Any
-
-from ._types import ParseFloat
-
-# E.g.
-# - 00:32:00.999999
-# - 00:32:00
-_TIME_RE_STR = r"([01][0-9]|2[0-3]):([0-5][0-9]):([0-5][0-9])(?:\.([0-9]{1,6})[0-9]*)?"
-
-RE_NUMBER = re.compile(
-    r"""
-0
-(?:
-    x[0-9A-Fa-f](?:_?[0-9A-Fa-f])*   # hex
-    |
-    b[01](?:_?[01])*                 # bin
-    |
-    o[0-7](?:_?[0-7])*               # oct
-)
-|
-[+-]?(?:0|[1-9](?:_?[0-9])*)         # dec, integer part
-(?P<floatpart>
-    (?:\.[0-9](?:_?[0-9])*)?         # optional fractional part
-    (?:[eE][+-]?[0-9](?:_?[0-9])*)?  # optional exponent part
-)
-""",
-    flags=re.VERBOSE,
-)
-RE_LOCALTIME = re.compile(_TIME_RE_STR)
-RE_DATETIME = re.compile(
-    rf"""
-([0-9]{{4}})-(0[1-9]|1[0-2])-(0[1-9]|[12][0-9]|3[01])  # date, e.g. 1988-10-27
-(?:
-    [Tt ]
-    {_TIME_RE_STR}
-    (?:([Zz])|([+-])([01][0-9]|2[0-3]):([0-5][0-9]))?  # optional time offset
-)?
-""",
-    flags=re.VERBOSE,
-)
-
-
-def match_to_datetime(match: re.Match) -> datetime | date:
-    """Convert a `RE_DATETIME` match to `datetime.datetime` or `datetime.date`.
-
-    Raises ValueError if the match does not correspond to a valid date
-    or datetime.
-    """
-    (
-        year_str,
-        month_str,
-        day_str,
-        hour_str,
-        minute_str,
-        sec_str,
-        micros_str,
-        zulu_time,
-        offset_sign_str,
-        offset_hour_str,
-        offset_minute_str,
-    ) = match.groups()
-    year, month, day = int(year_str), int(month_str), int(day_str)
-    if hour_str is None:
-        return date(year, month, day)
-    hour, minute, sec = int(hour_str), int(minute_str), int(sec_str)
-    micros = int(micros_str.ljust(6, "0")) if micros_str else 0
-    if offset_sign_str:
-        tz: tzinfo | None = cached_tz(
-            offset_hour_str, offset_minute_str, offset_sign_str
-        )
-    elif zulu_time:
-        tz = timezone.utc
-    else:  # local date-time
-        tz = None
-    return datetime(year, month, day, hour, minute, sec, micros, tzinfo=tz)
-
-
-@lru_cache(maxsize=None)
-def cached_tz(hour_str: str, minute_str: str, sign_str: str) -> timezone:
-    sign = 1 if sign_str == "+" else -1
-    return timezone(
-        timedelta(
-            hours=sign * int(hour_str),
-            minutes=sign * int(minute_str),
-        )
-    )
-
-
-def match_to_localtime(match: re.Match) -> time:
-    hour_str, minute_str, sec_str, micros_str = match.groups()
-    micros = int(micros_str.ljust(6, "0")) if micros_str else 0
-    return time(int(hour_str), int(minute_str), int(sec_str), micros)
-
-
-def match_to_number(match: re.Match, parse_float: ParseFloat) -> Any:
-    if match.group("floatpart"):
-        return parse_float(match.group())
-    return int(match.group(), 0)
diff --git a/src/poetry/core/_vendor/tomli/_types.py b/src/poetry/core/_vendor/tomli/_types.py
deleted file mode 100644
index d949412..0000000
--- a/src/poetry/core/_vendor/tomli/_types.py
+++ /dev/null
@@ -1,10 +0,0 @@
-# SPDX-License-Identifier: MIT
-# SPDX-FileCopyrightText: 2021 Taneli Hukkinen
-# Licensed to PSF under a Contributor Agreement.
-
-from typing import Any, Callable, Tuple
-
-# Type annotations
-ParseFloat = Callable[[str], Any]
-Key = Tuple[str, ...]
-Pos = int
diff --git a/src/poetry/core/_vendor/tomli/py.typed b/src/poetry/core/_vendor/tomli/py.typed
deleted file mode 100644
index 7632ecf..0000000
--- a/src/poetry/core/_vendor/tomli/py.typed
+++ /dev/null
@@ -1 +0,0 @@
-# Marker file for PEP 561
diff --git a/src/poetry/core/_vendor/vendor.txt b/src/poetry/core/_vendor/vendor.txt
deleted file mode 100644
index d6f238d..0000000
--- a/src/poetry/core/_vendor/vendor.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-fastjsonschema==2.18.1 ; python_version >= "3.8" and python_version < "4.0"
-lark==1.1.8 ; python_version >= "3.8" and python_version < "4.0"
-packaging==23.2 ; python_version >= "3.8" and python_version < "4.0"
-tomli==2.0.1 ; python_version >= "3.8" and python_version < "4.0"
diff --git a/src/poetry/core/json/__init__.py b/src/poetry/core/json/__init__.py
index 4301c9f..65c52c8 100644
--- a/src/poetry/core/json/__init__.py
+++ b/src/poetry/core/json/__init__.py
@@ -2,7 +2,7 @@ from __future__ import annotations
 
 import json
 
-from pathlib import Path
+from importlib import resources
 from typing import Any
 
 import fastjsonschema
@@ -10,22 +10,18 @@ import fastjsonschema
 from fastjsonschema.exceptions import JsonSchemaException
 
 
-SCHEMA_DIR = Path(__file__).parent / "schemas"
-
-
 class ValidationError(ValueError):
     pass
 
 
 def validate_object(obj: dict[str, Any], schema_name: str) -> list[str]:
-    schema_file = SCHEMA_DIR / f"{schema_name}.json"
-
-    if not schema_file.exists():
+    try:
+        schema = json.loads(
+            resources.read_text(f"{__name__}.schemas", f"{schema_name}.json")
+        )
+    except Exception:
         raise ValueError(f"Schema {schema_name} does not exist.")
 
-    with schema_file.open(encoding="utf-8") as f:
-        schema = json.load(f)
-
     validate = fastjsonschema.compile(schema)
 
     errors = []
diff --git a/src/poetry/core/_vendor/lark/grammars/__init__.py b/src/poetry/core/json/schemas/__init__.py
similarity index 100%
rename from src/poetry/core/_vendor/lark/grammars/__init__.py
rename to src/poetry/core/json/schemas/__init__.py
diff --git a/src/poetry/core/_vendor/lark/parsers/__init__.py b/src/poetry/core/spdx/data/__init__.py
similarity index 100%
rename from src/poetry/core/_vendor/lark/parsers/__init__.py
rename to src/poetry/core/spdx/data/__init__.py
diff --git a/src/poetry/core/spdx/helpers.py b/src/poetry/core/spdx/helpers.py
index a58a5f6..73f7c5f 100644
--- a/src/poetry/core/spdx/helpers.py
+++ b/src/poetry/core/spdx/helpers.py
@@ -3,7 +3,7 @@ from __future__ import annotations
 import functools
 import json
 
-from pathlib import Path
+from importlib import resources
 
 from poetry.core.spdx.license import License
 
@@ -20,11 +20,9 @@ def license_by_id(identifier: str) -> License:
 
 @functools.lru_cache
 def _load_licenses() -> dict[str, License]:
+    from . import __name__
     licenses = {}
-    licenses_file = Path(__file__).parent / "data" / "licenses.json"
-
-    with licenses_file.open(encoding="utf-8") as f:
-        data = json.load(f)
+    data = json.loads(resources.read_text(f"{__name__}.data", "licenses.json"))
 
     for name, license_info in data.items():
         license = License(name, license_info[0], license_info[1], license_info[2])
diff --git a/src/poetry/core/version/grammars/__init__.py b/src/poetry/core/version/grammars/__init__.py
index caf504b..971104e 100644
--- a/src/poetry/core/version/grammars/__init__.py
+++ b/src/poetry/core/version/grammars/__init__.py
@@ -1,9 +1,16 @@
 from __future__ import annotations
 
+import sys
+
 from pathlib import Path
 
 
-GRAMMAR_DIR = Path(__file__).parent
+if getattr(sys, "oxidized", False):
+    GRAMMAR_DIR = (
+        Path(__path__[0]).parents[4] / "assets" / "core" / "version" / "grammars"
+    )
+else:
+    GRAMMAR_DIR = Path(__path__[0])
 
 GRAMMAR_PEP_508_CONSTRAINTS = GRAMMAR_DIR / "pep508.lark"
 
diff --git a/vendors/deps.txt b/vendors/deps.txt
new file mode 100644
index 0000000..a923875
--- /dev/null
+++ b/vendors/deps.txt
@@ -0,0 +1,3 @@
+fastjsonschema==2.18.1
+packaging==23.2
+tomli==2.0.1
